{"cells":[{"cell_type":"markdown","metadata":{"id":"qMc1N8fpLc68"},"source":["# Baseline Code for ANLP\n","\n","## Table of content:\n","0. [Getting Started](#1.-Getting-Started)\n","\n","Neil Fabiao"]},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Ndhp98yCQRrC","executionInfo":{"status":"ok","timestamp":1651010600750,"user_tz":-60,"elapsed":3278,"user":{"displayName":"Neil Fabião","userId":"06437641205793281458"}},"outputId":"631b0bf0-8fbc-4f5b-d155-caa7f3253f95"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]}]},{"cell_type":"code","source":["cd 'drive/MyDrive/UniSussex/Baseline_code_Ngram'"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"xHUP96IBQeWt","executionInfo":{"status":"ok","timestamp":1651010604370,"user_tz":-60,"elapsed":1041,"user":{"displayName":"Neil Fabião","userId":"06437641205793281458"}},"outputId":"7e0a5b8e-714b-448c-9e82-0512ec384a65"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["/content/drive/MyDrive/UniSussex/Baseline_code_Ngram\n"]}]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"fnSvo6hZLyrB","outputId":"1e200181-ef0d-41a8-e195-79ad09355ca8","executionInfo":{"status":"ok","timestamp":1651010605584,"user_tz":-60,"elapsed":544,"user":{"displayName":"Neil Fabião","userId":"06437641205793281458"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Package punkt is already up-to-date!\n"]},{"output_type":"execute_result","data":{"text/plain":["True"]},"metadata":{},"execution_count":3}],"source":["import nltk\n","import pandas as pd\n","import numpy as np\n","import matplotlib.pyplot as plt\n","import os,random,math\n","from nltk import word_tokenize as tokenize\n","import pickle\n","import operator\n","\n","nltk.download('punkt')#make sure you have all the necessary packages on colab"]},{"cell_type":"markdown","metadata":{"id":"OWXrTqXGLcuW"},"source":["### 1 Getting started\n","\n","In this notebook, you are going to be applying an n-gram language model to the Microsoft Research Sentence Completion Challenge (Zweig and Burges, 2011).\n","\n","During the labs we developed an n-gram language model. "]},{"cell_type":"markdown","metadata":{"id":"be8tmKCG8j8H"},"source":["## 1.1 importing language model from the previous lab"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"YI5wCDZb8d35"},"outputs":[],"source":["def get_training_testing(training_dir,split=0.70):\n","    '''\n","    Initially we have a 50% split from the data.\n","    '''\n","    filenames=os.listdir(training_dir)\n","    n=len(filenames)\n","    print(\"There are {} files in the training directory: {}\".format(n,training_dir))\n","    random.seed(53)  #if you want the same random split every time\n","    random.shuffle(filenames)\n","    index=int(n*split)\n","    trainingfiles=filenames[:index]\n","    heldoutfiles=filenames[index:]\n","    return trainingfiles,heldoutfiles\n","\n","\n","class language_model():\n","    \n","    def __init__(self,trainingdir,files=[]):\n","        self.training_dir=trainingdir\n","        self.files=files\n","        self.train()\n","        \n","    def train(self):    \n","        self.unigram={}\n","        self.bigram={}\n","        self.trigram={}\n","        #self.quadrigram={}\n","         \n","        self._processfiles_2()\n","        self._make_unknowns()\n","        self._discount()\n","        self._kneser_ney()\n","        self._convert_to_probs()\n","    \n","    def _processline(self,line):\n","        tokens=[\"__START\"]+tokenize(line)+[\"__END\"]\n","        previous=\"__END\"\n","        previous_2=\"__END\"\n","        trigrama = tuple([previous,previous_2]) #a tuple that holdes the values for our trigram\n","\n","        for token in tokens:\n","            self.unigram[token]=self.unigram.get(token,0)+1\n","\n","            current=self.bigram.get(previous,{})\n","            current[token]=current.get(token,0)+1\n","\n","            #essentially trigrams are words going a group of 3 words. For bigrams we just use the previous word of unigram and assign to the bigram\n","            #basically the likely hood of 3 words going together. \n","            #you need two previous words instead of just one\n","\n","            \n","            current_trigram = self.trigram.get(trigrama,{})\n","            current_trigram[token]=current_trigram.get(token,0)+1\n","\n","            self.bigram[previous]=current\n","            self.trigram[trigrama]=current_trigram\n","\n","            previous_3 = previous_2\n","            previous_2 = previous\n","            previous=token\n","            \n","            trigrama = tuple([previous,previous_2]) #update the values such that we have w-2,w-1,wi\n"," \n","    def _processfiles(self):\n","        for afile in self.files:\n","            #print(\"Processing {}\".format(afile))\n","            try:\n","                with open(os.path.join(self.training_dir,afile)) as instream:\n","                    for line in instream:\n","                        line=line.rstrip()\n","                        if len(line)>0:\n","                            self._processline(line)\n","            except UnicodeDecodeError:\n","                print(\"UnicodeDecodeError processing {}: ignoring rest of file\".format(afile))\n","\n","    def _processfiles_2(self):\n","      for index,afile in enumerate(self.files):\n","          #print(\"Processing {}\".format(afile))\n","          lines = \" \"\n","          if index%100==0:\n","            print(\"Processing at index number {}\".format(index))\n","          try:\n","              with open(os.path.join(self.training_dir,afile)) as instream:\n","                  for line in instream:\n","                      lines+=(line)\n","                      #connan doyle is the author of The adventures of sherlock holves in the gutemberg dataset\n","                  if len(lines)>0 and re.search('conan doyle', lines, re.IGNORECASE):#use this if only training cherlock holmes books\n","                          self._processline(simple_cleaner(lines).rstrip())\n","          except UnicodeDecodeError:\n","              print(\"UnicodeDecodeError processing {}: ignoring rest of file\".format(afile))\n","      \n","            \n","    def _convert_to_probs(self):\n","        \n","        self.unigram={k:v/sum(self.unigram.values()) for (k,v) in self.unigram.items()}\n","        self.bigram={key:{k:v/sum(adict.values()) for (k,v) in adict.items()} for (key,adict) in self.bigram.items()}\n","        self.kn={k:v/sum(self.kn.values()) for (k,v) in self.kn.items()}\n","\n","        self.trigram = {key:{k:v/sum(adict.values()) for (k,v) in adict.items()} for (key,adict) in self.trigram.items()}\n","        self.kn_trigram = {k:v/sum(self.kn_trigram.values()) for (k,v) in self.kn_trigram.items()}\n","\n","        ###adjust __UNK probabilities to include probability of an individual unknown word (1/number_unknowns)\n","        \n","        self.unigram[\"__UNK\"]=self.unigram.get(\"__UNK\",0)/self.number_unknowns\n","        self.bigram[\"__UNK\"]={k:v/self.number_unknowns for (k,v) in self.bigram.get(\"__UNK\",{}).items()}\n","        #self.trigram[\"__UNK\"]={k:v/self.number_unknowns for (k,v) in self.trigram.get(\"__UNK\",{}).items()}\n","        \n","        for key,adict in self.bigram.items():\n","            adict[\"__UNK\"]=adict.get(\"__UNK\",0)/self.number_unknowns\n","            self.bigram[key]=adict\n","        self.kn[\"__UNK\"]=self.kn.get(\"__UNK\",0)/self.number_unknowns\n","        \n","        for key,adict in self.trigram.items():\n","            adict[\"__UNK\"]=adict.get(\"__UNK\",0)/self.number_unknowns\n","            self.trigram[key]=adict\n","        self.kn_trigram[\"__UNK\"]=self.kn_trigram.get(\"__UNK\",0)/self.number_unknowns\n","        \n","    def get_prob(self,token,context=\"\",methodparams={}):\n","      \n","        if methodparams.get(\"method\",\"unigram\")==\"unigram\":\n","            return self.unigram.get(token,self.unigram.get(\"__UNK\",0))\n","\n","        elif methodparams.get(\"method\",\"bigram\")==\"bigram\":\n","            if methodparams.get(\"smoothing\",\"kneser-ney\")==\"kneser-ney\":\n","                unidist=self.kn\n","            else:\n","                unidist=self.unigram\n","            bigram=self.bigram.get(context[-1],self.bigram.get(\"__UNK\",{}))\n","            big_p=bigram.get(token,bigram.get(\"__UNK\",0))\n","            lmbda=bigram[\"__DISCOUNT\"]\n","            uni_p=unidist.get(token,unidist.get(\"__UNK\",0))\n","            #print(big_p,lmbda,uni_p)\n","            p=big_p+lmbda*uni_p            \n","            return p\n","        \n","        elif methodparams.get(\"method\",\"trigram\")==\"trigram\":\n","          if methodparams.get(\"smoothing\",\"kneser-ney\") == \"kneser-ney\":\n","            unidist = self.kn_trigram\n","            unidist_bigram = self.kn\n","          else:\n","            unidist_bigram = self.unigram\n","\n","          if len(context) < 2:\n","            context = [\"__END\", context[0]]\n","          trigram=self.trigram.get(tuple(context[-2:]),self.trigram.get(\"__UNK\",{}))\n","          trig_p=trigram.get(token,trigram.get(\"__UNK\",0))\n","          lmbda_trigram = trigram[\"__DISCOUNT\"]\n","\n","          bigram=self.bigram.get(context[-1],self.bigram.get(\"__UNK\",{}))\n","          big_p=bigram.get(token,bigram.get(\"__UNK\",0))\n","          lmbda_bigram = bigram[\"__DISCOUNT\"]\n","          \n","          uni_p = unidist_bigram.get(token,unidist_bigram.get(\"__UNK\",0))\n","\n","          p = trig_p + (lmbda_trigram * big_p) + (lmbda_bigram * uni_p) \n","          return p  \n","    \n","    def nextlikely(self,k=1,current=\"\",method=\"unigram\"):\n","        #use probabilities according to method to generate a likely next sequence\n","        #choose random token from k best\n","        blacklist=[\"__START\",\"__UNK\",\"__DISCOUNT\"]\n","       \n","        if method==\"unigram\":\n","            dist=self.unigram\n","        else:\n","            dist=self.bigram.get(current,self.bigram.get(\"__UNK\",{}))\n","    \n","        #sort the tokens by unigram probability\n","        mostlikely=sorted(list(dist.items()),key=operator.itemgetter(1),reverse=True)\n","        #filter out any undesirable tokens\n","        filtered=[w for (w,p) in mostlikely if w not in blacklist]\n","        #choose one randomly from the top k\n","        res=random.choice(filtered[:k])\n","        return res\n","    \n","    def generate(self,k=1,end=\"__END\",limit=20,method=\"bigram\",methodparams={}):\n","        if method==\"\":\n","            method=methodparams.get(\"method\",\"bigram\")\n","        current=\"__START\"\n","        tokens=[]\n","        while current!=end and len(tokens)<limit:\n","            current=self.nextlikely(k=k,current=current,method=method)\n","            tokens.append(current)\n","        return \" \".join(tokens[:-1])\n","    \n","    def compute_prob_line(self,line,methodparams={}):\n","        #this will add _start to the beginning of a line of text\n","        #compute the probability of the line according to the desired model\n","        #and returns probability together with number of tokens\n","        \n","        tokens=[\"__START\"]+tokenize(line)+[\"__END\"]\n","        acc=0\n","        for i,token in enumerate(tokens[1:]):\n","            acc+=math.log(self.get_prob(token,tokens[:i+1],methodparams))\n","        return acc,len(tokens[1:])\n","    \n","    def compute_probability(self,filenames=[],methodparams={}):\n","        #computes the probability (and length) of a corpus contained in filenames\n","        if filenames==[]:\n","            filenames=self.files\n","        \n","        total_p=0\n","        total_N=0\n","        for i,afile in enumerate(filenames):\n","            #print(\"Processing file {}:{}\".format(i,afile))\n","            try:\n","                with open(os.path.join(self.training_dir,afile)) as instream:\n","                    for line in instream:\n","                        line=line.rstrip()\n","                        if len(line)>0 and re.search('conan doyle', lines, re.IGNORECASE) :\n","                            p,N=self.compute_prob_line(line,methodparams=methodparams)\n","                            total_p+=p\n","                            total_N+=N\n","            except UnicodeDecodeError:\n","                print(\"UnicodeDecodeError processing file {}: ignoring rest of file\".format(afile))\n","        return total_p,total_N\n","    \n","    def compute_perplexity(self,filenames=[],methodparams={\"method\":\"bigram\",\"smoothing\":\"kneser-ney\"}):\n","        \n","        #compute the probability and length of the corpus\n","        #calculate perplexity\n","        #lower perplexity means that the model better explains the data\n","        \n","        p,N=self.compute_probability(filenames=filenames,methodparams=methodparams)\n","        #print(p,N)\n","        pp=math.exp(-p/N)\n","        return pp  \n","    \n","    def _make_unknowns(self,known=2):\n","        unknown=0\n","        self.number_unknowns=0\n","\n","        #unigram\n","        for (k,v) in list(self.unigram.items()):\n","            if v<known:\n","                del self.unigram[k]\n","                self.unigram[\"__UNK\"]=self.unigram.get(\"__UNK\",0)+v\n","                self.number_unknowns+=1\n","        \n","        #bigrams\n","        for (k,adict) in list(self.bigram.items()):\n","            for (kk,v) in list(adict.items()):\n","                isknown=self.unigram.get(kk,0)\n","                if isknown==0 and not kk==\"__DISCOUNT\":\n","                    adict[\"__UNK\"]=adict.get(\"__UNK\",0)+v\n","                    del adict[kk]\n","            isknown=self.unigram.get(k,0)\n","            if isknown==0:\n","                del self.bigram[k]\n","                current=self.bigram.get(\"__UNK\",{})\n","                current.update(adict)\n","                self.bigram[\"__UNK\"]=current\n","            else:\n","                self.bigram[k]=adict\n","        \n","        #trigrams\n","        for (key_,adict) in list(self.trigram.items()):\n","          for (kk,v) in list(adict.items()):\n","            isknown = self.unigram.get(kk,0)\n","            if isknown == 0 and not kk == \"__DISCOUNT\":\n","              adict[\"__UNK\"] = adict.get(\"__UNK\",0) + v\n","              del adict[kk]\n","          previous, previous_2 = key_\n","          isknown_1 = self.unigram.get(previous,0)\n","          isknown_2 = self.unigram.get(previous_2,0)\n","          if isknown_1 == 0 or isknown_2 == 0:\n","            del self.trigram[key_]\n","            current = self.trigram.get(\"__UNK\",{})\n","            current.update(adict)\n","            self.trigram[\"__UNK\"] = current\n","          else:\n","            self.trigram[k] = adict\n","                \n","    def _discount(self,discount=0.75):\n","        #discount each bigram count by a small fixed amount\n","        self.bigram={k:{kk:value-discount for (kk,value) in adict.items()}for (k,adict) in self.bigram.items()}\n","\n","        # trigram discount\n","        self.trigram = {k:{kk:value-discount for (kk,value) in adict.items()}for (k,adict) in self.trigram.items()}\n","        \n","        #for each word, store the total amount of the discount so that the total is the same \n","        #i.e., so we are reserving this as probability mass \n","        for k in self.bigram.keys():\n","          lamb=len(self.bigram[k])\n","          self.bigram[k][\"__DISCOUNT\"]=lamb*discount\n","\n","        for k in self.trigram.keys():\n","          lamb = len(self.trigram[k])\n","          self.trigram[k][\"__DISCOUNT\"] = lamb*discount\n","               \n","    def _kneser_ney(self):\n","        #work out kneser-ney unigram probabilities #Neil Fabiao\n","        #count the number of contexts each word has been seen in\n","        self.kn={}\n","        for (k,adict) in self.bigram.items():\n","            for kk in adict.keys():\n","                self.kn[kk]=self.kn.get(kk,0)+1\n","\n","        self.kn_trigram = {}\n","        for (k,adict) in self.trigram.items():\n","          for kk in adict.keys():\n","            self.kn_trigram[kk] = self.kn_trigram.get(kk,0)+1\n","    "]},{"cell_type":"markdown","metadata":{"id":"zL2k2D6KO9cv"},"source":["## 1.1.1 Testing and makig sure we can:\n","* look up bigram probabilities\n","* generatate a sentence according to the model\n","* calculate the perplexity of a test sentence"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"iE82qTcIMDcy","outputId":"7edb382e-208a-413f-b254-5401c7990800","executionInfo":{"status":"ok","timestamp":1651014740413,"user_tz":-60,"elapsed":27217,"user":{"displayName":"Neil Fabião","userId":"06437641205793281458"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["There are 522 files in the training directory: ../sentence-completion/Holmes_Training_Data\n"]}],"source":["#you may need to update this when running on your own machine \n","parentdir= '../sentence-completion'\n","TRAINING_DIR=os.path.join(parentdir,\"Holmes_Training_Data\")\n","training,testing=get_training_testing(TRAINING_DIR)\n","mylm=language_model(trainingdir=TRAINING_DIR,files=training[:10])"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"w9dJ_-uNJk6i","outputId":"565959ea-3afc-45f1-cacd-8ec53bfcef70","executionInfo":{"status":"ok","timestamp":1651014750532,"user_tz":-60,"elapsed":10126,"user":{"displayName":"Neil Fabião","userId":"06437641205793281458"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["Looking up unigram probabilities\n","'' the the , `` and . , the to '' . of `` and , . . .\n","Generating sentence according to the model\n","\n","Computing perplexity\n"]},{"output_type":"execute_result","data":{"text/plain":["638.2420476803173"]},"metadata":{},"execution_count":99}],"source":["print('Looking up unigram probabilities')\n","print(mylm.generate(k=10,method='unigram'))\n","\n","print('Generating sentence according to the model')\n","print(mylm.generate(method='unigram'))\n","\n","print('Computing perplexity')\n","mylm.compute_perplexity(methodparams={\"method\":\"unigram\",\"smoothing\":\"kneser-ney\"})"]},{"cell_type":"code","source":["print('Looking up bigram probabilities')\n","print(mylm.generate(k=10,method='bigram'))\n","\n","print('Generating sentence according to the model')\n","print(mylm.generate(method='bigram'))\n","\n","print('Computing perplexity')\n","mylm.compute_perplexity(methodparams={\"method\":\"bigram\",\"smoothing\":\"kneser-ney\"})"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"AUW10UHNSANs","executionInfo":{"status":"ok","timestamp":1651014789141,"user_tz":-60,"elapsed":11728,"user":{"displayName":"Neil Fabião","userId":"06437641205793281458"}},"outputId":"794ca3a0-960c-4431-95e2-317a73d8bb6c"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Looking up bigram probabilities\n","the king , I 've got the world , ' She looked upon me , I can . The\n","Generating sentence according to the model\n","`` I am not\n","Computing perplexity\n"]},{"output_type":"execute_result","data":{"text/plain":["98.51955775889262"]},"metadata":{},"execution_count":103}]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"82czzxlUKK6O","outputId":"38a83865-a339-4bce-a822-1e7db70f2d8c","executionInfo":{"status":"ok","timestamp":1651014776549,"user_tz":-60,"elapsed":14311,"user":{"displayName":"Neil Fabião","userId":"06437641205793281458"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["Looking up trigram probabilities\n","to see it ? And I\n","Generating sentence according to the model\n","`` I am not\n","Computing perplexity\n"]},{"output_type":"execute_result","data":{"text/plain":["129.60295441092222"]},"metadata":{},"execution_count":101}],"source":["print('Looking up trigram probabilities')\n","print(mylm.generate(k=10,method='trigram'))\n","\n","print('Generating sentence according to the model')\n","print(mylm.generate(method='trigram'))\n","\n","print('Computing perplexity')\n","mylm.compute_perplexity(methodparams={\"method\":\"trigram\",\"smoothing\":\"kneser-ney\"})"]},{"cell_type":"markdown","metadata":{"id":"0KBiXTsyPnoV"},"source":["## 1.2 Now lets load in and have a look at the sentence completion challenge questions."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":204},"id":"YPfz_gfCPg3C","outputId":"10db15a7-3719-4ba5-9b72-c6c55a61fc3a","executionInfo":{"status":"ok","timestamp":1651014485219,"user_tz":-60,"elapsed":25,"user":{"displayName":"Neil Fabião","userId":"06437641205793281458"}}},"outputs":[{"output_type":"execute_result","data":{"text/plain":["  id                                           question        a)  \\\n","0  1  I have it from the same source that you are bo...    crying   \n","1  2  It was furnished partly as a sitting and partl...  daintily   \n","2  3  As I descended , my old ally , the _____ , cam...      gods   \n","3  4  We got off , _____ our fare , and the trap rat...   rubbing   \n","4  5  He held in his hand a _____ of blue paper , sc...    supply   \n","\n","                b)             c)         d)             e)  \n","0  instantaneously       residing    matched        walking  \n","1        privately  inadvertently  miserably    comfortably  \n","2             moon        panther      guard  country-dance  \n","3         doubling           paid     naming       carrying  \n","4           parcel           sign      sheet         chorus  "],"text/html":["\n","  <div id=\"df-54f1b635-0312-44c5-b934-80312d348481\">\n","    <div class=\"colab-df-container\">\n","      <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>id</th>\n","      <th>question</th>\n","      <th>a)</th>\n","      <th>b)</th>\n","      <th>c)</th>\n","      <th>d)</th>\n","      <th>e)</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>1</td>\n","      <td>I have it from the same source that you are bo...</td>\n","      <td>crying</td>\n","      <td>instantaneously</td>\n","      <td>residing</td>\n","      <td>matched</td>\n","      <td>walking</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>2</td>\n","      <td>It was furnished partly as a sitting and partl...</td>\n","      <td>daintily</td>\n","      <td>privately</td>\n","      <td>inadvertently</td>\n","      <td>miserably</td>\n","      <td>comfortably</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>3</td>\n","      <td>As I descended , my old ally , the _____ , cam...</td>\n","      <td>gods</td>\n","      <td>moon</td>\n","      <td>panther</td>\n","      <td>guard</td>\n","      <td>country-dance</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>4</td>\n","      <td>We got off , _____ our fare , and the trap rat...</td>\n","      <td>rubbing</td>\n","      <td>doubling</td>\n","      <td>paid</td>\n","      <td>naming</td>\n","      <td>carrying</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>5</td>\n","      <td>He held in his hand a _____ of blue paper , sc...</td>\n","      <td>supply</td>\n","      <td>parcel</td>\n","      <td>sign</td>\n","      <td>sheet</td>\n","      <td>chorus</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>\n","      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-54f1b635-0312-44c5-b934-80312d348481')\"\n","              title=\"Convert this dataframe to an interactive table.\"\n","              style=\"display:none;\">\n","        \n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","       width=\"24px\">\n","    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n","    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n","  </svg>\n","      </button>\n","      \n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      flex-wrap:wrap;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","      <script>\n","        const buttonEl =\n","          document.querySelector('#df-54f1b635-0312-44c5-b934-80312d348481 button.colab-df-convert');\n","        buttonEl.style.display =\n","          google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","        async function convertToInteractive(key) {\n","          const element = document.querySelector('#df-54f1b635-0312-44c5-b934-80312d348481');\n","          const dataTable =\n","            await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                     [key], {});\n","          if (!dataTable) return;\n","\n","          const docLinkHtml = 'Like what you see? Visit the ' +\n","            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","            + ' to learn more about interactive tables.';\n","          element.innerHTML = '';\n","          dataTable['output_type'] = 'display_data';\n","          await google.colab.output.renderOutput(dataTable, element);\n","          const docLink = document.createElement('div');\n","          docLink.innerHTML = docLinkHtml;\n","          element.appendChild(docLink);\n","        }\n","      </script>\n","    </div>\n","  </div>\n","  "]},"metadata":{},"execution_count":88}],"source":["import pandas as pd, csv\n","questions=os.path.join(parentdir,\"testing_data.csv\")\n","answers=os.path.join(parentdir,\"test_answer.csv\")\n","\n","with open(questions) as instream:\n","    csvreader=csv.reader(instream)\n","    lines=list(csvreader)\n","qs_df=pd.DataFrame(lines[1:],columns=lines[0])\n","qs_df.head()"]},{"cell_type":"markdown","metadata":{"id":"B9dp3qC5_j7-"},"source":["## 1.3 Building and evaluating a system to carry out the Sentence Completion Challenge"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"dzc0iTXFOzOu"},"outputs":[],"source":["import csv\n","\n","class question:\n","    \n","    def __init__(self,aline):\n","        self.fields=aline\n","    \n","    def get_field(self,field):\n","        return self.fields[question.colnames[field]]\n","    \n","    def add_answer(self,fields):\n","        self.answer=fields[1]\n","   \n","    def chooseA(self):\n","        return(\"a\")\n","\n","    def get_tokens(self):\n","      return [\"__START\"]+tokenize(self.fields[question.colnames[\"question\"]])+[\"__END\"]\n","    \n","    def get_left_context(self,window=1,target=\"_____\"):\n","        '''\n","        if the current position has a blank point where you need to insert an answer. Then this code\n","        will look at the left hand side according to the window size and estimate the best probable word\n","        '''\n","        found=-1\n","        sent_tokens=self.get_tokens()\n","        for i,token in enumerate(sent_tokens):\n","          if token==target:\n","              found=i\n","              break  \n","            \n","        if found>-1:\n","            return sent_tokens[i-window:i]\n","        else:\n","            return []\n","\n","    def get_right_context(self,window=1,target=\"_____\"):\n","      '''\n","      if the current position has a blank point where you need to insert an answer. Then this code\n","      will look at the right hand side according to the window size and estimate the best probable word\n","      '''\n","      found=-1\n","      sent_tokens=self.get_tokens()\n","      for i,token in enumerate(sent_tokens):\n","        if token==target:\n","              found=i\n","              break  \n","        \n","      if found>-1:\n","          \n","          return sent_tokens[found+1:found+window+1]\n","          \n","      else:\n","          return []\n","        \n","    def predict_and_score(self,method):\n","        \n","        #compare prediction according to method with the correct answer\n","        #return 1 or 0 accordingly\n","        prediction=self.predict(method=method)\n","        if prediction ==self.answer:\n","            return 1\n","        else:\n","            return 0\n","\n","    def choose(self,lm,method,choices=[]):\n","        if choices==[]:\n","            choices=[\"a\",\"b\",\"c\",\"d\",\"e\"]\n","        \n","        #the tests are the n-grams which use both left and right context windows\n","        if method == \"bigram\" or method == \"unigram\" or method == \"trigram\":\n","          #this is a n-gram probability that uses both left and right context window\n","          rc=self.get_right_context(window=2)\n","          lc=self.get_left_context(window=2)\n","          #here is where you get the probability \n","          probs=[lm.get_prob(rc[0],[self.get_field(ch+\")\")],methodparams={\"method\":method.split(\"_\")[0]})\n","          *lm.get_prob(self.get_field(ch+\")\"),lc,methodparams={\"method\":method.split(\"_\")[0]}) for ch in choices]\n","\n","        elif method==\"bigram_right\" or method == \"unigram_right\" or method == \"trigram_right\":\n","          context=self.get_right_context(window=1)\n","          probs=[lm.get_prob(context[0],[self.get_field(ch+\")\")],methodparams={\"method\":method.split(\"_\")[0]}) for ch in choices]\n","        \n","        #the normal bigram and unigram use only the left context\n","        elif method==\"bigram_left\" or method == \"unigram_left\" or method == \"trigram_left\":\n","          #this is the default method\n","          #if the method is just unigram it will go here\n","          #this is a n-gram probability that uses left context window\n","          context=self.get_left_context(window=2)\n","          #here is where you get the probability \n","          probs=[lm.get_prob(self.get_field(ch+\")\"),context,methodparams={\"method\":method.split(\"_\")[0]}) for ch in choices]\n","\n","        else:\n","          context=self.get_left_context(window=2)\n","          probs=[lm.get_prob(self.get_field(ch+\")\"),context,methodparams={\"method\":method.split(\"_\")[0]}) for ch in choices]\n","\n","        maxprob=max(probs)\n","        bestchoices=[ch for ch,prob in zip(choices,probs) if prob == maxprob]\n","        return np.random.choice(bestchoices)\n","    \n","    def predict(self,method=\"chooseA\",model=mylm):\n","      if method==\"chooseA\":\n","          return self.chooseA()\n","      elif method==\"random\":\n","          return self.chooserandom()\n","      else:\n","          return self.choose(mylm,method=method)\n","\n","class scc_reader:\n","    \n","    def __init__(self,qs,ans):\n","        self.qs=qs\n","        self.ans=ans\n","        self.read_files()\n","        \n","    def read_files(self):\n","        \n","        #read in the question file\n","        with open(self.qs) as instream:\n","            csvreader=csv.reader(instream)\n","            qlines=list(csvreader)\n","        \n","        #store the column names as a reverse index so they can be used to reference parts of the question\n","        question.colnames={item:i for i,item in enumerate(qlines[0])}\n","        \n","        #create a question instance for each line of the file (other than heading line)\n","        self.questions=[question(qline) for qline in qlines[1:]]\n","        \n","        #read in the answer file\n","        with open(self.ans) as instream:\n","            csvreader=csv.reader(instream)\n","            alines=list(csvreader)\n","            \n","        #add answers to questions so predictions can be checked    \n","        for q,aline in zip(self.questions,alines[1:]):\n","            q.add_answer(aline)\n","        \n","    def get_field(self,field):\n","        return [q.get_field(field) for q in self.questions] \n","    \n","    def predict(self,method):\n","        return [q.predict(method=method) for q in self.questions]\n","    \n","    def predict_and_score(self,method):\n","        scores=[q.predict_and_score(method=method) for q in self.questions]\n","        return sum(scores)/len(scores)\n","    "]},{"cell_type":"markdown","metadata":{"id":"KCXgFkNGAjk3"},"source":["## 1.3.1 Testing and makig sure we can:\n","* predict the score given by our model"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"82fTQRJ4PEE1","outputId":"826cae2a-5ef7-4648-cfeb-af498c286089","executionInfo":{"status":"ok","timestamp":1651014790876,"user_tz":-60,"elapsed":1434,"user":{"displayName":"Neil Fabião","userId":"06437641205793281458"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["This is the score using unigrams: 0.25384615384615383\n","This is the score using bigrams: 0.2528846153846154\n","This is the score using trigrams: 0.2519230769230769\n"]}],"source":["SCC = scc_reader(qs=questions,ans=answers)\n","#since bigrams and unigrams are probability based models. It is best to score the average of 10 - 20 runs depending on the speed.\n","#here I am just running the model and verifying that it works.\n","print('This is the score using unigrams:',SCC.predict_and_score(method=\"unigram\"))\n","print('This is the score using bigrams:',SCC.predict_and_score(method=\"bigram\"))\n","print('This is the score using trigrams:',SCC.predict_and_score(method=\"trigram\"))"]},{"cell_type":"code","source":["print('This is the score using unigram testing:',SCC.predict_and_score(method=\"unigram_right\"))\n","print('This is the score using bigrams testing:',SCC.predict_and_score(method=\"bigram_right\"))\n","print('This is the score using trigrams testing:',SCC.predict_and_score(method=\"trigram_right\"))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"N8h3BQYnVvzZ","executionInfo":{"status":"ok","timestamp":1651014791811,"user_tz":-60,"elapsed":940,"user":{"displayName":"Neil Fabião","userId":"06437641205793281458"}},"outputId":"47d59f3d-5d7d-40e0-86ff-ebfd225832e6"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["This is the score using unigram testing: 0.1875\n","This is the score using bigrams testing: 0.18461538461538463\n","This is the score using trigrams testing: 0.1798076923076923\n"]}]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"pISodBLIl3q7","outputId":"0df44b6a-c7b0-4b82-e904-9f697d63978e","executionInfo":{"status":"ok","timestamp":1651014880118,"user_tz":-60,"elapsed":207,"user":{"displayName":"Neil Fabião","userId":"06437641205793281458"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["This is the score using unigram testing: 0.2778846153846154\n","This is the score using bigrams testing: 0.27307692307692305\n","This is the score using trigrams testing: 0.27692307692307694\n"]}],"source":["print('This is the score using unigram testing:',SCC.predict_and_score(method=\"unigram_\"))\n","print('This is the score using bigrams testing:',SCC.predict_and_score(method=\"bigram_\"))\n","print('This is the score using trigrams testing:',SCC.predict_and_score(method=\"trigram_\"))"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":436},"id":"BQzunGU3PsTb","outputId":"d725e451-dc2f-43e2-fbb0-b2b89520616e","scrolled":true,"executionInfo":{"status":"ok","timestamp":1651014885783,"user_tz":-60,"elapsed":394,"user":{"displayName":"Neil Fabião","userId":"06437641205793281458"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["This is the prediction\n"]},{"output_type":"execute_result","data":{"text/plain":["        id                                           question          a)  \\\n","0        1  I have it from the same source that you are bo...      crying   \n","1        2  It was furnished partly as a sitting and partl...    daintily   \n","2        3  As I descended , my old ally , the _____ , cam...        gods   \n","3        4  We got off , _____ our fare , and the trap rat...     rubbing   \n","4        5  He held in his hand a _____ of blue paper , sc...      supply   \n","...    ...                                                ...         ...   \n","1035  1036  The bedrooms in this _____ are on the ground f...        wing   \n","1036  1037  Our visitor bore every mark of being an averag...       blind   \n","1037  1038  The terror of his face lay in his eyes , howev...     cruelty   \n","1038  1039  It is your commonplace , _____ crimes which ar...  underlying   \n","1039  1040  On the last occasion he had _____ that if my f...    believed   \n","\n","                   b)             c)          d)             e) unigram_pred  \\\n","0     instantaneously       residing     matched        walking            e   \n","1           privately  inadvertently   miserably    comfortably            d   \n","2                moon        panther       guard  country-dance            d   \n","3            doubling           paid      naming       carrying            c   \n","4              parcel           sign       sheet         chorus            c   \n","...               ...            ...         ...            ...          ...   \n","1035            coach        balcony     kingdom  neighbourhood            a   \n","1036        energetic       eloquent     pompous   sandy-haired            a   \n","1037        novitiate        justice      broker        success            e   \n","1038      featureless    theological  flattering     inevitable            e   \n","1039           proved     discovered    remarked        dreamed            d   \n","\n","     bigram_pred trigram_pred  \n","0              a            a  \n","1              d            d  \n","2              d            a  \n","3              e            e  \n","4              c            c  \n","...          ...          ...  \n","1035           a            a  \n","1036           b            b  \n","1037           c            c  \n","1038           e            e  \n","1039           c            e  \n","\n","[1040 rows x 10 columns]"],"text/html":["\n","  <div id=\"df-060a5290-f079-479e-9572-99b5d3c341cf\">\n","    <div class=\"colab-df-container\">\n","      <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>id</th>\n","      <th>question</th>\n","      <th>a)</th>\n","      <th>b)</th>\n","      <th>c)</th>\n","      <th>d)</th>\n","      <th>e)</th>\n","      <th>unigram_pred</th>\n","      <th>bigram_pred</th>\n","      <th>trigram_pred</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>1</td>\n","      <td>I have it from the same source that you are bo...</td>\n","      <td>crying</td>\n","      <td>instantaneously</td>\n","      <td>residing</td>\n","      <td>matched</td>\n","      <td>walking</td>\n","      <td>e</td>\n","      <td>a</td>\n","      <td>a</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>2</td>\n","      <td>It was furnished partly as a sitting and partl...</td>\n","      <td>daintily</td>\n","      <td>privately</td>\n","      <td>inadvertently</td>\n","      <td>miserably</td>\n","      <td>comfortably</td>\n","      <td>d</td>\n","      <td>d</td>\n","      <td>d</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>3</td>\n","      <td>As I descended , my old ally , the _____ , cam...</td>\n","      <td>gods</td>\n","      <td>moon</td>\n","      <td>panther</td>\n","      <td>guard</td>\n","      <td>country-dance</td>\n","      <td>d</td>\n","      <td>d</td>\n","      <td>a</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>4</td>\n","      <td>We got off , _____ our fare , and the trap rat...</td>\n","      <td>rubbing</td>\n","      <td>doubling</td>\n","      <td>paid</td>\n","      <td>naming</td>\n","      <td>carrying</td>\n","      <td>c</td>\n","      <td>e</td>\n","      <td>e</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>5</td>\n","      <td>He held in his hand a _____ of blue paper , sc...</td>\n","      <td>supply</td>\n","      <td>parcel</td>\n","      <td>sign</td>\n","      <td>sheet</td>\n","      <td>chorus</td>\n","      <td>c</td>\n","      <td>c</td>\n","      <td>c</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>1035</th>\n","      <td>1036</td>\n","      <td>The bedrooms in this _____ are on the ground f...</td>\n","      <td>wing</td>\n","      <td>coach</td>\n","      <td>balcony</td>\n","      <td>kingdom</td>\n","      <td>neighbourhood</td>\n","      <td>a</td>\n","      <td>a</td>\n","      <td>a</td>\n","    </tr>\n","    <tr>\n","      <th>1036</th>\n","      <td>1037</td>\n","      <td>Our visitor bore every mark of being an averag...</td>\n","      <td>blind</td>\n","      <td>energetic</td>\n","      <td>eloquent</td>\n","      <td>pompous</td>\n","      <td>sandy-haired</td>\n","      <td>a</td>\n","      <td>b</td>\n","      <td>b</td>\n","    </tr>\n","    <tr>\n","      <th>1037</th>\n","      <td>1038</td>\n","      <td>The terror of his face lay in his eyes , howev...</td>\n","      <td>cruelty</td>\n","      <td>novitiate</td>\n","      <td>justice</td>\n","      <td>broker</td>\n","      <td>success</td>\n","      <td>e</td>\n","      <td>c</td>\n","      <td>c</td>\n","    </tr>\n","    <tr>\n","      <th>1038</th>\n","      <td>1039</td>\n","      <td>It is your commonplace , _____ crimes which ar...</td>\n","      <td>underlying</td>\n","      <td>featureless</td>\n","      <td>theological</td>\n","      <td>flattering</td>\n","      <td>inevitable</td>\n","      <td>e</td>\n","      <td>e</td>\n","      <td>e</td>\n","    </tr>\n","    <tr>\n","      <th>1039</th>\n","      <td>1040</td>\n","      <td>On the last occasion he had _____ that if my f...</td>\n","      <td>believed</td>\n","      <td>proved</td>\n","      <td>discovered</td>\n","      <td>remarked</td>\n","      <td>dreamed</td>\n","      <td>d</td>\n","      <td>c</td>\n","      <td>e</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>1040 rows × 10 columns</p>\n","</div>\n","      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-060a5290-f079-479e-9572-99b5d3c341cf')\"\n","              title=\"Convert this dataframe to an interactive table.\"\n","              style=\"display:none;\">\n","        \n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","       width=\"24px\">\n","    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n","    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n","  </svg>\n","      </button>\n","      \n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      flex-wrap:wrap;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","      <script>\n","        const buttonEl =\n","          document.querySelector('#df-060a5290-f079-479e-9572-99b5d3c341cf button.colab-df-convert');\n","        buttonEl.style.display =\n","          google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","        async function convertToInteractive(key) {\n","          const element = document.querySelector('#df-060a5290-f079-479e-9572-99b5d3c341cf');\n","          const dataTable =\n","            await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                     [key], {});\n","          if (!dataTable) return;\n","\n","          const docLinkHtml = 'Like what you see? Visit the ' +\n","            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","            + ' to learn more about interactive tables.';\n","          element.innerHTML = '';\n","          dataTable['output_type'] = 'display_data';\n","          await google.colab.output.renderOutput(dataTable, element);\n","          const docLink = document.createElement('div');\n","          docLink.innerHTML = docLinkHtml;\n","          element.appendChild(docLink);\n","        }\n","      </script>\n","    </div>\n","  </div>\n","  "]},"metadata":{},"execution_count":118}],"source":["print('This is the prediction')\n","qs_df[\"unigram_pred\"]=SCC.predict(method=\"unigram\")\n","qs_df[\"bigram_pred\"]=SCC.predict(method=\"bigram\")\n","qs_df[\"trigram_pred\"]=SCC.predict(method=\"trigram\")\n","qs_df"]},{"cell_type":"markdown","metadata":{"id":"Gxw4p40_MTXC"},"source":["### 2 Experimental setup "]},{"cell_type":"markdown","source":["#### 2.1 Experiment with different len of files "],"metadata":{"id":"l_TPFcSgPjvK"}},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"fK1pUsb1Ggg1","outputId":"ad94b7d9-7b41-4740-8650-28bc4bc02306","executionInfo":{"status":"ok","timestamp":1651014887028,"user_tz":-60,"elapsed":302,"user":{"displayName":"Neil Fabião","userId":"06437641205793281458"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["There are 522 files in the training directory: ../sentence-completion/Holmes_Training_Data\n","In this experiment we have the following setup:\n","[  5  10  50  75 100 125 150 175 200 225]\n"]}],"source":["TRAINING_DIR=os.path.join(parentdir,\"Holmes_Training_Data\")\n","\n","length_of_file = []\n","length_of_file = np.append(length_of_file, 5)#we want to start at 5 \n","length_of_file = np.append(length_of_file, 10)#we want to add 10 \n","length_of_file = np.append(length_of_file,np.arange(50, 250, 25)) #Create a sequence of numbers from 50 to 552\n","split_size = np.arange(0.0, 1.0, 0.1) #this is for later testing on split size.\n","#length_of_file = np.append(length_of_file, 200)\n","length_of_file = length_of_file.astype(int) #the array elemetns must be int\n","\n","training,testing=get_training_testing(TRAINING_DIR,0.7)\n","print('In this experiment we have the following setup:')\n","print(length_of_file)\n","\n","name_1 ='Sentence_Completion vanilla_unigram_5_225'\n","name_2 ='Sentence_Completion vanilla_bigram_5_225'\n","name_3 ='Sentence_Completion vanilla_trigram_5_225'\n","results_unigram = {}\n","results_bigram = {}\n","results_trigram = {}\n","number_of_runs = 3"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"G9DaGS5yM1m8","outputId":"8296b675-1a81-4825-c682-d2acd8f876db","executionInfo":{"status":"ok","timestamp":1651034765646,"user_tz":-60,"elapsed":19877711,"user":{"displayName":"Neil Fabião","userId":"06437641205793281458"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["Processing file of length 5\n","finished run 0\n","finished run 1\n","finished run 2\n","Processing file of length 10\n","finished run 0\n","finished run 1\n","finished run 2\n","Processing file of length 50\n","finished run 0\n","finished run 1\n","finished run 2\n","Processing file of length 75\n","finished run 0\n","finished run 1\n","finished run 2\n","Processing file of length 100\n","finished run 0\n","finished run 1\n","finished run 2\n","Processing file of length 125\n","finished run 0\n","finished run 1\n","finished run 2\n","Processing file of length 150\n","finished run 0\n","finished run 1\n","finished run 2\n","Processing file of length 175\n","UnicodeDecodeError processing WTSLW10.TXT: ignoring rest of file\n","finished run 0\n","UnicodeDecodeError processing WTSLW10.TXT: ignoring rest of file\n","finished run 1\n","UnicodeDecodeError processing WTSLW10.TXT: ignoring rest of file\n","finished run 2\n","UnicodeDecodeError processing file WTSLW10.TXT: ignoring rest of file\n","UnicodeDecodeError processing file WTSLW10.TXT: ignoring rest of file\n","UnicodeDecodeError processing file WTSLW10.TXT: ignoring rest of file\n","Processing file of length 200\n","UnicodeDecodeError processing WTSLW10.TXT: ignoring rest of file\n","UnicodeDecodeError processing MOHIC10.TXT: ignoring rest of file\n","finished run 0\n","UnicodeDecodeError processing WTSLW10.TXT: ignoring rest of file\n","UnicodeDecodeError processing MOHIC10.TXT: ignoring rest of file\n","finished run 1\n","UnicodeDecodeError processing WTSLW10.TXT: ignoring rest of file\n","UnicodeDecodeError processing MOHIC10.TXT: ignoring rest of file\n","finished run 2\n","UnicodeDecodeError processing file WTSLW10.TXT: ignoring rest of file\n","UnicodeDecodeError processing file MOHIC10.TXT: ignoring rest of file\n","UnicodeDecodeError processing file WTSLW10.TXT: ignoring rest of file\n","UnicodeDecodeError processing file MOHIC10.TXT: ignoring rest of file\n","UnicodeDecodeError processing file WTSLW10.TXT: ignoring rest of file\n","UnicodeDecodeError processing file MOHIC10.TXT: ignoring rest of file\n","Processing file of length 225\n","UnicodeDecodeError processing WTSLW10.TXT: ignoring rest of file\n","UnicodeDecodeError processing MOHIC10.TXT: ignoring rest of file\n","UnicodeDecodeError processing TBTAS10.TXT: ignoring rest of file\n","finished run 0\n","UnicodeDecodeError processing WTSLW10.TXT: ignoring rest of file\n","UnicodeDecodeError processing MOHIC10.TXT: ignoring rest of file\n","UnicodeDecodeError processing TBTAS10.TXT: ignoring rest of file\n","finished run 1\n","UnicodeDecodeError processing WTSLW10.TXT: ignoring rest of file\n","UnicodeDecodeError processing MOHIC10.TXT: ignoring rest of file\n","UnicodeDecodeError processing TBTAS10.TXT: ignoring rest of file\n","finished run 2\n","UnicodeDecodeError processing file WTSLW10.TXT: ignoring rest of file\n","UnicodeDecodeError processing file MOHIC10.TXT: ignoring rest of file\n","UnicodeDecodeError processing file TBTAS10.TXT: ignoring rest of file\n","UnicodeDecodeError processing file WTSLW10.TXT: ignoring rest of file\n","UnicodeDecodeError processing file MOHIC10.TXT: ignoring rest of file\n","UnicodeDecodeError processing file TBTAS10.TXT: ignoring rest of file\n","UnicodeDecodeError processing file WTSLW10.TXT: ignoring rest of file\n","UnicodeDecodeError processing file MOHIC10.TXT: ignoring rest of file\n","UnicodeDecodeError processing file TBTAS10.TXT: ignoring rest of file\n"]}],"source":["#sentence completion vanilla is the first step to understand the best length of file.\n","runs = 0\n","\n","#for size in length_of_file:\n","for size in length_of_file:\n","    res_unigram = {}\n","    res_bigram = {}\n","    res_trigram = {}\n","\n","    print(\"Processing file of length {}\".format(size))\n","\n","    for i in range(number_of_runs):\n","    \n","        mylm=language_model(trainingdir=TRAINING_DIR,files=training[:size])#loading the data from the files\n","\n","        SCC = scc_reader(qs=questions,ans=answers)#computing the questions and answers given by the model\n","\n","        accuracy_1 = SCC.predict_and_score(method=\"unigram_\")\n","        accuracy_2 = SCC.predict_and_score(method=\"bigram_\")\n","        accuracy_3 = SCC.predict_and_score(method=\"trigram_\")\n","\n","        res_unigram[name_1]=res_unigram.get(name_1,0)+accuracy_1/number_of_runs\n","        res_bigram[name_2]=res_bigram.get(name_2,0)+accuracy_2/number_of_runs\n","        res_trigram[name_3]=res_trigram.get(name_3,0)+accuracy_3/number_of_runs\n","        print('finished run',runs)\n","        runs+=1\n","        \n","    #computing perplexity for each n-gram\n","    perplexity_1 = mylm.compute_perplexity(methodparams={\"method\":\"unigram\",\"smoothing\":\"kneser-ney\"})\n","    perplexity_2 = mylm.compute_perplexity(methodparams={\"method\":\"bigram\",\"smoothing\":\"kneser-ney\"})\n","    perplexity_3 = mylm.compute_perplexity(methodparams={\"method\":\"trigram\",\"smoothing\":\"kneser-ney\"})\n","\n","    results_unigram[size]=[res_unigram,perplexity_1]\n","    results_bigram[size]=[res_bigram,perplexity_2]\n","    results_trigram[size]=[res_trigram,perplexity_3]\n","    \n","    \n","    #open a file, where you ant to store the data\n","    # opening and dump information to file\n","    with open('Sentence_Completion_vanilla_unigram_file_len225.pickle', 'wb') as f:\n","        pickle.dump(results_unigram, f)\n","        \n","    with open('Sentence_Completion_vanilla_bigram_file_len225.pickle', 'wb') as f:\n","        pickle.dump(results_bigram, f)\n","    \n","    with open('Sentence_Completion_vanilla_trigram_file_len225.pickle', 'wb') as f:\n","        pickle.dump(results_trigram, f)\n","\n","    runs=0\n","    "]},{"cell_type":"markdown","source":["### 2.1 Experiment with context windows size 1"],"metadata":{"id":"U8PMmBQCYWCG"}},{"cell_type":"code","source":["class question(question):\n","    #you wouldn't normally have a class inherit from itself like this\n","    #but it is quite a neat way in jupyter notebooks to extend pre-existing classes\n","    #you could alternatively redefine the class (copying all of the pre-existing class)\n","\n","    def choose(self,lm,method,choices=[]):\n","        if choices==[]:\n","            choices=[\"a\",\"b\",\"c\",\"d\",\"e\"]\n","        \n","        #the tests are the n-grams which use both left and right context windows\n","        if method == \"bigram\" or method == \"unigram\" or method == \"trigram\":\n","          #this is a n-gram probability that uses both left and right context window\n","          rc=self.get_right_context(window=1)\n","          lc=self.get_left_context(window=1)\n","          #here is where you get the probability \n","          probs=[lm.get_prob(rc[0],[self.get_field(ch+\")\")],methodparams={\"method\":method.split(\"_\")[0]})\n","          *lm.get_prob(self.get_field(ch+\")\"),lc,methodparams={\"method\":method.split(\"_\")[0]}) for ch in choices]\n","\n","        elif method==\"bigram_right\" or method == \"unigram_right\" or method == \"trigram_right\":\n","            context=self.get_right_context(window=1)\n","            probs=[lm.get_prob(context[0],[self.get_field(ch+\")\")],methodparams={\"method\":method.split(\"_\")[0]}) for ch in choices]\n","        \n","        #the normal bigram and unigram use only the left context\n","        elif method==\"bigram_left\" or method == \"unigram_left\" or method == \"trigram_left\":\n","          #this is the default method\n","          #if the method is just unigram it will go here\n","          #this is a n-gram probability that uses left context window\n","          context=self.get_left_context(window=1)\n","          #here is where you get the probability \n","          probs=[lm.get_prob(self.get_field(ch+\")\"),context,methodparams={\"method\":method.split(\"_\")[0]}) for ch in choices]\n","\n","        else:\n","            context=self.get_left_context(window=1)\n","            probs=[lm.get_prob(self.get_field(ch+\")\"),context,methodparams={\"method\":method.split(\"_\")[0]}) for ch in choices]\n","\n","        maxprob=max(probs)\n","        bestchoices=[ch for ch,prob in zip(choices,probs) if prob == maxprob]\n","        return np.random.choice(bestchoices)"],"metadata":{"id":"y_BjzaMeYHFE"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["results_unigram = {}\n","results_bigram = {}\n","results_trigram = {}\n","\n","name_1 ='Sentence_Completion vanilla_unigram_rightW2_100'\n","name_2 ='Sentence_Completion vanilla_bigram_rightW2_100'\n","name_3 ='Sentence_Completion vanilla_trigram_rightW2_100'\n","number_of_runs=3\n","\n","#sentence completion vanilla is the first step to understand the best length of file.\n","runs = 0\n","size = 100\n","\n","print(\"Processing file of length {}\".format(size))\n","\n","for i in range(number_of_runs):\n","\n","    mylm=language_model(trainingdir=TRAINING_DIR,files=training[:size])#loading the data from the files\n","\n","    SCC = scc_reader(qs=questions,ans=answers)#computing the questions and answers given by the model\n","\n","    accuracy_1 = SCC.predict_and_score(method=\"unigram_right\")\n","    accuracy_2 = SCC.predict_and_score(method=\"bigram_right\")\n","    accuracy_3 = SCC.predict_and_score(method=\"trigram_right\")\n","\n","    res_unigram[name_1]=res_unigram.get(name_1,0)+accuracy_1/number_of_runs\n","    res_bigram[name_2]=res_bigram.get(name_2,0)+accuracy_2/number_of_runs\n","    res_trigram[name_3]=res_trigram.get(name_3,0)+accuracy_3/number_of_runs\n","    print('finished run',runs)\n","    runs+=1\n","    \n","#computing perplexity for each n-gram\n","perplexity_1 = mylm.compute_perplexity(methodparams={\"method\":\"unigram\",\"smoothing\":\"kneser-ney\"})\n","perplexity_2 = mylm.compute_perplexity(methodparams={\"method\":\"bigram\",\"smoothing\":\"kneser-ney\"})\n","perplexity_3 = mylm.compute_perplexity(methodparams={\"method\":\"trigram\",\"smoothing\":\"kneser-ney\"})\n","\n","results_unigram[size]=[res_unigram,perplexity_1]\n","results_bigram[size]=[res_bigram,perplexity_2]\n","results_trigram[size]=[res_trigram,perplexity_3]\n","\n","#open a file, where you ant to store the data\n","# opening and dump information to file\n","with open('Sentence_Completion_vanilla_unigram_right_rightW2_100.pickle', 'wb') as f:\n","    pickle.dump(results_unigram, f)\n","    \n","with open('Sentence_Completion_vanilla_bigram_right_rightW2_100.pickle', 'wb') as f:\n","    pickle.dump(results_bigram, f)\n","\n","with open('Sentence_Completion_vanilla_trigram_right_rightW2_100.pickle', 'wb') as f:\n","    pickle.dump(results_trigram, f)\n","\n","runs=0"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"l0JNYWb0djEN","executionInfo":{"status":"ok","timestamp":1651036871166,"user_tz":-60,"elapsed":1693695,"user":{"displayName":"Neil Fabião","userId":"06437641205793281458"}},"outputId":"021fcbf8-c165-4538-bfc9-964450723dd7"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Processing file of length 100\n","finished run 0\n","finished run 1\n","finished run 2\n"]}]},{"cell_type":"code","source":["results_unigram = {}\n","results_bigram = {}\n","results_trigram = {}\n","\n","name_1 ='Sentence_Completion vanilla_unigram_leftW2_100'\n","name_2 ='Sentence_Completion vanilla_bigram_leftW2_100'\n","name_3 ='Sentence_Completion vanilla_trigram_leftW2_100'\n","number_of_runs=3\n","\n","#sentence completion vanilla is the first step to understand the best length of file.\n","runs = 0\n","size = 100\n","\n","print(\"Processing file of length {}\".format(size))\n","\n","for i in range(number_of_runs):\n","\n","    mylm=language_model(trainingdir=TRAINING_DIR,files=training[:size])#loading the data from the files\n","\n","    SCC = scc_reader(qs=questions,ans=answers)#computing the questions and answers given by the model\n","\n","    accuracy_1 = SCC.predict_and_score(method=\"unigram_left\")\n","    accuracy_2 = SCC.predict_and_score(method=\"bigram_left\")\n","    accuracy_3 = SCC.predict_and_score(method=\"trigram_left\")\n","\n","    res_unigram[name_1]=res_unigram.get(name_1,0)+accuracy_1/number_of_runs\n","    res_bigram[name_2]=res_bigram.get(name_2,0)+accuracy_2/number_of_runs\n","    res_trigram[name_3]=res_trigram.get(name_3,0)+accuracy_3/number_of_runs\n","    print('finished run',runs)\n","    runs+=1\n","    \n","#computing perplexity for each n-gram\n","perplexity_1 = mylm.compute_perplexity(methodparams={\"method\":\"unigram\",\"smoothing\":\"kneser-ney\"})\n","perplexity_2 = mylm.compute_perplexity(methodparams={\"method\":\"bigram\",\"smoothing\":\"kneser-ney\"})\n","perplexity_3 = mylm.compute_perplexity(methodparams={\"method\":\"trigram\",\"smoothing\":\"kneser-ney\"})\n","\n","results_unigram[size]=[res_unigram,perplexity_1]\n","results_bigram[size]=[res_bigram,perplexity_2]\n","results_trigram[size]=[res_trigram,perplexity_3]\n","\n","#open a file, where you ant to store the data\n","# opening and dump information to file\n","with open('Sentence_Completion_vanilla_unigram_right_leftW2_100.pickle', 'wb') as f:\n","    pickle.dump(results_unigram, f)\n","    \n","with open('Sentence_Completion_vanilla_bigram_right_leftW2_100.pickle', 'wb') as f:\n","    pickle.dump(results_bigram, f)\n","\n","with open('Sentence_Completion_vanilla_trigram_right_leftW2_100.pickle', 'wb') as f:\n","    pickle.dump(results_trigram, f)\n","\n","runs=0"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"m0kSG1qHdczr","executionInfo":{"status":"ok","timestamp":1651038542499,"user_tz":-60,"elapsed":1671356,"user":{"displayName":"Neil Fabião","userId":"06437641205793281458"}},"outputId":"ad9651be-1f48-4f54-8578-ff3ca56ec10a"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Processing file of length 100\n","finished run 0\n","finished run 1\n","finished run 2\n"]}]},{"cell_type":"code","source":["results_unigram = {}\n","results_bigram = {}\n","results_trigram = {}\n","\n","name_1 ='Sentence_Completion vanilla_unigram_NormaltW2_100'\n","name_2 ='Sentence_Completion vanilla_bigram_NormalW2_100'\n","name_3 ='Sentence_Completion vanilla_trigram_NormalW2_100'\n","number_of_runs=3\n","\n","#sentence completion vanilla is the first step to understand the best length of file.\n","runs = 0\n","size = 100\n","\n","print(\"Processing file of length {}\".format(size))\n","\n","for i in range(number_of_runs):\n","\n","    mylm=language_model(trainingdir=TRAINING_DIR,files=training[:size])#loading the data from the files\n","\n","    SCC = scc_reader(qs=questions,ans=answers)#computing the questions and answers given by the model\n","\n","    accuracy_1 = SCC.predict_and_score(method=\"unigram\")\n","    accuracy_2 = SCC.predict_and_score(method=\"bigram\")\n","    accuracy_3 = SCC.predict_and_score(method=\"trigram\")\n","\n","    res_unigram[name_1]=res_unigram.get(name_1,0)+accuracy_1/number_of_runs\n","    res_bigram[name_2]=res_bigram.get(name_2,0)+accuracy_2/number_of_runs\n","    res_trigram[name_3]=res_trigram.get(name_3,0)+accuracy_3/number_of_runs\n","    print('finished run',runs)\n","    runs+=1\n","    \n","#computing perplexity for each n-gram\n","perplexity_1 = mylm.compute_perplexity(methodparams={\"method\":\"unigram\",\"smoothing\":\"kneser-ney\"})\n","perplexity_2 = mylm.compute_perplexity(methodparams={\"method\":\"bigram\",\"smoothing\":\"kneser-ney\"})\n","perplexity_3 = mylm.compute_perplexity(methodparams={\"method\":\"trigram\",\"smoothing\":\"kneser-ney\"})\n","\n","results_unigram[size]=[res_unigram,perplexity_1]\n","results_bigram[size]=[res_bigram,perplexity_2]\n","results_trigram[size]=[res_trigram,perplexity_3]\n","\n","#open a file, where you ant to store the data\n","# opening and dump information to file\n","with open('Sentence_Completion_vanilla_unigram_right_NormalW2_100.pickle', 'wb') as f:\n","    pickle.dump(results_unigram, f)\n","    \n","with open('Sentence_Completion_vanilla_bigram_right_NormalW2_100.pickle', 'wb') as f:\n","    pickle.dump(results_bigram, f)\n","\n","with open('Sentence_Completion_vanilla_trigram_right_NormalW2_100.pickle', 'wb') as f:\n","    pickle.dump(results_trigram, f)\n","\n","runs=0"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"qZ_LDk7zePyw","executionInfo":{"status":"ok","timestamp":1651040220381,"user_tz":-60,"elapsed":1677910,"user":{"displayName":"Neil Fabião","userId":"06437641205793281458"}},"outputId":"1d6497df-fa55-4f6a-be39-c0a3f6b88872"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Processing file of length 100\n","finished run 0\n","finished run 1\n","finished run 2\n"]}]},{"cell_type":"markdown","source":["### 2.2 Experiment with context windows size 3"],"metadata":{"id":"2B7qKcDHv-GL"}},{"cell_type":"code","source":["class question(question):\n","    #you wouldn't normally have a class inherit from itself like this\n","    #but it is quite a neat way in jupyter notebooks to extend pre-existing classes\n","    #you could alternatively redefine the class (copying all of the pre-existing class)\n","\n","    def choose(self,lm,method,choices=[]):\n","        if choices==[]:\n","            choices=[\"a\",\"b\",\"c\",\"d\",\"e\"]\n","        \n","        #the tests are the n-grams which use both left and right context windows\n","        if method == \"bigram\" or method == \"unigram\" or method == \"trigram\":\n","          #this is a n-gram probability that uses both left and right context window\n","          rc=self.get_right_context(window=3)\n","          lc=self.get_left_context(window=3)\n","          #here is where you get the probability \n","          probs=[lm.get_prob(rc[0],[self.get_field(ch+\")\")],methodparams={\"method\":method.split(\"_\")[0]})\n","          *lm.get_prob(self.get_field(ch+\")\"),lc,methodparams={\"method\":method.split(\"_\")[0]}) for ch in choices]\n","\n","        elif method==\"bigram_right\" or method == \"unigram_right\" or method == \"trigram_right\":\n","            context=self.get_right_context(window=3)\n","            probs=[lm.get_prob(context[0],[self.get_field(ch+\")\")],methodparams={\"method\":method.split(\"_\")[0]}) for ch in choices]\n","        \n","        #the normal bigram and unigram use only the left context\n","        elif method==\"bigram_left\" or method == \"unigram_left\" or method == \"trigram_left\":\n","          #this is the default method\n","          #if the method is just unigram it will go here\n","          #this is a n-gram probability that uses left context window\n","          context=self.get_left_context(window=3)\n","          #here is where you get the probability \n","          probs=[lm.get_prob(self.get_field(ch+\")\"),context,methodparams={\"method\":method.split(\"_\")[0]}) for ch in choices]\n","\n","        else:\n","            context=self.get_left_context(window=3)\n","            probs=[lm.get_prob(self.get_field(ch+\")\"),context,methodparams={\"method\":method.split(\"_\")[0]}) for ch in choices]\n","\n","        maxprob=max(probs)\n","        bestchoices=[ch for ch,prob in zip(choices,probs) if prob == maxprob]\n","        return np.random.choice(bestchoices)"],"metadata":{"id":"grlELQokv-GQ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["results_unigram = {}\n","results_bigram = {}\n","results_trigram = {}\n","\n","name_1 ='Sentence_Completion vanilla_unigram_rightW3_100'\n","name_2 ='Sentence_Completion vanilla_bigram_rightW3_100'\n","name_3 ='Sentence_Completion vanilla_trigram_rightW3_100'\n","number_of_runs=3\n","\n","#sentence completion vanilla is the first step to understand the best length of file.\n","runs = 0\n","size = 100\n","\n","print(\"Processing file of length {}\".format(size))\n","\n","for i in range(number_of_runs):\n","\n","    mylm=language_model(trainingdir=TRAINING_DIR,files=training[:size])#loading the data from the files\n","\n","    SCC = scc_reader(qs=questions,ans=answers)#computing the questions and answers given by the model\n","\n","    accuracy_1 = SCC.predict_and_score(method=\"unigram_right\")\n","    accuracy_2 = SCC.predict_and_score(method=\"bigram_right\")\n","    accuracy_3 = SCC.predict_and_score(method=\"trigram_right\")\n","\n","    res_unigram[name_1]=res_unigram.get(name_1,0)+accuracy_1/number_of_runs\n","    res_bigram[name_2]=res_bigram.get(name_2,0)+accuracy_2/number_of_runs\n","    res_trigram[name_3]=res_trigram.get(name_3,0)+accuracy_3/number_of_runs\n","    print('finished run',runs)\n","    runs+=1\n","    \n","#computing perplexity for each n-gram\n","perplexity_1 = mylm.compute_perplexity(methodparams={\"method\":\"unigram\",\"smoothing\":\"kneser-ney\"})\n","perplexity_2 = mylm.compute_perplexity(methodparams={\"method\":\"bigram\",\"smoothing\":\"kneser-ney\"})\n","perplexity_3 = mylm.compute_perplexity(methodparams={\"method\":\"trigram\",\"smoothing\":\"kneser-ney\"})\n","\n","results_unigram[size]=[res_unigram,perplexity_1]\n","results_bigram[size]=[res_bigram,perplexity_2]\n","results_trigram[size]=[res_trigram,perplexity_3]\n","\n","#open a file, where you ant to store the data\n","# opening and dump information to file\n","with open('Sentence_Completion_vanilla_unigram_right_rightW3_100.pickle', 'wb') as f:\n","    pickle.dump(results_unigram, f)\n","    \n","with open('Sentence_Completion_vanilla_bigram_right_rightW3_100.pickle', 'wb') as f:\n","    pickle.dump(results_bigram, f)\n","\n","with open('Sentence_Completion_vanilla_trigram_right_rightW3_100.pickle', 'wb') as f:\n","    pickle.dump(results_trigram, f)\n","\n","runs=0"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1651041901353,"user_tz":-60,"elapsed":1680988,"user":{"displayName":"Neil Fabião","userId":"06437641205793281458"}},"outputId":"63bdbf36-acc3-4b97-eca7-34a7f2592b76","id":"fmDjC2Mdv-GT"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Processing file of length 100\n","finished run 0\n","finished run 1\n","finished run 2\n"]}]},{"cell_type":"code","source":["results_unigram = {}\n","results_bigram = {}\n","results_trigram = {}\n","\n","name_1 ='Sentence_Completion vanilla_unigram_leftW3_100'\n","name_2 ='Sentence_Completion vanilla_bigram_leftW3_100'\n","name_3 ='Sentence_Completion vanilla_trigram_leftW3_100'\n","number_of_runs=3\n","\n","#sentence completion vanilla is the first step to understand the best length of file.\n","runs = 0\n","size = 100\n","\n","print(\"Processing file of length {}\".format(size))\n","\n","for i in range(number_of_runs):\n","\n","    mylm=language_model(trainingdir=TRAINING_DIR,files=training[:size])#loading the data from the files\n","\n","    SCC = scc_reader(qs=questions,ans=answers)#computing the questions and answers given by the model\n","\n","    accuracy_1 = SCC.predict_and_score(method=\"unigram_left\")\n","    accuracy_2 = SCC.predict_and_score(method=\"bigram_left\")\n","    accuracy_3 = SCC.predict_and_score(method=\"trigram_left\")\n","\n","    res_unigram[name_1]=res_unigram.get(name_1,0)+accuracy_1/number_of_runs\n","    res_bigram[name_2]=res_bigram.get(name_2,0)+accuracy_2/number_of_runs\n","    res_trigram[name_3]=res_trigram.get(name_3,0)+accuracy_3/number_of_runs\n","    print('finished run',runs)\n","    runs+=1\n","    \n","#computing perplexity for each n-gram\n","perplexity_1 = mylm.compute_perplexity(methodparams={\"method\":\"unigram\",\"smoothing\":\"kneser-ney\"})\n","perplexity_2 = mylm.compute_perplexity(methodparams={\"method\":\"bigram\",\"smoothing\":\"kneser-ney\"})\n","perplexity_3 = mylm.compute_perplexity(methodparams={\"method\":\"trigram\",\"smoothing\":\"kneser-ney\"})\n","\n","results_unigram[size]=[res_unigram,perplexity_1]\n","results_bigram[size]=[res_bigram,perplexity_2]\n","results_trigram[size]=[res_trigram,perplexity_3]\n","\n","#open a file, where you ant to store the data\n","# opening and dump information to file\n","with open('Sentence_Completion_vanilla_unigram_right_leftW3_100.pickle', 'wb') as f:\n","    pickle.dump(results_unigram, f)\n","    \n","with open('Sentence_Completion_vanilla_bigram_right_leftW3_100.pickle', 'wb') as f:\n","    pickle.dump(results_bigram, f)\n","\n","with open('Sentence_Completion_vanilla_trigram_right_leftW3_100.pickle', 'wb') as f:\n","    pickle.dump(results_trigram, f)\n","\n","runs=0"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"outputId":"43d77098-e093-408a-d8d0-74b0c3d086d3","id":"OhyBmCByv-GW","executionInfo":{"status":"ok","timestamp":1651043585528,"user_tz":-60,"elapsed":1684207,"user":{"displayName":"Neil Fabião","userId":"06437641205793281458"}}},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Processing file of length 100\n","finished run 0\n","finished run 1\n","finished run 2\n"]}]},{"cell_type":"code","source":["results_unigram = {}\n","results_bigram = {}\n","results_trigram = {}\n","\n","name_1 ='Sentence_Completion vanilla_unigram_NormaltW3_100'\n","name_2 ='Sentence_Completion vanilla_bigram_NormalW3_100'\n","name_3 ='Sentence_Completion vanilla_trigram_NormalW3_100'\n","number_of_runs=3\n","\n","#sentence completion vanilla is the first step to understand the best length of file.\n","runs = 0\n","size = 100\n","\n","print(\"Processing file of length {}\".format(size))\n","\n","for i in range(number_of_runs):\n","\n","    mylm=language_model(trainingdir=TRAINING_DIR,files=training[:size])#loading the data from the files\n","\n","    SCC = scc_reader(qs=questions,ans=answers)#computing the questions and answers given by the model\n","\n","    accuracy_1 = SCC.predict_and_score(method=\"unigram\")\n","    accuracy_2 = SCC.predict_and_score(method=\"bigram\")\n","    accuracy_3 = SCC.predict_and_score(method=\"trigram\")\n","\n","    res_unigram[name_1]=res_unigram.get(name_1,0)+accuracy_1/number_of_runs\n","    res_bigram[name_2]=res_bigram.get(name_2,0)+accuracy_2/number_of_runs\n","    res_trigram[name_3]=res_trigram.get(name_3,0)+accuracy_3/number_of_runs\n","    print('finished run',runs)\n","    runs+=1\n","    \n","#computing perplexity for each n-gram\n","perplexity_1 = mylm.compute_perplexity(methodparams={\"method\":\"unigram\",\"smoothing\":\"kneser-ney\"})\n","perplexity_2 = mylm.compute_perplexity(methodparams={\"method\":\"bigram\",\"smoothing\":\"kneser-ney\"})\n","perplexity_3 = mylm.compute_perplexity(methodparams={\"method\":\"trigram\",\"smoothing\":\"kneser-ney\"})\n","\n","results_unigram[size]=[res_unigram,perplexity_1]\n","results_bigram[size]=[res_bigram,perplexity_2]\n","results_trigram[size]=[res_trigram,perplexity_3]\n","\n","#open a file, where you ant to store the data\n","# opening and dump information to file\n","with open('Sentence_Completion_vanilla_unigram_right_NormalW3_100.pickle', 'wb') as f:\n","    pickle.dump(results_unigram, f)\n","    \n","with open('Sentence_Completion_vanilla_bigram_right_NormalW3_100.pickle', 'wb') as f:\n","    pickle.dump(results_bigram, f)\n","\n","with open('Sentence_Completion_vanilla_trigram_right_NormalW3_100.pickle', 'wb') as f:\n","    pickle.dump(results_trigram, f)\n","\n","runs=0"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"v7-hJ1yCv-GX","executionInfo":{"status":"ok","timestamp":1651045262196,"user_tz":-60,"elapsed":1676696,"user":{"displayName":"Neil Fabião","userId":"06437641205793281458"}},"outputId":"48715bc2-f3da-474b-cee0-3d848b210f02"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Processing file of length 100\n","finished run 0\n","finished run 1\n","finished run 2\n"]}]},{"cell_type":"markdown","source":["### 3 Reading the data "],"metadata":{"id":"VuoBQjwGaih3"}},{"cell_type":"code","source":["ls"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"IVN1ABReat9v","executionInfo":{"status":"ok","timestamp":1651045355417,"user_tz":-60,"elapsed":773,"user":{"displayName":"Neil Fabião","userId":"06437641205793281458"}},"outputId":"580bbc81-1d5d-497a-b5d7-771465bb7798"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Baseline_Uni_TrigramCode25095-ColabPro.ipynb\n","Sentence_Completion_vanilla_bigram_file_len225.pickle\n","Sentence_Completion_vanilla_bigram_right_leftW2_100.pickle\n","Sentence_Completion_vanilla_bigram_right_leftW3_100.pickle\n","Sentence_Completion_vanilla_bigram_right_NormalW2_100.pickle\n","Sentence_Completion_vanilla_bigram_right_NormalW3_100.pickle\n","Sentence_Completion_vanilla_bigram_right_rightW2_100.pickle\n","Sentence_Completion_vanilla_bigram_right_rightW3_100.pickle\n","Sentence_Completion_vanilla_trigram_file_len225.pickle\n","Sentence_Completion_vanilla_trigram_right_leftW2_100.pickle\n","Sentence_Completion_vanilla_trigram_right_leftW3_100.pickle\n","Sentence_Completion_vanilla_trigram_right_NormalW2_100.pickle\n","Sentence_Completion_vanilla_trigram_right_NormalW3_100.pickle\n","Sentence_Completion_vanilla_trigram_right_rightW2_100.pickle\n","Sentence_Completion_vanilla_trigram_right_rightW3_100.pickle\n","Sentence_Completion_vanilla_unigram_file_len225.pickle\n","Sentence_Completion_vanilla_unigram_right_leftW2_100.pickle\n","Sentence_Completion_vanilla_unigram_right_leftW3_100.pickle\n","Sentence_Completion_vanilla_unigram_right_NormalW2_100.pickle\n","Sentence_Completion_vanilla_unigram_right_NormalW3_100.pickle\n","Sentence_Completion_vanilla_unigram_right_rightW2_100.pickle\n","Sentence_Completion_vanilla_unigram_right_rightW3_100.pickle\n"]}]},{"cell_type":"code","source":["file = open(\"Sentence_Completion_vanilla_unigram_right_NormalW3_100.pickle\",'rb')\n","object_file = pickle.load(file)\n","file.close()\n","object_file"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"A1cceHk_bKwM","executionInfo":{"status":"ok","timestamp":1651045356605,"user_tz":-60,"elapsed":6,"user":{"displayName":"Neil Fabião","userId":"06437641205793281458"}},"outputId":"8550d8e9-0e04-4a20-e984-d92ed4b8e10e"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["{100: [{'Sentence_Completion vanilla_unigram_5_225': 0.2548076923076923,\n","   'Sentence_Completion vanilla_unigram_NormaltW2_100': 0.2580128205128205,\n","   'Sentence_Completion vanilla_unigram_NormaltW3_100': 0.25833333333333336,\n","   'Sentence_Completion vanilla_unigram_leftW2_100': 0.2580128205128205,\n","   'Sentence_Completion vanilla_unigram_leftW3_100': 0.25833333333333336,\n","   'Sentence_Completion vanilla_unigram_rightW2_100': 0.19391025641025642,\n","   'Sentence_Completion vanilla_unigram_rightW3_100': 0.20384615384615384},\n","  715.2507506897674]}"]},"metadata":{},"execution_count":144}]},{"cell_type":"code","source":["file = open(\"Sentence_Completion_vanilla_bigram_right_NormalW3_100.pickle\",'rb')\n","object_file = pickle.load(file)\n","file.close()\n","object_file"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"4IRzPwFrayY0","executionInfo":{"status":"ok","timestamp":1651045358170,"user_tz":-60,"elapsed":277,"user":{"displayName":"Neil Fabião","userId":"06437641205793281458"}},"outputId":"b43c3726-da59-4703-cf99-43b7c2dbfee0"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["{100: [{'Sentence_Completion vanilla_bigram_5_225': 0.28173076923076923,\n","   'Sentence_Completion vanilla_bigram_NormalW2_100': 0.27307692307692305,\n","   'Sentence_Completion vanilla_bigram_NormalW3_100': 0.27307692307692305,\n","   'Sentence_Completion vanilla_bigram_leftW2_100': 0.2701923076923077,\n","   'Sentence_Completion vanilla_bigram_leftW3_100': 0.2701923076923077,\n","   'Sentence_Completion vanilla_bigram_rightW2_100': 0.20961538461538462,\n","   'Sentence_Completion vanilla_bigram_rightW3_100': 0.20961538461538462},\n","  134.19058325485688]}"]},"metadata":{},"execution_count":145}]},{"cell_type":"code","source":["file = open(\"Sentence_Completion_vanilla_trigram_right_NormalW3_100.pickle\",'rb')\n","object_file = pickle.load(file)\n","file.close()\n","object_file"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"qsZ7hPBIbU8d","executionInfo":{"status":"ok","timestamp":1651045358558,"user_tz":-60,"elapsed":5,"user":{"displayName":"Neil Fabião","userId":"06437641205793281458"}},"outputId":"e580ad65-6507-46eb-a20b-dfc655387396"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["{100: [{'Sentence_Completion vanilla_trigram_5_225': 0.2846153846153846,\n","   'Sentence_Completion vanilla_trigram_NormalW2_100': 0.2692307692307692,\n","   'Sentence_Completion vanilla_trigram_NormalW3_100': 0.27403846153846156,\n","   'Sentence_Completion vanilla_trigram_leftW2_100': 0.2644230769230769,\n","   'Sentence_Completion vanilla_trigram_leftW3_100': 0.26634615384615384,\n","   'Sentence_Completion vanilla_trigram_rightW2_100': 0.20865384615384616,\n","   'Sentence_Completion vanilla_trigram_rightW3_100': 0.20865384615384616},\n","  193.7862955533785]}"]},"metadata":{},"execution_count":146}]},{"cell_type":"code","source":["file = open(\"Sentence_Completion_vanilla_unigram_file_len225.pickle\",'rb')\n","object_file = pickle.load(file)\n","file.close()\n","object_file"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"VDPyrhy6bjBs","executionInfo":{"status":"ok","timestamp":1651045360069,"user_tz":-60,"elapsed":7,"user":{"displayName":"Neil Fabião","userId":"06437641205793281458"}},"outputId":"66b86e71-6ee0-4b9e-be95-ed222e12a983"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["{5: [{'Sentence_Completion vanilla_unigram_5_225': 0.28365384615384615},\n","  573.8737834886357],\n"," 10: [{'Sentence_Completion vanilla_unigram_5_225': 0.2503205128205128},\n","  638.2420476803173],\n"," 50: [{'Sentence_Completion vanilla_unigram_5_225': 0.2551282051282051},\n","  669.2677807643289],\n"," 75: [{'Sentence_Completion vanilla_unigram_5_225': 0.25993589743589746},\n","  679.8450453955679],\n"," 100: [{'Sentence_Completion vanilla_unigram_5_225': 0.25833333333333336},\n","  715.2507506897674],\n"," 125: [{'Sentence_Completion vanilla_unigram_5_225': 0.25576923076923075},\n","  725.0357531186069],\n"," 150: [{'Sentence_Completion vanilla_unigram_5_225': 0.2548076923076923},\n","  724.0925795717693],\n"," 175: [{'Sentence_Completion vanilla_unigram_5_225': 0.2528846153846154},\n","  733.6932929945617],\n"," 200: [{'Sentence_Completion vanilla_unigram_5_225': 0.25576923076923075},\n","  743.1415424490483],\n"," 225: [{'Sentence_Completion vanilla_unigram_5_225': 0.2548076923076923},\n","  747.2611725369585]}"]},"metadata":{},"execution_count":147}]},{"cell_type":"code","source":["ls"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"eCK__itPWR5r","executionInfo":{"status":"ok","timestamp":1651034894331,"user_tz":-60,"elapsed":638,"user":{"displayName":"Neil Fabião","userId":"06437641205793281458"}},"outputId":"723166b7-ff88-4a00-ba58-3657697b3e4f"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Baseline_Uni_TrigramCode25095-ColabPro.ipynb\n","Sentence_Completion_vanilla_bigram_file_len225.pickle\n","Sentence_Completion_vanilla_bigram_right_leftW2_100.pickle\n","Sentence_Completion_vanilla_bigram_right_leftW3_100.pickle\n","Sentence_Completion_vanilla_bigram_right_NormalW2_100.pickle\n","Sentence_Completion_vanilla_bigram_right_NormalW3_100.pickle\n","Sentence_Completion_vanilla_bigram_right_rightW2_100.pickle\n","Sentence_Completion_vanilla_bigram_right_rightW3_100.pickle\n","Sentence_Completion_vanilla_trigram_file_len225.pickle\n","Sentence_Completion_vanilla_trigram_right_leftW2_100.pickle\n","Sentence_Completion_vanilla_trigram_right_leftW3_100.pickle\n","Sentence_Completion_vanilla_trigram_right_NormalW2_100.pickle\n","Sentence_Completion_vanilla_trigram_right_NormalW3_100.pickle\n","Sentence_Completion_vanilla_trigram_right_rightW2_100.pickle\n","Sentence_Completion_vanilla_trigram_right_rightW3_100.pickle\n","Sentence_Completion_vanilla_unigram_file_len225.pickle\n","Sentence_Completion_vanilla_unigram_right_leftW2_100.pickle\n","Sentence_Completion_vanilla_unigram_right_leftW3_100.pickle\n","Sentence_Completion_vanilla_unigram_right_NormalW2_100.pickle\n","Sentence_Completion_vanilla_unigram_right_NormalW3_100.pickle\n","Sentence_Completion_vanilla_unigram_right_rightW2_100.pickle\n","Sentence_Completion_vanilla_unigram_right_rightW3_100.pickle\n"]}]},{"cell_type":"code","source":["\n","\n","file = open(\"Sentence_Completion_vanilla_trigram_file_len225.pickle\",'rb')\n","object_file = pickle.load(file)\n","file.close()\n","object_file"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"qRENHOTIb2wA","executionInfo":{"status":"ok","timestamp":1651034952911,"user_tz":-60,"elapsed":338,"user":{"displayName":"Neil Fabião","userId":"06437641205793281458"}},"outputId":"7302708b-453a-4271-af15-37d52f3bb43d"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["{5: [{'Sentence_Completion vanilla_trigram_5_225': 0.2766025641025641},\n","  118.56727661853265],\n"," 10: [{'Sentence_Completion vanilla_trigram_5_225': 0.25608974358974357},\n","  137.970096016552],\n"," 50: [{'Sentence_Completion vanilla_trigram_5_225': 0.28173076923076923},\n","  167.17674804386803],\n"," 75: [{'Sentence_Completion vanilla_trigram_5_225': 0.28685897435897434},\n","  174.64438793975313],\n"," 100: [{'Sentence_Completion vanilla_trigram_5_225': 0.26634615384615384},\n","  193.7862955533785],\n"," 125: [{'Sentence_Completion vanilla_trigram_5_225': 0.2701923076923077},\n","  202.98960397658175],\n"," 150: [{'Sentence_Completion vanilla_trigram_5_225': 0.2618589743589744},\n","  206.302004871104],\n"," 175: [{'Sentence_Completion vanilla_trigram_5_225': 0.27115384615384613},\n","  212.5296252853717],\n"," 200: [{'Sentence_Completion vanilla_trigram_5_225': 0.27115384615384613},\n","  218.05271354586284],\n"," 225: [{'Sentence_Completion vanilla_trigram_5_225': 0.2846153846153846},\n","  221.54607151657177]}"]},"metadata":{},"execution_count":124}]},{"cell_type":"code","source":["file = open(\"Sentence_Completion_vanilla_bigram_file_len225.pickle\",'rb')\n","object_file = pickle.load(file)\n","file.close()\n","object_file"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"_8UVej1h-CDV","executionInfo":{"status":"ok","timestamp":1651034971096,"user_tz":-60,"elapsed":213,"user":{"displayName":"Neil Fabião","userId":"06437641205793281458"}},"outputId":"77c0ec18-a925-4598-8f24-03fd57b09355"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["{5: [{'Sentence_Completion vanilla_bigram_5_225': 0.2794871794871795},\n","  103.71212437773605],\n"," 10: [{'Sentence_Completion vanilla_bigram_5_225': 0.2448717948717949},\n","  114.15920565488275],\n"," 50: [{'Sentence_Completion vanilla_bigram_5_225': 0.28173076923076923},\n","  122.51580963433388],\n"," 75: [{'Sentence_Completion vanilla_bigram_5_225': 0.28878205128205126},\n","  124.61971587432365],\n"," 100: [{'Sentence_Completion vanilla_bigram_5_225': 0.2701923076923077},\n","  134.19058325485688],\n"," 125: [{'Sentence_Completion vanilla_bigram_5_225': 0.26826923076923076},\n","  138.10647939469212],\n"," 150: [{'Sentence_Completion vanilla_bigram_5_225': 0.2637820512820513},\n","  138.36774251852324],\n"," 175: [{'Sentence_Completion vanilla_bigram_5_225': 0.275},\n","  140.55454556789525],\n"," 200: [{'Sentence_Completion vanilla_bigram_5_225': 0.2708333333333333},\n","  142.5508457013068],\n"," 225: [{'Sentence_Completion vanilla_bigram_5_225': 0.28173076923076923},\n","  143.79326434187826]}"]},"metadata":{},"execution_count":125}]},{"cell_type":"code","source":["file = open(\"Sentence_Completion_vanilla_unigram_file_len225.pickle\",'rb')\n","object_file = pickle.load(file)\n","file.close()\n","object_file"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"3zW0HiWeWlfr","executionInfo":{"status":"ok","timestamp":1651034979248,"user_tz":-60,"elapsed":364,"user":{"displayName":"Neil Fabião","userId":"06437641205793281458"}},"outputId":"6a044e79-4d1c-435e-99c4-c065df04dd51"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["{5: [{'Sentence_Completion vanilla_unigram_5_225': 0.28365384615384615},\n","  573.8737834886357],\n"," 10: [{'Sentence_Completion vanilla_unigram_5_225': 0.2503205128205128},\n","  638.2420476803173],\n"," 50: [{'Sentence_Completion vanilla_unigram_5_225': 0.2551282051282051},\n","  669.2677807643289],\n"," 75: [{'Sentence_Completion vanilla_unigram_5_225': 0.25993589743589746},\n","  679.8450453955679],\n"," 100: [{'Sentence_Completion vanilla_unigram_5_225': 0.25833333333333336},\n","  715.2507506897674],\n"," 125: [{'Sentence_Completion vanilla_unigram_5_225': 0.25576923076923075},\n","  725.0357531186069],\n"," 150: [{'Sentence_Completion vanilla_unigram_5_225': 0.2548076923076923},\n","  724.0925795717693],\n"," 175: [{'Sentence_Completion vanilla_unigram_5_225': 0.2528846153846154},\n","  733.6932929945617],\n"," 200: [{'Sentence_Completion vanilla_unigram_5_225': 0.25576923076923075},\n","  743.1415424490483],\n"," 225: [{'Sentence_Completion vanilla_unigram_5_225': 0.2548076923076923},\n","  747.2611725369585]}"]},"metadata":{},"execution_count":126}]},{"cell_type":"code","source":["results_unigram"],"metadata":{"id":"F9F2to6cZIav"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["results_bigram"],"metadata":{"id":"eKplKwclC71B"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["results_trigram"],"metadata":{"id":"95h5kUNZC9iI"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[""],"metadata":{"id":"Y8bDp7TBC_TO"},"execution_count":null,"outputs":[]}],"metadata":{"colab":{"collapsed_sections":[],"name":"Baseline_Uni_TrigramCode25095-ColabPro.ipynb","provenance":[],"machine_shape":"hm"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.7"}},"nbformat":4,"nbformat_minor":0}