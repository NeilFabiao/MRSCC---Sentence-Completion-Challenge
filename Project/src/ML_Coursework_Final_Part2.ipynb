{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Submit_ML_Coursework_Final_Part2.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# 0. Welcome <a name=\"Welcome\"></a>\n",
        "\n",
        "- $Candidate Number:$ 250945\n",
        "- $Date:$    Spring 2021\n",
        "\n",
        "In this notebook, we are going to be looking at the fairness library AIF360 for the 934G5: Machine Learning Coursework.\n",
        "\n",
        "Moreover, the Submit_ML_Coursework_Final_Part2 demostrates techniques and scenarios which supplement what was learned during the ML course. These include :\n",
        "- Fairness methods beyond binary sensitive features and in processing algorithms such as AdversarialDebiasing\n",
        "- when sensitive attribute is a feature of the main input X and when\n",
        "we exclude it from the features of X\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "7uYK3O_zkhda"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ypr1TX6tkHal",
        "outputId": "acb2c666-abf7-421d-d91e-5ee544b25111"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting aif360[LFR]\n",
            "  Downloading aif360-0.4.0-py3-none-any.whl (175 kB)\n",
            "\u001b[?25l\r\u001b[K     |█▉                              | 10 kB 30.3 MB/s eta 0:00:01\r\u001b[K     |███▊                            | 20 kB 9.2 MB/s eta 0:00:01\r\u001b[K     |█████▋                          | 30 kB 8.1 MB/s eta 0:00:01\r\u001b[K     |███████▌                        | 40 kB 7.6 MB/s eta 0:00:01\r\u001b[K     |█████████▍                      | 51 kB 4.2 MB/s eta 0:00:01\r\u001b[K     |███████████▎                    | 61 kB 5.0 MB/s eta 0:00:01\r\u001b[K     |█████████████                   | 71 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |███████████████                 | 81 kB 5.5 MB/s eta 0:00:01\r\u001b[K     |████████████████▉               | 92 kB 6.1 MB/s eta 0:00:01\r\u001b[K     |██████████████████▊             | 102 kB 5.1 MB/s eta 0:00:01\r\u001b[K     |████████████████████▋           | 112 kB 5.1 MB/s eta 0:00:01\r\u001b[K     |██████████████████████▌         | 122 kB 5.1 MB/s eta 0:00:01\r\u001b[K     |████████████████████████▎       | 133 kB 5.1 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▏     | 143 kB 5.1 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████    | 153 kB 5.1 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████  | 163 kB 5.1 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▉| 174 kB 5.1 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 175 kB 5.1 MB/s \n",
            "\u001b[33mWARNING: aif360 0.4.0 does not provide the extra 'lfr'\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pandas>=0.24.0 in /usr/local/lib/python3.7/dist-packages (from aif360[LFR]) (1.3.5)\n",
            "Requirement already satisfied: numpy>=1.16 in /usr/local/lib/python3.7/dist-packages (from aif360[LFR]) (1.21.6)\n",
            "Requirement already satisfied: scipy<1.6.0,>=1.2.0 in /usr/local/lib/python3.7/dist-packages (from aif360[LFR]) (1.4.1)\n",
            "Collecting tempeh\n",
            "  Downloading tempeh-0.1.12-py3-none-any.whl (39 kB)\n",
            "Requirement already satisfied: scikit-learn>=0.22.1 in /usr/local/lib/python3.7/dist-packages (from aif360[LFR]) (1.0.2)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.7/dist-packages (from aif360[LFR]) (3.2.2)\n",
            "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.7/dist-packages (from pandas>=0.24.0->aif360[LFR]) (2022.1)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas>=0.24.0->aif360[LFR]) (2.8.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.7.3->pandas>=0.24.0->aif360[LFR]) (1.15.0)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=0.22.1->aif360[LFR]) (1.1.0)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=0.22.1->aif360[LFR]) (3.1.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->aif360[LFR]) (1.4.2)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->aif360[LFR]) (3.0.8)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib->aif360[LFR]) (0.11.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from kiwisolver>=1.0.1->matplotlib->aif360[LFR]) (4.2.0)\n",
            "Collecting memory-profiler\n",
            "  Downloading memory_profiler-0.60.0.tar.gz (38 kB)\n",
            "Requirement already satisfied: pytest in /usr/local/lib/python3.7/dist-packages (from tempeh->aif360[LFR]) (3.6.4)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from tempeh->aif360[LFR]) (2.23.0)\n",
            "Collecting shap\n",
            "  Downloading shap-0.40.0-cp37-cp37m-manylinux2010_x86_64.whl (564 kB)\n",
            "\u001b[K     |████████████████████████████████| 564 kB 59.4 MB/s \n",
            "\u001b[?25hRequirement already satisfied: psutil in /usr/local/lib/python3.7/dist-packages (from memory-profiler->tempeh->aif360[LFR]) (5.4.8)\n",
            "Requirement already satisfied: pluggy<0.8,>=0.5 in /usr/local/lib/python3.7/dist-packages (from pytest->tempeh->aif360[LFR]) (0.7.1)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from pytest->tempeh->aif360[LFR]) (57.4.0)\n",
            "Requirement already satisfied: py>=1.5.0 in /usr/local/lib/python3.7/dist-packages (from pytest->tempeh->aif360[LFR]) (1.11.0)\n",
            "Requirement already satisfied: more-itertools>=4.0.0 in /usr/local/lib/python3.7/dist-packages (from pytest->tempeh->aif360[LFR]) (8.12.0)\n",
            "Requirement already satisfied: atomicwrites>=1.0 in /usr/local/lib/python3.7/dist-packages (from pytest->tempeh->aif360[LFR]) (1.4.0)\n",
            "Requirement already satisfied: attrs>=17.4.0 in /usr/local/lib/python3.7/dist-packages (from pytest->tempeh->aif360[LFR]) (21.4.0)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->tempeh->aif360[LFR]) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->tempeh->aif360[LFR]) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->tempeh->aif360[LFR]) (2021.10.8)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->tempeh->aif360[LFR]) (1.24.3)\n",
            "Requirement already satisfied: cloudpickle in /usr/local/lib/python3.7/dist-packages (from shap->tempeh->aif360[LFR]) (1.3.0)\n",
            "Collecting slicer==0.0.7\n",
            "  Downloading slicer-0.0.7-py3-none-any.whl (14 kB)\n",
            "Requirement already satisfied: numba in /usr/local/lib/python3.7/dist-packages (from shap->tempeh->aif360[LFR]) (0.51.2)\n",
            "Requirement already satisfied: packaging>20.9 in /usr/local/lib/python3.7/dist-packages (from shap->tempeh->aif360[LFR]) (21.3)\n",
            "Requirement already satisfied: tqdm>4.25.0 in /usr/local/lib/python3.7/dist-packages (from shap->tempeh->aif360[LFR]) (4.64.0)\n",
            "Requirement already satisfied: llvmlite<0.35,>=0.34.0.dev0 in /usr/local/lib/python3.7/dist-packages (from numba->shap->tempeh->aif360[LFR]) (0.34.0)\n",
            "Building wheels for collected packages: memory-profiler\n",
            "  Building wheel for memory-profiler (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for memory-profiler: filename=memory_profiler-0.60.0-py3-none-any.whl size=31284 sha256=a105d90d70ac7161a57fac5c08fc52f0c81a970a3716cf6755e587bae39af9da\n",
            "  Stored in directory: /root/.cache/pip/wheels/67/2b/fb/326e30d638c538e69a5eb0aa47f4223d979f502bbdb403950f\n",
            "Successfully built memory-profiler\n",
            "Installing collected packages: slicer, shap, memory-profiler, tempeh, aif360\n",
            "Successfully installed aif360-0.4.0 memory-profiler-0.60.0 shap-0.40.0 slicer-0.0.7 tempeh-0.1.12\n"
          ]
        }
      ],
      "source": [
        "!pip install 'aif360[LFR]'"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "cd /usr/local/lib/python3.7/dist-packages/aif360/data/raw/adult"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FFIpGFwRkp3X",
        "outputId": "498a1e70-e4bd-407e-84b5-a4d8db2f2e06"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/aif360/data/raw/adult\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://archive.ics.uci.edu/ml/machine-learning-databases/adult/adult.data\n",
        "!wget https://archive.ics.uci.edu/ml/machine-learning-databases/adult/adult.names\n",
        "!wget https://archive.ics.uci.edu/ml/machine-learning-databases/adult/adult.test"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NulTGiKTf6yY",
        "outputId": "7a15899b-a772-4286-a2e8-9a002f1885c0"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2022-05-13 13:20:02--  https://archive.ics.uci.edu/ml/machine-learning-databases/adult/adult.data\n",
            "Resolving archive.ics.uci.edu (archive.ics.uci.edu)... 128.195.10.252\n",
            "Connecting to archive.ics.uci.edu (archive.ics.uci.edu)|128.195.10.252|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 3974305 (3.8M) [application/x-httpd-php]\n",
            "Saving to: ‘adult.data’\n",
            "\n",
            "adult.data          100%[===================>]   3.79M  7.23MB/s    in 0.5s    \n",
            "\n",
            "2022-05-13 13:20:03 (7.23 MB/s) - ‘adult.data’ saved [3974305/3974305]\n",
            "\n",
            "--2022-05-13 13:20:03--  https://archive.ics.uci.edu/ml/machine-learning-databases/adult/adult.names\n",
            "Resolving archive.ics.uci.edu (archive.ics.uci.edu)... 128.195.10.252\n",
            "Connecting to archive.ics.uci.edu (archive.ics.uci.edu)|128.195.10.252|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 5229 (5.1K) [application/x-httpd-php]\n",
            "Saving to: ‘adult.names’\n",
            "\n",
            "adult.names         100%[===================>]   5.11K  --.-KB/s    in 0s      \n",
            "\n",
            "2022-05-13 13:20:03 (196 MB/s) - ‘adult.names’ saved [5229/5229]\n",
            "\n",
            "--2022-05-13 13:20:03--  https://archive.ics.uci.edu/ml/machine-learning-databases/adult/adult.test\n",
            "Resolving archive.ics.uci.edu (archive.ics.uci.edu)... 128.195.10.252\n",
            "Connecting to archive.ics.uci.edu (archive.ics.uci.edu)|128.195.10.252|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 2003153 (1.9M) [application/x-httpd-php]\n",
            "Saving to: ‘adult.test’\n",
            "\n",
            "adult.test          100%[===================>]   1.91M  4.19MB/s    in 0.5s    \n",
            "\n",
            "2022-05-13 13:20:04 (4.19 MB/s) - ‘adult.test’ saved [2003153/2003153]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "cd /usr/local/lib/python3.7/dist-packages/aif360/data/raw/german"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ToI84kj1f64q",
        "outputId": "96694258-a583-496e-b0f7-428ec025a6f0"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/aif360/data/raw/german\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://archive.ics.uci.edu/ml/machine-learning-databases/statlog/german/german.data\n",
        "!wget https://archive.ics.uci.edu/ml/machine-learning-databases/statlog/german/german.doc"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UzTMcV3564oI",
        "outputId": "0079a775-f00f-45c7-954b-b8f6ba77d3c6"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2022-05-13 13:20:04--  https://archive.ics.uci.edu/ml/machine-learning-databases/statlog/german/german.data\n",
            "Resolving archive.ics.uci.edu (archive.ics.uci.edu)... 128.195.10.252\n",
            "Connecting to archive.ics.uci.edu (archive.ics.uci.edu)|128.195.10.252|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 79793 (78K) [application/x-httpd-php]\n",
            "Saving to: ‘german.data’\n",
            "\n",
            "german.data         100%[===================>]  77.92K  --.-KB/s    in 0.1s    \n",
            "\n",
            "2022-05-13 13:20:05 (598 KB/s) - ‘german.data’ saved [79793/79793]\n",
            "\n",
            "--2022-05-13 13:20:05--  https://archive.ics.uci.edu/ml/machine-learning-databases/statlog/german/german.doc\n",
            "Resolving archive.ics.uci.edu (archive.ics.uci.edu)... 128.195.10.252\n",
            "Connecting to archive.ics.uci.edu (archive.ics.uci.edu)|128.195.10.252|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 4679 (4.6K) [application/x-httpd-php]\n",
            "Saving to: ‘german.doc’\n",
            "\n",
            "german.doc          100%[===================>]   4.57K  --.-KB/s    in 0s      \n",
            "\n",
            "2022-05-13 13:20:05 (181 MB/s) - ‘german.doc’ saved [4679/4679]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! pip install \"fairlearn\"\n",
        "!pip install BlackBoxAuditing"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Jd9O-KLQk-fo",
        "outputId": "2f0e0c89-6288-4e03-f3be-305660490e5c"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting fairlearn\n",
            "  Downloading fairlearn-0.7.0-py3-none-any.whl (177 kB)\n",
            "\u001b[?25l\r\u001b[K     |█▉                              | 10 kB 34.2 MB/s eta 0:00:01\r\u001b[K     |███▊                            | 20 kB 10.0 MB/s eta 0:00:01\r\u001b[K     |█████▌                          | 30 kB 8.6 MB/s eta 0:00:01\r\u001b[K     |███████▍                        | 40 kB 4.3 MB/s eta 0:00:01\r\u001b[K     |█████████▎                      | 51 kB 4.3 MB/s eta 0:00:01\r\u001b[K     |███████████                     | 61 kB 5.1 MB/s eta 0:00:01\r\u001b[K     |█████████████                   | 71 kB 5.5 MB/s eta 0:00:01\r\u001b[K     |██████████████▊                 | 81 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |████████████████▋               | 92 kB 6.0 MB/s eta 0:00:01\r\u001b[K     |██████████████████▌             | 102 kB 5.0 MB/s eta 0:00:01\r\u001b[K     |████████████████████▎           | 112 kB 5.0 MB/s eta 0:00:01\r\u001b[K     |██████████████████████▏         | 122 kB 5.0 MB/s eta 0:00:01\r\u001b[K     |████████████████████████        | 133 kB 5.0 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▉      | 143 kB 5.0 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▊    | 153 kB 5.0 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▌  | 163 kB 5.0 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▍| 174 kB 5.0 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 177 kB 5.0 MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.17.2 in /usr/local/lib/python3.7/dist-packages (from fairlearn) (1.21.6)\n",
            "Requirement already satisfied: scikit-learn>=0.22.1 in /usr/local/lib/python3.7/dist-packages (from fairlearn) (1.0.2)\n",
            "Requirement already satisfied: pandas>=0.25.1 in /usr/local/lib/python3.7/dist-packages (from fairlearn) (1.3.5)\n",
            "Requirement already satisfied: scipy>=1.4.1 in /usr/local/lib/python3.7/dist-packages (from fairlearn) (1.4.1)\n",
            "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.7/dist-packages (from pandas>=0.25.1->fairlearn) (2022.1)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas>=0.25.1->fairlearn) (2.8.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.7.3->pandas>=0.25.1->fairlearn) (1.15.0)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=0.22.1->fairlearn) (3.1.0)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=0.22.1->fairlearn) (1.1.0)\n",
            "Installing collected packages: fairlearn\n",
            "Successfully installed fairlearn-0.7.0\n",
            "Collecting BlackBoxAuditing\n",
            "  Downloading BlackBoxAuditing-0.1.54.tar.gz (2.6 MB)\n",
            "\u001b[K     |████████████████████████████████| 2.6 MB 5.1 MB/s \n",
            "\u001b[?25hRequirement already satisfied: networkx in /usr/local/lib/python3.7/dist-packages (from BlackBoxAuditing) (2.6.3)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.7/dist-packages (from BlackBoxAuditing) (3.2.2)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (from BlackBoxAuditing) (1.3.5)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from BlackBoxAuditing) (1.21.6)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->BlackBoxAuditing) (1.4.2)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->BlackBoxAuditing) (3.0.8)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->BlackBoxAuditing) (2.8.2)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib->BlackBoxAuditing) (0.11.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from kiwisolver>=1.0.1->matplotlib->BlackBoxAuditing) (4.2.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.1->matplotlib->BlackBoxAuditing) (1.15.0)\n",
            "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.7/dist-packages (from pandas->BlackBoxAuditing) (2022.1)\n",
            "Building wheels for collected packages: BlackBoxAuditing\n",
            "  Building wheel for BlackBoxAuditing (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for BlackBoxAuditing: filename=BlackBoxAuditing-0.1.54-py2.py3-none-any.whl size=1394770 sha256=e7da330fea2f50ad54a0257426b14efe0f91778a9f6a263af4f84eead8f4f0ef\n",
            "  Stored in directory: /root/.cache/pip/wheels/05/9f/ee/541a74be4cf5dad17430e64d3276370ea7b6a834a76cb4215a\n",
            "Successfully built BlackBoxAuditing\n",
            "Installing collected packages: BlackBoxAuditing\n",
            "Successfully installed BlackBoxAuditing-0.1.54\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Baseline code adapted from sections of V Sharmanska. \n",
        "#STEP 1: Import the necessary libraries for this notebook\n",
        "\n",
        "# Libraries to study in aif360\n",
        "from aif360.datasets import AdultDataset, GermanDataset, BankDataset\n",
        "from aif360.algorithms.preprocessing.optim_preproc_helpers.data_preproc_functions import load_preproc_data_adult,load_preproc_data_german\n",
        "from aif360.algorithms.preprocessing import DisparateImpactRemover,LFR\n",
        "from aif360.sklearn.datasets import fetch_adult,fetch_german\n",
        "from aif360.sklearn.metrics import disparate_impact_ratio, average_odds_error, generalized_fpr,equal_opportunity_difference,statistical_parity_difference\n",
        "from aif360.sklearn.preprocessing import ReweighingMeta,Reweighing\n",
        "from aif360.sklearn.inprocessing import AdversarialDebiasing,ExponentiatedGradientReduction\n",
        "from aif360.sklearn.postprocessing import CalibratedEqualizedOdds\n",
        "\n",
        "# Sklearn imports\n",
        "from sklearn.compose import make_column_transformer\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.preprocessing import StandardScaler,MinMaxScaler,OneHotEncoder\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.metrics import accuracy_score,classification_report\n",
        "from sklearn.model_selection import *\n",
        "from sklearn.metrics import confusion_matrix, accuracy_score, f1_score, roc_curve, auc\n",
        "\n",
        "# ML libraries\n",
        "import numpy as np\n",
        "import pickle\n",
        "import pdb\n",
        "import torch\n",
        "import tensorflow as tf\n",
        "import time\n",
        "from torch import optim\n",
        "from sklearn.model_selection import KFold\n",
        "from aif360.metrics import BinaryLabelDatasetMetric, ClassificationMetric\n",
        "from torch import nn\n",
        "from torch.nn import functional as F\n",
        "from typing import Callable\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Graphs libraries\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.patches as patches\n",
        "plt.style.use('seaborn-white')\n",
        "import seaborn as sns\n",
        "\n",
        "import plotly.offline as py\n",
        "py.init_notebook_mode(connected=True)\n",
        "import plotly.graph_objs as go\n",
        "import plotly.tools as tls\n",
        "import plotly.figure_factory as ff\n",
        "from plotly import tools\n",
        "from pprint import pprint\n",
        "\n",
        "# Design libraries\n",
        "from IPython.display import Markdown, display\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "np.random.seed(0)"
      ],
      "metadata": {
        "id": "vzmshfKJkyYZ",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "outputId": "f4db90c0-c28f-45e0-9d6f-d9519de827e1"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "        <script type=\"text/javascript\">\n",
              "        window.PlotlyConfig = {MathJaxConfig: 'local'};\n",
              "        if (window.MathJax) {MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}\n",
              "        if (typeof require !== 'undefined') {\n",
              "        require.undef(\"plotly\");\n",
              "        requirejs.config({\n",
              "            paths: {\n",
              "                'plotly': ['https://cdn.plot.ly/plotly-2.8.3.min']\n",
              "            }\n",
              "        });\n",
              "        require(['plotly'], function(Plotly) {\n",
              "            window._Plotly = Plotly;\n",
              "        });\n",
              "        }\n",
              "        </script>\n",
              "        "
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Part1: Beyond binary sensitive features\n",
        "\n",
        "In order to work with non binary sensitve features we need to reindex the number of columns we want to use. The baseline index which are normaly used in the binary cases are sex and race, which can be male and female or white and non whites (which encompass black, hispanic, asian, etc). For this reason when using non binary features such as occupation and relationship (as seen on the table bellow) one needs to pre process the features with reweigh.\n",
        "\n",
        "The drawback of using non sensitive features are the avaliable fairness metrics. Majority of the avaliable metrics mostly account for binary classifition. Altough there are some applications of group bias count examples avaliable. \n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "zclIiTzu1ldr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1.1 Multiclass sensitive feature relationship"
      ],
      "metadata": {
        "id": "BOU8W7mUAc9q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X, y, sample_weight = fetch_adult()#for this dataset the protected variables are race and sex\n",
        "X.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 364
        },
        "id": "3b73O2BY0nDT",
        "outputId": "5debd075-d22a-4307-8839-02ad0be61361"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                   age  workclass     education  education-num  \\\n",
              "  race      sex                                                  \n",
              "0 Non-white Male  25.0    Private          11th            7.0   \n",
              "1 White     Male  38.0    Private       HS-grad            9.0   \n",
              "2 White     Male  28.0  Local-gov    Assoc-acdm           12.0   \n",
              "3 Non-white Male  44.0    Private  Some-college           10.0   \n",
              "5 White     Male  34.0    Private          10th            6.0   \n",
              "\n",
              "                      marital-status         occupation   relationship  \\\n",
              "  race      sex                                                          \n",
              "0 Non-white Male       Never-married  Machine-op-inspct      Own-child   \n",
              "1 White     Male  Married-civ-spouse    Farming-fishing        Husband   \n",
              "2 White     Male  Married-civ-spouse    Protective-serv        Husband   \n",
              "3 Non-white Male  Married-civ-spouse  Machine-op-inspct        Husband   \n",
              "5 White     Male       Never-married      Other-service  Not-in-family   \n",
              "\n",
              "                       race   sex  capital-gain  capital-loss  hours-per-week  \\\n",
              "  race      sex                                                                 \n",
              "0 Non-white Male  Non-white  Male           0.0           0.0            40.0   \n",
              "1 White     Male      White  Male           0.0           0.0            50.0   \n",
              "2 White     Male      White  Male           0.0           0.0            40.0   \n",
              "3 Non-white Male  Non-white  Male        7688.0           0.0            40.0   \n",
              "5 White     Male      White  Male           0.0           0.0            30.0   \n",
              "\n",
              "                 native-country  \n",
              "  race      sex                  \n",
              "0 Non-white Male  United-States  \n",
              "1 White     Male  United-States  \n",
              "2 White     Male  United-States  \n",
              "3 Non-white Male  United-States  \n",
              "5 White     Male  United-States  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-fccc1221-dadb-41af-a374-2e64de5c67a5\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th>age</th>\n",
              "      <th>workclass</th>\n",
              "      <th>education</th>\n",
              "      <th>education-num</th>\n",
              "      <th>marital-status</th>\n",
              "      <th>occupation</th>\n",
              "      <th>relationship</th>\n",
              "      <th>race</th>\n",
              "      <th>sex</th>\n",
              "      <th>capital-gain</th>\n",
              "      <th>capital-loss</th>\n",
              "      <th>hours-per-week</th>\n",
              "      <th>native-country</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th></th>\n",
              "      <th>race</th>\n",
              "      <th>sex</th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <th>Non-white</th>\n",
              "      <th>Male</th>\n",
              "      <td>25.0</td>\n",
              "      <td>Private</td>\n",
              "      <td>11th</td>\n",
              "      <td>7.0</td>\n",
              "      <td>Never-married</td>\n",
              "      <td>Machine-op-inspct</td>\n",
              "      <td>Own-child</td>\n",
              "      <td>Non-white</td>\n",
              "      <td>Male</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>40.0</td>\n",
              "      <td>United-States</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <th>White</th>\n",
              "      <th>Male</th>\n",
              "      <td>38.0</td>\n",
              "      <td>Private</td>\n",
              "      <td>HS-grad</td>\n",
              "      <td>9.0</td>\n",
              "      <td>Married-civ-spouse</td>\n",
              "      <td>Farming-fishing</td>\n",
              "      <td>Husband</td>\n",
              "      <td>White</td>\n",
              "      <td>Male</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>50.0</td>\n",
              "      <td>United-States</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <th>White</th>\n",
              "      <th>Male</th>\n",
              "      <td>28.0</td>\n",
              "      <td>Local-gov</td>\n",
              "      <td>Assoc-acdm</td>\n",
              "      <td>12.0</td>\n",
              "      <td>Married-civ-spouse</td>\n",
              "      <td>Protective-serv</td>\n",
              "      <td>Husband</td>\n",
              "      <td>White</td>\n",
              "      <td>Male</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>40.0</td>\n",
              "      <td>United-States</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <th>Non-white</th>\n",
              "      <th>Male</th>\n",
              "      <td>44.0</td>\n",
              "      <td>Private</td>\n",
              "      <td>Some-college</td>\n",
              "      <td>10.0</td>\n",
              "      <td>Married-civ-spouse</td>\n",
              "      <td>Machine-op-inspct</td>\n",
              "      <td>Husband</td>\n",
              "      <td>Non-white</td>\n",
              "      <td>Male</td>\n",
              "      <td>7688.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>40.0</td>\n",
              "      <td>United-States</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <th>White</th>\n",
              "      <th>Male</th>\n",
              "      <td>34.0</td>\n",
              "      <td>Private</td>\n",
              "      <td>10th</td>\n",
              "      <td>6.0</td>\n",
              "      <td>Never-married</td>\n",
              "      <td>Other-service</td>\n",
              "      <td>Not-in-family</td>\n",
              "      <td>White</td>\n",
              "      <td>Male</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>30.0</td>\n",
              "      <td>United-States</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-fccc1221-dadb-41af-a374-2e64de5c67a5')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-fccc1221-dadb-41af-a374-2e64de5c67a5 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-fccc1221-dadb-41af-a374-2e64de5c67a5');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#reference: https://github.com/Trusted-AI/AIF360/blob/master/examples/sklearn/demo_new_features.ipynb and stackoverflow\n",
        "#adult dataset\n",
        "y = y.to_frame()#transforming the target variables from series to dataframe \n",
        "values = X['relationship'].tolist()\n",
        "y['relationship'] = values# adding the new non binary sensitive features\n",
        "y = y.set_index('relationship',append=True)#re indexing the target variable table with non binary sensitive features\n",
        "y = y.squeeze()#tranforming the dataframe back to series \n",
        "X = X.set_index('relationship',append=True).copy()#reindexing the main input with non binary sensitive features\n",
        "X['relationship'] = values\n",
        "X['relationship'] = X['relationship'].astype('category')#make sure this table is category to be used during encoding \n",
        "X.index = pd.MultiIndex.from_arrays(X.index.codes, names=X.index.names)\n",
        "y.index = pd.MultiIndex.from_arrays(y.index.codes, names=y.index.names)\n",
        "y = pd.Series(y.factorize(sort=True)[0], index=y.index)\n",
        "y.head()"
      ],
      "metadata": {
        "id": "csL2u-XNCgCe",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f9fba4b7-0df0-495a-d840-9de4b293dac9"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "   race  sex  relationship\n",
              "0  0     1    3               0\n",
              "1  1     1    0               0\n",
              "2  1     1    0               1\n",
              "3  0     1    0               1\n",
              "4  1     1    1               0\n",
              "dtype: int64"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#german dataset\n",
        "X_, y_ = fetch_german()#on the other hand this dataset has sex,age and foreign_worker as protected features\n",
        "X_.index = pd.MultiIndex.from_arrays(X_.index.codes, names=X_.index.names)\n",
        "y_.index = pd.MultiIndex.from_arrays(y_.index.codes, names=y_.index.names)\n",
        "y_ = pd.Series(y_.factorize(sort=True)[0], index=y_.index)\n",
        "y_.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gpSseHH2c6Kh",
        "outputId": "3abc6a6e-ec30-4e82-d254-f68e66a58662"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "   sex  age  foreign_worker\n",
              "0  1    1    1                 1\n",
              "1  0    0    1                 0\n",
              "2  1    1    1                 1\n",
              "3  1    1    1                 1\n",
              "4  1    1    1                 0\n",
              "dtype: int64"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#adult dataset\n",
        "y_ = y_.to_frame()#transforming the target variables from series to dataframe \n",
        "values = X_['job'].tolist()\n",
        "y_['job'] = values# adding the new non binary sensitive features\n",
        "y_ = y_.set_index('job',append=True)#re indexing the target variable table with non binary sensitive features\n",
        "y_ = y_.squeeze()#tranforming the dataframe back to series \n",
        "X_ = X_.set_index('job',append=True).copy()#reindexing the main input with non binary sensitive features\n",
        "X_['job'] = values\n",
        "X_['job'] = X_['job'].astype('category')#make sure this table is category to be used during encoding \n",
        "X_.index = pd.MultiIndex.from_arrays(X_.index.codes, names=X_.index.names)\n",
        "y_.index = pd.MultiIndex.from_arrays(y_.index.codes, names=y_.index.names)\n",
        "y_ = pd.Series(y_.factorize(sort=True)[0], index=y_.index)\n",
        "y_.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dS2C4nJgQZrl",
        "outputId": "ba9a8276-76c2-4866-dae5-0fc47c54190b"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "   sex  age  foreign_worker  job\n",
              "0  1    1    1               1      1\n",
              "1  0    0    1               1      0\n",
              "2  1    1    1               3      1\n",
              "3  1    1    1               1      1\n",
              "4  1    1    1               1      0\n",
              "dtype: int64"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "start = time.time()#this is to measure elapsed time\n",
        "\n",
        "import tensorflow.compat.v1 as tf\n",
        "tf.disable_v2_behavior()\n",
        "#tf.enable_eager_execution()\n",
        "tf.disable_eager_execution()\n",
        "\n",
        "kf = KFold(n_splits=5, random_state=41, shuffle=True)\n",
        "results_ = {}\n",
        "\n",
        "criterions = [0.00001,0.001,1,100,10000]\n",
        "\n",
        "for criterion in criterions:\n",
        "  #print(\"TRAIN:\", train_index, \"TEST:\", test_index)\n",
        "  print('\\n Welcome to Criterion number {} \\n'.format(criterion))\n",
        "  \n",
        "  #Step 1: get the criterion\n",
        "  for fold_num,(train_index, test_index) in enumerate(kf.split(X,y)):\n",
        "    \n",
        "    #Step 2: get the train and test fold subsets from the dataset\n",
        "    X_train_fold, X_test_fold = X.loc[train_index], X.loc[test_index]\n",
        "    y_train_fold, y_test_fold = y.loc[train_index], y.loc[test_index]\n",
        "\n",
        "    #Step 3: one hot encode the variables before fitting on the model\n",
        "    ohe = make_column_transformer(\n",
        "          (OneHotEncoder(sparse=False,handle_unknown='ignore'), X_train_fold.dtypes == 'category'),\n",
        "          remainder='passthrough')\n",
        "    X_train_fold  = pd.DataFrame(ohe.fit_transform(X_train_fold), index=X_train_fold.index)\n",
        "    X_test_fold = pd.DataFrame(ohe.transform(X_test_fold), index=X_test_fold.index)\n",
        "\n",
        "    #Step 4: run the adversarial debiasing classifier\n",
        "    learner = AdversarialDebiasing(prot_attr='relationship', random_state=1234567,adversary_loss_weight=criterion)\n",
        "    learner.fit(X_train_fold,y_train_fold)\n",
        "    predictions = learner.predict(X_test_fold)\n",
        "\n",
        "    model_acc = sum(predictions==y_test_fold)/len(y_test_fold)\n",
        "    test_pred = y.loc[test_index].copy()\n",
        "    test_pred = predictions\n",
        "    #Step 5: retrieve the metrics\n",
        "    metric_dis = disparate_impact_ratio(y_test_fold, learner.predict(X_test_fold), prot_attr='relationship')\n",
        "    metric_eqq = equal_opportunity_difference(y_test_fold, learner.predict(X_test_fold), prot_attr='relationship')\n",
        "    metric_stat = statistical_parity_difference(y_test_fold, learner.predict(X_test_fold), prot_attr='relationship')\n",
        "    #retrieving metrics from the classifier. The accuracies include percentage of true pred, statistical parity difference, disparate impact and equal opportunity.\n",
        "    result = {'Accuracy':model_acc,'statistical_parity_difference': metric_stat,'disparate_impact': metric_dis,'equal_opportunity_difference': metric_eqq}\n",
        "\n",
        "    results_[str(fold_num)+'AdversarialDebiasing_'+str(criterion)]=result#saving the results of the decision tree classifier \n",
        "\n",
        "results_occupation_add = results_.copy()\n",
        "end = time.time()\n",
        "total = end - start\n",
        "print('Total elapsed time was',total)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MdcCnnMk-rD_",
        "outputId": "736303fa-1427-4131-a33f-7fb5af8a589e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/tensorflow/python/compat/v2_compat.py:107: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "non-resource variables are not supported in the long term\n",
            "\n",
            " Welcome to Fold number 0 \n",
            "\n",
            "\n",
            " Welcome to Fold number 1 \n",
            "\n",
            "\n",
            " Welcome to Fold number 2 \n",
            "\n",
            "\n",
            " Welcome to Fold number 3 \n",
            "\n",
            "\n",
            " Welcome to Fold number 4 \n",
            "\n",
            "Total elapsed time was 915.2160284519196\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "31p7jteFpU6Z",
        "outputId": "159fa20e-6d64-401e-b0cb-709e5a73ed1d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! ls '/content/drive/MyDrive/UniSussex/ML'"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cDHYVOYTpHxn",
        "outputId": "f2ab079f-94cf-4fe5-8088-3a5a16b5786c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Submit_ML_Coursework_Final_Part2_final.ipynb\n",
            "Submit_ML_Coursework_Final_Part2.ipynb\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle\n",
        "#save results to google drive\n",
        "with open('/content/drive/MyDrive/UniSussex/ML/AdversarialDebiasing_relationshipfeature_.pickle', 'wb') as f:\n",
        "    pickle.dump(results_occupation_add, f)"
      ],
      "metadata": {
        "id": "S90ngRhVpCWA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "start = time.time()#this is to measure elapsed time\n",
        "\n",
        "import tensorflow.compat.v1 as tf\n",
        "tf.disable_v2_behavior()\n",
        "#tf.enable_eager_execution()\n",
        "tf.disable_eager_execution()\n",
        "\n",
        "kf = KFold(n_splits=5, random_state=41, shuffle=True)\n",
        "results_ = {}\n",
        "\n",
        "criterions = [0.00001,0.001,1,100,10000]\n",
        "\n",
        "for criterion in criterions:\n",
        "  #Step 1: get the criterion for this particular run\n",
        "  #print(\"TRAIN:\", train_index, \"TEST:\", test_index)\n",
        "  print('\\n Welcome to Criterion number {} \\n'.format(criterion))\n",
        "\n",
        "  for fold_num,(train_index, test_index) in enumerate(kf.split(X_,y_)):\n",
        "    #Step 2: get the train and test fold subsets from the dataset\n",
        "    X_train_fold, X_test_fold = X_.loc[train_index], X_.loc[test_index]\n",
        "    y_train_fold, y_test_fold = y_.loc[train_index], y_.loc[test_index]\n",
        "    #Step 3: one hot encode the variables before fitting on the model\n",
        "    ohe = make_column_transformer(\n",
        "          (OneHotEncoder(sparse=False,handle_unknown='ignore'), X_train_fold.dtypes == 'category'),\n",
        "          remainder='passthrough')\n",
        "    X_train_fold  = pd.DataFrame(ohe.fit_transform(X_train_fold), index=X_train_fold.index)\n",
        "    X_test_fold = pd.DataFrame(ohe.transform(X_test_fold), index=X_test_fold.index)\n",
        "    #Step 4: run the adversarial debiasing classifier\n",
        "    learner = AdversarialDebiasing(prot_attr='job', random_state=1234567,adversary_loss_weight=criterion)\n",
        "    learner.fit(X_train_fold,y_train_fold)\n",
        "    predictions = learner.predict(X_test_fold)\n",
        "\n",
        "    model_acc = sum(predictions==y_test_fold)/len(y_test_fold)\n",
        "    test_pred = y.loc[test_index].copy()\n",
        "    test_pred = predictions\n",
        "    #Step 5: retrieve the metrics\n",
        "    metric_dis = disparate_impact_ratio(y_test_fold, learner.predict(X_test_fold), prot_attr='job')\n",
        "    metric_eqq = equal_opportunity_difference(y_test_fold, learner.predict(X_test_fold), prot_attr='job')\n",
        "    metric_stat = statistical_parity_difference(y_test_fold, learner.predict(X_test_fold), prot_attr='job')\n",
        "    #retrieving metrics from the classifier. The accuracies include percentage of true pred, statistical parity difference, disparate impact and equal opportunity.\n",
        "    result = {'Accuracy':model_acc,'statistical_parity_difference': metric_stat,'disparate_impact': metric_dis,'equal_opportunity_difference': metric_eqq}\n",
        "\n",
        "    results_[str(fold_num)+'AdversarialDebiasing_'+str(criterion)]=result#saving the results of the decision tree classifier \n",
        "\n",
        "results_occupation_add = results_.copy()\n",
        "\n",
        "import pickle\n",
        "#save results to google drive\n",
        "with open('/content/drive/MyDrive/UniSussex/ML/AdversarialDebiasing_job_.pickle', 'wb') as f:\n",
        "    pickle.dump(results_occupation_add, f)\n",
        "    \n",
        "end = time.time()\n",
        "total = end - start\n",
        "print('Total elapsed time was',total)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O_wrjkSTKaYk",
        "outputId": "62ad47e0-3993-49ec-ff08-bb8d57149761"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " Welcome to Fold number 0 \n",
            "\n",
            "\n",
            " Welcome to Fold number 1 \n",
            "\n",
            "\n",
            " Welcome to Fold number 2 \n",
            "\n",
            "\n",
            " Welcome to Fold number 3 \n",
            "\n",
            "\n",
            " Welcome to Fold number 4 \n",
            "\n",
            "Total elapsed time was 32.36254024505615\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1.2 Multiclass sensitive after Reweighing"
      ],
      "metadata": {
        "id": "KfVokg0SApx0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "start = time.time()#this is to measure elapsed time\n",
        "from aif360.algorithms.inprocessing import AdversarialDebiasing, PrejudiceRemover, GerryFairClassifier\n",
        "kf = KFold(n_splits=5, random_state=41, shuffle=True)\n",
        "results_ = {}\n",
        "\n",
        "# Different criterions\n",
        "criterions = ['gini', 'entropy']\n",
        "# Maximum number of levels in tree\n",
        "max_depth = [int(x) for x in np.linspace(2, 100, num = 5)]\n",
        "max_depth.append(None)\n",
        "\n",
        "for criterion_1 in criterions:\n",
        "  #Step 1: get the criterion\n",
        "  #print(\"TRAIN:\", train_index, \"TEST:\", test_index)\n",
        "  print('\\n Welcome to Criterion number {} \\n'.format(criterion))\n",
        "  for depth in max_depth:\n",
        "    for fold_num,(train_index, test_index) in enumerate(kf.split(X,y)):\n",
        "      #Step 2: get the train and test fold subsets from the dataset\n",
        "      X_train_fold, X_test_fold = X.loc[train_index], X.loc[test_index]\n",
        "      y_train_fold, y_test_fold = y.loc[train_index], y.loc[test_index]\n",
        "\n",
        "      #Step 3: one hot encode the variables before fitting on the model\n",
        "      ohe = make_column_transformer(\n",
        "            (OneHotEncoder(sparse=False,handle_unknown='ignore'), X_train_fold.dtypes == 'category'),\n",
        "            remainder='passthrough')\n",
        "      X_train_fold  = pd.DataFrame(ohe.fit_transform(X_train_fold), index=X_train_fold.index)\n",
        "      X_test_fold = pd.DataFrame(ohe.transform(X_test_fold), index=X_test_fold.index)\n",
        "      \n",
        "      #STEP 4: Mitigate the bias, e.g. by transforming the original dataset via reweighing.\n",
        "      RW = Reweighing(prot_attr='relationship')\n",
        "      #We obtain a set of weights for the training set, to use in scikit-learn.\n",
        "      trainining_ = RW.fit_transform(X_train_fold,y_train_fold)\n",
        "\n",
        "      #Step 5: run the decision tree classifier\n",
        "      learner = DecisionTreeClassifier(criterion=criterion_1,max_depth=depth)  \n",
        "      learner.fit(X_train_fold,y_train_fold,sample_weight=trainining_[1])\n",
        "      predictions = learner.predict(X_test_fold)\n",
        "\n",
        "      model_acc = sum(predictions==y_test_fold)/len(y_test_fold)\n",
        "      test_pred = y.loc[test_index].copy()\n",
        "      test_pred = predictions\n",
        "\n",
        "      #Step 6: retrieve the metrics\n",
        "      metric_dis = disparate_impact_ratio(y_test_fold, learner.predict(X_test_fold), prot_attr='relationship')\n",
        "      metric_eqq = equal_opportunity_difference(y_test_fold, learner.predict(X_test_fold), prot_attr='relationship')\n",
        "      metric_stat = statistical_parity_difference(y_test_fold, learner.predict(X_test_fold), prot_attr='relationship')\n",
        "      #retrieving metrics from the classifier. The accuracies include percentage of true pred, statistical parity difference, disparate impact and equal opportunity.\n",
        "      result = {'Accuracy':model_acc,'statistical_parity_difference': metric_stat,'disparate_impact': metric_dis,'equal_opportunity_difference': metric_eqq}\n",
        "\n",
        "      results_[str(fold_num)+'DecisionTree_'+str(criterion_1)+str(depth)]=result#saving the results of the decision tree classifier \n",
        "\n",
        "results_occupation_Tree = results_.copy()\n",
        "#save results to google drive\n",
        "with open('/content/drive/MyDrive/UniSussex/ML/DecisionTree_relationshipfeature_weighted_.pickle', 'wb') as f:\n",
        "    pickle.dump(results_occupation_Tree, f)\n",
        "\n",
        "end = time.time()\n",
        "total = end - start\n",
        "print('Total elapsed time was',total)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YeFRa398AQq8",
        "outputId": "7acbd110-6a30-46bf-83ca-434784e8fd4c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total elapsed time was 17.575918436050415\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#in this run we are using the german dataset were they implemented the foreign_worker as sensitive feature\n",
        "start = time.time()#this is to measure elapsed time\n",
        "\n",
        "kf = KFold(n_splits=5, random_state=41, shuffle=True)\n",
        "results_ = {}\n",
        "\n",
        "# Different criterions\n",
        "criterions = ['gini', 'entropy']\n",
        "# Maximum number of levels in tree\n",
        "max_depth = [int(x) for x in np.linspace(2, 100, num = 5)]\n",
        "max_depth.append(None)\n",
        "\n",
        "for criterion_1 in criterions:\n",
        "  #Step 1: get the criterion\n",
        "  print('\\n Welcome to Criterion number {} \\n'.format(criterion))\n",
        "  for depth in max_depth:\n",
        "\n",
        "    for fold_num,(train_index, test_index) in enumerate(kf.split(X_,y_)):\n",
        "      #Step 2: get the train and test fold subsets from the dataset\n",
        "      X_train_fold, X_test_fold = X_.loc[train_index], X_.loc[test_index]\n",
        "      y_train_fold, y_test_fold = y_.loc[train_index], y_.loc[test_index]\n",
        "      #Step 3: one hot encode the variables before fitting on the model\n",
        "      ohe = make_column_transformer(\n",
        "            (OneHotEncoder(sparse=False,handle_unknown='ignore'), X_train_fold.dtypes == 'category'),\n",
        "            remainder='passthrough')\n",
        "      X_train_fold  = pd.DataFrame(ohe.fit_transform(X_train_fold), index=X_train_fold.index)\n",
        "      X_test_fold = pd.DataFrame(ohe.transform(X_test_fold), index=X_test_fold.index)\n",
        "      \n",
        "      #STEP 4: Mitigate the bias, e.g. by transforming the original dataset via reweighing.\n",
        "      RW = Reweighing(prot_attr='job')\n",
        "      #We obtain a set of weights for the training set, to use in scikit-learn.\n",
        "      trainining_ = RW.fit_transform(X_train_fold,y_train_fold)\n",
        "      #Step 5: run the decision tree classifier\n",
        "      learner = DecisionTreeClassifier(criterion=criterion_1,max_depth=depth)  \n",
        "      learner.fit(X_train_fold,y_train_fold,sample_weight=trainining_[1])\n",
        "      predictions = learner.predict(X_test_fold)\n",
        "\n",
        "      model_acc = sum(predictions==y_test_fold)/len(y_test_fold)\n",
        "      test_pred = y.loc[test_index].copy()\n",
        "      test_pred = predictions\n",
        "      #Step 6: retrieve the metrics\n",
        "      metric_dis = disparate_impact_ratio(y_test_fold, learner.predict(X_test_fold), prot_attr='job')\n",
        "      metric_eqq = equal_opportunity_difference(y_test_fold, learner.predict(X_test_fold), prot_attr='job')\n",
        "      metric_stat = statistical_parity_difference(y_test_fold, learner.predict(X_test_fold), prot_attr='job')\n",
        "      #retrieving metrics from the classifier. The accuracies include percentage of true pred, statistical parity difference, disparate impact and equal opportunity.\n",
        "      result = {'Accuracy':model_acc,'statistical_parity_difference': metric_stat,'disparate_impact': metric_dis,'equal_opportunity_difference': metric_eqq}\n",
        "\n",
        "      results_[str(fold_num)+'DecisionTree_'+str(criterion_1)+str(depth)]=result#saving the results of the decision tree classifier \n",
        "\n",
        "results_foreign_worker_Tree = results_.copy()\n",
        "#save results to google drive\n",
        "with open('/content/drive/MyDrive/UniSussex/ML/DecisionTree_jobfeature_weighted_.pickle', 'wb') as f:\n",
        "    pickle.dump(results_foreign_worker_Tree, f)\n",
        "    \n",
        "end = time.time()\n",
        "total = end - start\n",
        "print('Total elapsed time was',total)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jOw8fi1uYs8d",
        "outputId": "f8a37ef8-a5e2-47e6-de03-758b49e260cf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total elapsed time was 1.0466723442077637\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Part 2: Removing sensitive features \n",
        "\n",
        "\n",
        "In this section of the report we investigate the effects of removing sensitive features. In order to achive this we only need to read the columns starting from index 2. Thus sex and age or race will be omited.\n",
        "\n",
        "However, we need to remember that at times proxy variables exist. Thus all the features are important. \n",
        "\n"
      ],
      "metadata": {
        "id": "NlSpbnRWCxNB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from aif360.algorithms.postprocessing import CalibratedEqOddsPostprocessing, EqOddsPostprocessing, RejectOptionClassification"
      ],
      "metadata": {
        "id": "2t1pcvcOXvOb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "privileged_groups = [{'sex':1,'race': 1}]\n",
        "unprivileged_groups = [{'sex':0,'race': 0}]\n",
        "dataset_orig = load_preproc_data_adult(['sex','race'])\n",
        "\n",
        "#dataset_orig_fix = dataset_orig.convert_to_dataframe()[0].drop(['race', 'sex'], axis=1)\n",
        "#STEP 3: We split between training and test set.\n",
        "train_, test_ = dataset_orig.split([0.7], shuffle=True)\n",
        "print(\"training data size\", train_.features.shape)\n",
        "print(\"dataset feature names\", train_.feature_names)\n",
        "\n",
        "kf = KFold(n_splits=5, random_state=41, shuffle=True)\n",
        "results_ = {}\n",
        "\n",
        "# Different criterions\n",
        "criterions = ['gini', 'entropy']\n",
        "# Maximum number of levels in tree\n",
        "max_depth = [int(x) for x in np.linspace(2, 100, num = 5)]\n",
        "max_depth.append(None)\n",
        "\n",
        "for criterion_1 in criterions:\n",
        "  #Step 1: get the criterion \n",
        "  for depth in max_depth:\n",
        "    for fold_num,(train_index, test_index) in enumerate(kf.split(train_.features)):\n",
        "      trainining_ = train_.subset(train_index)\n",
        "      testing_ = train_.subset(test_index)\n",
        "\n",
        "      #Step 2: Normalize the dataset, both train and test. This should always be done in any machine learning pipeline!\n",
        "      scale = StandardScaler()\n",
        "\n",
        "      X_train_ = scale.fit_transform(trainining_.features[:, 2:])#do not read the first two column (sex,race) while fitting to the data\n",
        "      y_train_ = trainining_.labels.ravel()\n",
        "\n",
        "      X_test_ = scale.transform(testing_.features[:, 2:]) #do not read the first two column (sex,race) while fitting to the data\n",
        "      y_test_ = testing_.labels.ravel()\n",
        "\n",
        "      #Step 3: run the code on the decision tree classifier\n",
        "      learner = DecisionTreeClassifier(criterion=criterion_1,max_depth=depth)  \n",
        "      learner.fit(X_train_,y_train_,sample_weight=trainining_.instance_weights)\n",
        "      predictions = learner.predict(X_test_)\n",
        "      model_acc = sum(predictions==y_test_)/len(y_test_)\n",
        "\n",
        "      #Step 4: retrieve the metrics\n",
        "      test_pred = testing_.copy()\n",
        "      test_pred.labels = predictions\n",
        "      metric_pred = ClassificationMetric(testing_, test_pred, unprivileged_groups=unprivileged_groups, privileged_groups=privileged_groups)\n",
        "      classified_metric = ClassificationMetric(testing_, test_pred, unprivileged_groups=unprivileged_groups, privileged_groups=privileged_groups)\n",
        "\n",
        "      #retrieving metrics from the classifier. The accuracies include percentage of true pred, statistical parity difference, disparate impact and equal opportunity.\n",
        "      result = {'Accuracy_pre':model_acc,'Accuracy_post':classified_metric.accuracy(),'statistical_parity_difference': metric_pred.statistical_parity_difference(),\n",
        "            'disparate_impact': metric_pred.disparate_impact(),'equal_opportunity_difference': classified_metric.equal_opportunity_difference()}\n",
        "\n",
        "      results_[str(fold_num)+'DecisionTree_'+str(criterion_1)+str(depth)]=result#saving the results of the decision tree classifier \n",
        "\n",
        "results_Tree_nofeature = results_.copy()\n",
        "#save results to google drive\n",
        "with open('/content/drive/MyDrive/UniSussex/ML/DecisionTree_nosensitivefeatureAdult_.pickle', 'wb') as f:\n",
        "    pickle.dump(results_Tree_nofeature, f)"
      ],
      "metadata": {
        "id": "QOWU0WfUIdLS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f04947b4-fcdb-4388-c3dc-24054921178d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "training data size (34189, 18)\n",
            "dataset feature names ['race', 'sex', 'Age (decade)=10', 'Age (decade)=20', 'Age (decade)=30', 'Age (decade)=40', 'Age (decade)=50', 'Age (decade)=60', 'Age (decade)=>=70', 'Education Years=6', 'Education Years=7', 'Education Years=8', 'Education Years=9', 'Education Years=10', 'Education Years=11', 'Education Years=12', 'Education Years=<6', 'Education Years=>12']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "privileged_groups = [{'sex':1,'age': 1}]\n",
        "unprivileged_groups = [{'sex':0,'age': 0}]\n",
        "dataset_orig = load_preproc_data_german(['sex','age'])\n",
        "\n",
        "#dataset_orig_fix = dataset_orig.convert_to_dataframe()[0].drop(['race', 'sex'], axis=1)\n",
        "#STEP 3: We split between training and test set.\n",
        "train_, test_ = dataset_orig.split([0.7], shuffle=True)\n",
        "print(\"training data size\", train_.features.shape)\n",
        "print(\"dataset feature names\", train_.feature_names)\n",
        "\n",
        "kf = KFold(n_splits=5, random_state=41, shuffle=True)\n",
        "results_ = {}\n",
        "\n",
        "# Different criterions\n",
        "criterions = ['gini', 'entropy']\n",
        "# Maximum number of levels in tree\n",
        "max_depth = [int(x) for x in np.linspace(2, 100, num = 5)]\n",
        "max_depth.append(None)\n",
        "\n",
        "for criterion_1 in criterions:\n",
        "  #Step 1: retrieve the criterion\n",
        "  for depth in max_depth:\n",
        "\n",
        "    for fold_num,(train_index, test_index) in enumerate(kf.split(train_.features)):\n",
        "\n",
        "      #Step 2: get the train and test fold subsets from the dataset\n",
        "      trainining_ = train_.subset(train_index)\n",
        "      testing_ = train_.subset(test_index)\n",
        "\n",
        "      #Step3: Normalize the dataset, both train and test. This should always be done in any machine learning pipeline!\n",
        "      scale = StandardScaler()\n",
        "\n",
        "      X_train_ = scale.fit_transform(trainining_.features[:, 2:])#do not read the first two column (sex,race) while fitting to the data\n",
        "      y_train_ = trainining_.labels.ravel()\n",
        "\n",
        "      X_test_ = scale.transform(testing_.features[:, 2:]) #do not read the first two column (sex,race) while fitting to the data\n",
        "      y_test_ = testing_.labels.ravel()\n",
        "\n",
        "      #Step 4: run the decision tree classifier\n",
        "      learner = DecisionTreeClassifier(criterion=criterion_1,max_depth=depth)  \n",
        "      learner.fit(X_train_,y_train_,sample_weight=trainining_.instance_weights)\n",
        "      predictions = learner.predict(X_test_)\n",
        "      model_acc = sum(predictions==y_test_)/len(y_test_)\n",
        "\n",
        "      test_pred = testing_.copy()\n",
        "      test_pred.labels = predictions\n",
        "\n",
        "      #Step 5: apply post processing technique\n",
        "      CPP = CalibratedEqOddsPostprocessing(privileged_groups = privileged_groups,\n",
        "                                    unprivileged_groups = unprivileged_groups,\n",
        "                                    cost_constraint='weighted',\n",
        "                                    seed=42)\n",
        "      \n",
        "      CPP = CPP.fit(testing_, test_pred)\n",
        "      transf_test_pred = CPP.predict(test_pred)\n",
        "\n",
        "      #Step 6: retrieve the metrics\n",
        "      metric_pred = ClassificationMetric(testing_, transf_test_pred, unprivileged_groups=unprivileged_groups, privileged_groups=privileged_groups)\n",
        "      classified_metric = ClassificationMetric(testing_, transf_test_pred, unprivileged_groups=unprivileged_groups, privileged_groups=privileged_groups)\n",
        "\n",
        "      #retrieving metrics from the classifier. The accuracies include percentage of true pred, statistical parity difference, disparate impact and equal opportunity.\n",
        "      result = {'Accuracy_pre':model_acc,'Accuracy_post':classified_metric.accuracy(),'statistical_parity_difference': metric_pred.statistical_parity_difference(),\n",
        "            'disparate_impact': metric_pred.disparate_impact(),'equal_opportunity_difference': classified_metric.equal_opportunity_difference()}\n",
        "\n",
        "      results_[str(fold_num)+'DecisionTree_'+str(criterion_1)+str(depth)]=result#saving the results of the decision tree classifier \n",
        "\n",
        "\n",
        "results_occupation_Tree_nofeature_post = results_.copy()\n",
        "#save results to google drive\n",
        "with open('/content/drive/MyDrive/UniSussex/ML/DecisionTree_nosensitivefeature_postprocessingGerman_.pickle', 'wb') as f:\n",
        "    pickle.dump(results_occupation_Tree_nofeature_post, f)"
      ],
      "metadata": {
        "id": "zg260Uk5JybB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "754e220c-9d9f-444d-884d-efe73b6dee64"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "training data size (700, 11)\n",
            "dataset feature names ['age', 'sex', 'credit_history=Delay', 'credit_history=None/Paid', 'credit_history=Other', 'savings=500+', 'savings=<500', 'savings=Unknown/None', 'employment=1-4 years', 'employment=4+ years', 'employment=Unemployed']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Part 3: Extra \n",
        "\n",
        "1. Trying to see the effects of having a decision tree with pre and post processing. \n",
        "\n",
        "2. two different in processing techniques. Gerry fair and adversarial debiasing. Moreover, the check if different train and test splits have an affect on the classifiers."
      ],
      "metadata": {
        "id": "yVs3_s2lQRNd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Welcome to test case 1\n",
        "\n",
        "privileged_groups = [{'sex':1,'race': 1}]\n",
        "unprivileged_groups = [{'sex':0,'race': 0}]\n",
        "dataset_orig = load_preproc_data_adult(['sex','race'])\n",
        "\n",
        "#dataset_orig_fix = dataset_orig.convert_to_dataframe()[0].drop(['race', 'sex'], axis=1)\n",
        "#STEP 0: We split between training and test set.\n",
        "train_, test_ = dataset_orig.split([0.7], shuffle=True)\n",
        "print(\"training data size\", train_.features.shape)\n",
        "print(\"dataset feature names\", train_.feature_names)\n",
        "\n",
        "kf = KFold(n_splits=5, random_state=41, shuffle=True)\n",
        "results_ = {}\n",
        "\n",
        "# Different criterions\n",
        "criterions = ['gini']\n",
        "max_depth = [100]\n",
        "repair_level = [0.1,0.3,0.5,0.7,1]\n",
        "\n",
        "for repair in repair_level:\n",
        "  #Step 1: get the criterion\n",
        "  print('\\n Welcome to Repair number {} \\n'.format(repair))\n",
        "\n",
        "  for fold_num,(train_index, test_index) in enumerate(kf.split(train_.features)):\n",
        "    #Step 2: get the train and test fold subsets from the dataset\n",
        "    trainining_ = train_.subset(train_index)\n",
        "    testing_ = train_.subset(test_index)\n",
        "\n",
        "    #Step 3: Normalize the dataset, both train and test. This should always be done in any machine learning pipeline!\n",
        "    scale = StandardScaler()\n",
        "\n",
        "    X_train_ = scale.fit_transform(trainining_.features)\n",
        "    y_train_ = trainining_.labels.ravel()\n",
        "\n",
        "    X_test_ = scale.transform(testing_.features)\n",
        "    y_test_ = testing_.labels.ravel()\n",
        "  \n",
        "    #STEP 4: Mitigate the bias, e.g. by transforming the original dataset via DisparateImpactRemover.\n",
        "    di = DisparateImpactRemover(sensitive_attribute='sex',repair_level = repair)\n",
        "    #We obtain a set of weights for the training set, to use in scikit-learn.\n",
        "    trainining_weight = di.fit_transform(trainining_)\n",
        "\n",
        "    #Step 5: Run the decision tree classifier\n",
        "    learner = DecisionTreeClassifier(criterion='entropy',max_depth=100)  \n",
        "    learner.fit(X_train_,y_train_,sample_weight=trainining_.instance_weights)\n",
        "    predictions = learner.predict(X_test_)\n",
        "    model_acc = sum(predictions==y_test_)/len(y_test_)\n",
        "\n",
        "    test_pred = testing_.copy()\n",
        "    test_pred.labels = predictions.reshape(test_pred.labels.shape)\n",
        "\n",
        "    \n",
        "    #Step 6: Post process the results from the classifier \n",
        "    EOPP = EqOddsPostprocessing(privileged_groups = privileged_groups,\n",
        "                             unprivileged_groups = unprivileged_groups,\n",
        "                             seed=42)\n",
        "    EOPP = EOPP.fit(testing_, test_pred)\n",
        "    transf_test_pred = EOPP.predict(test_pred)\n",
        "    \n",
        "    #Step 7: retrive the metrics\n",
        "    metric_pred = ClassificationMetric(testing_, transf_test_pred, unprivileged_groups=unprivileged_groups, privileged_groups=privileged_groups)\n",
        "    classified_metric = ClassificationMetric(testing_, transf_test_pred, unprivileged_groups=unprivileged_groups, privileged_groups=privileged_groups)\n",
        "    \n",
        "    #retrieving metrics from the classifier. The accuracies include percentage of true pred, statistical parity difference, disparate impact and equal opportunity.\n",
        "    result = {'Accuracy_pre':model_acc,'Accuracy_post':classified_metric.accuracy(),'statistical_parity_difference': metric_pred.statistical_parity_difference(),\n",
        "              'disparate_impact': metric_pred.disparate_impact(),'equal_opportunity_difference': classified_metric.equal_opportunity_difference()}\n",
        "\n",
        "    results_[str(fold_num)+'DecisionTree_'+str(repair)]=result#saving the results of the decision tree classifier \n",
        "\n",
        "results_occupation_Tree_Extra1 = results_.copy()\n",
        "with open('/content/drive/MyDrive/UniSussex/ML/results_occupation_Tree_Extra_prepostprocessing.pickle', 'wb') as f:\n",
        "    pickle.dump(results_occupation_Tree_Extra1, f)"
      ],
      "metadata": {
        "id": "uh4dQmkNQQwi",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "84003768-6d0f-4ceb-c6ea-e10b225e8ab2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "training data size (34189, 18)\n",
            "dataset feature names ['race', 'sex', 'Age (decade)=10', 'Age (decade)=20', 'Age (decade)=30', 'Age (decade)=40', 'Age (decade)=50', 'Age (decade)=60', 'Age (decade)=>=70', 'Education Years=6', 'Education Years=7', 'Education Years=8', 'Education Years=9', 'Education Years=10', 'Education Years=11', 'Education Years=12', 'Education Years=<6', 'Education Years=>12']\n",
            "\n",
            " Welcome to Fold number 0 \n",
            "\n",
            "\n",
            " Welcome to Fold number 0 \n",
            "\n",
            "\n",
            " Welcome to Fold number 0 \n",
            "\n",
            "\n",
            " Welcome to Fold number 0 \n",
            "\n",
            "\n",
            " Welcome to Fold number 0 \n",
            "\n",
            "\n",
            " Welcome to Fold number 1 \n",
            "\n",
            "\n",
            " Welcome to Fold number 1 \n",
            "\n",
            "\n",
            " Welcome to Fold number 1 \n",
            "\n",
            "\n",
            " Welcome to Fold number 1 \n",
            "\n",
            "\n",
            " Welcome to Fold number 1 \n",
            "\n",
            "\n",
            " Welcome to Fold number 2 \n",
            "\n",
            "\n",
            " Welcome to Fold number 2 \n",
            "\n",
            "\n",
            " Welcome to Fold number 2 \n",
            "\n",
            "\n",
            " Welcome to Fold number 2 \n",
            "\n",
            "\n",
            " Welcome to Fold number 2 \n",
            "\n",
            "\n",
            " Welcome to Fold number 3 \n",
            "\n",
            "\n",
            " Welcome to Fold number 3 \n",
            "\n",
            "\n",
            " Welcome to Fold number 3 \n",
            "\n",
            "\n",
            " Welcome to Fold number 3 \n",
            "\n",
            "\n",
            " Welcome to Fold number 3 \n",
            "\n",
            "\n",
            " Welcome to Fold number 4 \n",
            "\n",
            "\n",
            " Welcome to Fold number 4 \n",
            "\n",
            "\n",
            " Welcome to Fold number 4 \n",
            "\n",
            "\n",
            " Welcome to Fold number 4 \n",
            "\n",
            "\n",
            " Welcome to Fold number 4 \n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from aif360.algorithms.inprocessing import AdversarialDebiasing, PrejudiceRemover, GerryFairClassifier\n",
        "import tensorflow.compat.v1 as tf\n",
        "\n",
        "privileged_groups = [{'sex':1,'race': 1}]\n",
        "unprivileged_groups = [{'sex':0,'race': 0}]\n",
        "dataset_orig = load_preproc_data_adult(['sex','race'])\n",
        "\n",
        "#dataset_orig_fix = dataset_orig.convert_to_dataframe()[0].drop(['race', 'sex'], axis=1)\n",
        "#STEP 0: We split between training and test set.\n",
        "train_, test_ = dataset_orig.split([0.7], shuffle=True)\n",
        "print(\"training data size\", train_.features.shape)\n",
        "print(\"dataset feature names\", train_.feature_names)\n",
        "\n",
        "kf = KFold(n_splits=5, random_state=41, shuffle=True)\n",
        "results_ = {}\n",
        "\n",
        "# Different criterions\n",
        "criterions = [0.00001,0.001,1,100,10000]\n",
        "\n",
        "for cri in criterions:\n",
        "  #Step 1: get the criterion\n",
        "  print('\\n Welcome to Criterion number {} \\n'.format(cri))\n",
        "\n",
        "  for fold_num,(train_index, test_index) in enumerate(kf.split(train_.features)):\n",
        "    #Step 2: get the train and test fold subsets from the dataset\n",
        "    trainining_ = train_.subset(train_index)\n",
        "    testing_ = train_.subset(test_index)\n",
        "\n",
        "    tf.disable_v2_behavior()\n",
        "    tf.reset_default_graph() \n",
        "    sess = tf.Session()\n",
        "    #Step 4: run the GerryFair Classifier\n",
        "    learner = GerryFairClassifier(C=cri,max_iters=20)\n",
        "    \n",
        "    learner.fit(trainining_)\n",
        "    predictions = learner.predict(testing_)\n",
        "    sess.close()\n",
        "    predictions_ = predictions.labels.ravel().copy()\n",
        "\n",
        "    model_acc = sum(predictions_==testing_.labels.ravel())/len(testing_.labels.ravel())\n",
        "    test_pred = testing_.copy()\n",
        "    test_pred.labels = predictions_.reshape(test_pred.labels.shape)\n",
        "\n",
        "    #Step 5: run the post processing algorithm\n",
        "    EOPP = EqOddsPostprocessing(privileged_groups = privileged_groups,\n",
        "                             unprivileged_groups = unprivileged_groups,\n",
        "                             seed=42)\n",
        "    EOPP = EOPP.fit(testing_, test_pred)\n",
        "    transf_test_pred = EOPP.predict(test_pred)\n",
        "\n",
        "    #Step 6: retrieve the metrics\n",
        "    metric_pred = ClassificationMetric(testing_, transf_test_pred, unprivileged_groups=unprivileged_groups, privileged_groups=privileged_groups)\n",
        "    classified_metric = ClassificationMetric(testing_, transf_test_pred, unprivileged_groups=unprivileged_groups, privileged_groups=privileged_groups)\n",
        "\n",
        "    #retrieving metrics from the classifier. The accuracies include percentage of true pred, statistical parity difference, disparate impact and equal opportunity.\n",
        "    result = {'Accuracy_pre':model_acc,'Accuracy_post':classified_metric.accuracy(),'statistical_parity_difference': metric_pred.statistical_parity_difference(),\n",
        "              'disparate_impact': metric_pred.disparate_impact(),'equal_opportunity_difference': classified_metric.equal_opportunity_difference()}\n",
        "\n",
        "    results_[str(fold_num)+'GerryFairClassifier_'+str(cri)]=result#saving the results of the decision tree classifier \n",
        "\n",
        "results_occupation_Tree_Extra2 = results_.copy()\n",
        "with open('/content/drive/MyDrive/UniSussex/ML/results_occupation_Tree_Extra_postprocessing_GerryFair_.pickle', 'wb') as f:\n",
        "    pickle.dump(results_occupation_Tree_Extra2, f)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DekaOxhsXeeL",
        "outputId": "6b7c6822-aec9-4d07-8aab-2c2c487a6d3f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "training data size (34189, 18)\n",
            "dataset feature names ['race', 'sex', 'Age (decade)=10', 'Age (decade)=20', 'Age (decade)=30', 'Age (decade)=40', 'Age (decade)=50', 'Age (decade)=60', 'Age (decade)=>=70', 'Education Years=6', 'Education Years=7', 'Education Years=8', 'Education Years=9', 'Education Years=10', 'Education Years=11', 'Education Years=12', 'Education Years=<6', 'Education Years=>12']\n",
            "\n",
            " Welcome to Fold number 0 \n",
            "\n",
            "\n",
            " Welcome to Fold number 1 \n",
            "\n",
            "\n",
            " Welcome to Fold number 2 \n",
            "\n",
            "\n",
            " Welcome to Fold number 3 \n",
            "\n",
            "\n",
            " Welcome to Fold number 4 \n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from aif360.algorithms.inprocessing import AdversarialDebiasing, PrejudiceRemover, GerryFairClassifier\n",
        "import tensorflow.compat.v1 as tf\n",
        "\n",
        "privileged_groups = [{'sex':1,'race': 1}]\n",
        "unprivileged_groups = [{'sex':0,'race': 0}]\n",
        "dataset_orig = load_preproc_data_adult(['sex','race'])\n",
        "splits = [[0.1],[0.2],[0.3],[0.4],[0.5],[0.6],[0.7],[0.8],[0.9]]\n",
        "\n",
        "results_final = {}\n",
        "\n",
        "\n",
        "for split in splits:\n",
        "  #Step 1: retrive the different splits\n",
        "  print('\\n Welcome to split number {} \\n'.format(split))\n",
        "  #dataset_orig_fix = dataset_orig.convert_to_dataframe()[0].drop(['race', 'sex'], axis=1)\n",
        "  #STEP 2: We split between training and test set from pre processing method.\n",
        "  train_, test_ = dataset_orig.split(split, shuffle=True)\n",
        "  print(\"training data size\", train_.features.shape)\n",
        "  print(\"dataset feature names\", train_.feature_names)\n",
        "\n",
        "  kf = KFold(n_splits=5, random_state=41, shuffle=True)\n",
        "  results_ = {}\n",
        "\n",
        "  # Different criterions\n",
        "  criterions = [0.00001,0.001,1,100,10000]\n",
        "  results_occupation_Tree_Extra2 = []\n",
        "\n",
        "  for cri in criterions:\n",
        "    #Step 3: retrive the criterion\n",
        "    print('\\n Welcome to Criterion number {} \\n'.format(cri))\n",
        "\n",
        "    for fold_num,(train_index, test_index) in enumerate(kf.split(train_.features)):\n",
        "      #Step 4: get the train and test fold subsets from the dataset\n",
        "      trainining_ = train_.subset(train_index)\n",
        "      testing_ = train_.subset(test_index)\n",
        "\n",
        "      tf.disable_v2_behavior()\n",
        "      tf.reset_default_graph() \n",
        "      sess = tf.Session()\n",
        "      #Step 5: run the Gerry Fair Classifier\n",
        "      learner = GerryFairClassifier(C=cri,max_iters=20)\n",
        "      \n",
        "      learner.fit(trainining_)\n",
        "      predictions = learner.predict(testing_)\n",
        "      sess.close()\n",
        "      predictions_ = predictions.labels.ravel().copy()\n",
        "      #Step 6: retrieve the metrics\n",
        "      model_acc = sum(predictions_==testing_.labels.ravel())/len(testing_.labels.ravel())\n",
        "      test_pred = testing_.copy()\n",
        "      test_pred.labels = predictions_.reshape(test_pred.labels.shape)\n",
        "\n",
        "      metric_pred = ClassificationMetric(testing_, test_pred, unprivileged_groups=unprivileged_groups, privileged_groups=privileged_groups)\n",
        "      classified_metric = ClassificationMetric(testing_, test_pred, unprivileged_groups=unprivileged_groups, privileged_groups=privileged_groups)\n",
        "\n",
        "      #retrieving metrics from the classifier. The accuracies include percentage of true pred, statistical parity difference, disparate impact and equal opportunity.\n",
        "      result = {'Accuracy_pre':model_acc,'Accuracy_post':classified_metric.accuracy(),'statistical_parity_difference': metric_pred.statistical_parity_difference(),\n",
        "                'disparate_impact': metric_pred.disparate_impact(),'equal_opportunity_difference': classified_metric.equal_opportunity_difference()}\n",
        "\n",
        "      results_[str(fold_num)+'GerryFairClassifier_'+str(cri)]=result#saving the results of the decision tree classifier \n",
        "\n",
        "  results_occupation_Tree_Extra2 = results_.copy()\n",
        "  results_final[str(split)] = results_occupation_Tree_Extra2\n",
        "\n",
        "with open('/content/drive/MyDrive/UniSussex/ML/SplitsExtra_GerryFairClassifier_adult_.pickle', 'wb') as f:\n",
        "    pickle.dump(results_final, f)"
      ],
      "metadata": {
        "id": "ka8MEhsam-Q1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "39b65ecf-a711-4be4-cef0-06ce11dd0178"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " Welcome to split number [0.1] \n",
            "\n",
            "training data size (4884, 18)\n",
            "dataset feature names ['race', 'sex', 'Age (decade)=10', 'Age (decade)=20', 'Age (decade)=30', 'Age (decade)=40', 'Age (decade)=50', 'Age (decade)=60', 'Age (decade)=>=70', 'Education Years=6', 'Education Years=7', 'Education Years=8', 'Education Years=9', 'Education Years=10', 'Education Years=11', 'Education Years=12', 'Education Years=<6', 'Education Years=>12']\n",
            "\n",
            " Welcome to Fold number 0 \n",
            "\n",
            "\n",
            " Welcome to Fold number 1 \n",
            "\n",
            "\n",
            " Welcome to Fold number 2 \n",
            "\n",
            "\n",
            " Welcome to Fold number 3 \n",
            "\n",
            "\n",
            " Welcome to Fold number 4 \n",
            "\n",
            "\n",
            " Welcome to split number [0.2] \n",
            "\n",
            "training data size (9768, 18)\n",
            "dataset feature names ['race', 'sex', 'Age (decade)=10', 'Age (decade)=20', 'Age (decade)=30', 'Age (decade)=40', 'Age (decade)=50', 'Age (decade)=60', 'Age (decade)=>=70', 'Education Years=6', 'Education Years=7', 'Education Years=8', 'Education Years=9', 'Education Years=10', 'Education Years=11', 'Education Years=12', 'Education Years=<6', 'Education Years=>12']\n",
            "\n",
            " Welcome to Fold number 0 \n",
            "\n",
            "\n",
            " Welcome to Fold number 1 \n",
            "\n",
            "\n",
            " Welcome to Fold number 2 \n",
            "\n",
            "\n",
            " Welcome to Fold number 3 \n",
            "\n",
            "\n",
            " Welcome to Fold number 4 \n",
            "\n",
            "\n",
            " Welcome to split number [0.3] \n",
            "\n",
            "training data size (14652, 18)\n",
            "dataset feature names ['race', 'sex', 'Age (decade)=10', 'Age (decade)=20', 'Age (decade)=30', 'Age (decade)=40', 'Age (decade)=50', 'Age (decade)=60', 'Age (decade)=>=70', 'Education Years=6', 'Education Years=7', 'Education Years=8', 'Education Years=9', 'Education Years=10', 'Education Years=11', 'Education Years=12', 'Education Years=<6', 'Education Years=>12']\n",
            "\n",
            " Welcome to Fold number 0 \n",
            "\n",
            "\n",
            " Welcome to Fold number 1 \n",
            "\n",
            "\n",
            " Welcome to Fold number 2 \n",
            "\n",
            "\n",
            " Welcome to Fold number 3 \n",
            "\n",
            "\n",
            " Welcome to Fold number 4 \n",
            "\n",
            "\n",
            " Welcome to split number [0.4] \n",
            "\n",
            "training data size (19536, 18)\n",
            "dataset feature names ['race', 'sex', 'Age (decade)=10', 'Age (decade)=20', 'Age (decade)=30', 'Age (decade)=40', 'Age (decade)=50', 'Age (decade)=60', 'Age (decade)=>=70', 'Education Years=6', 'Education Years=7', 'Education Years=8', 'Education Years=9', 'Education Years=10', 'Education Years=11', 'Education Years=12', 'Education Years=<6', 'Education Years=>12']\n",
            "\n",
            " Welcome to Fold number 0 \n",
            "\n",
            "\n",
            " Welcome to Fold number 1 \n",
            "\n",
            "\n",
            " Welcome to Fold number 2 \n",
            "\n",
            "\n",
            " Welcome to Fold number 3 \n",
            "\n",
            "\n",
            " Welcome to Fold number 4 \n",
            "\n",
            "\n",
            " Welcome to split number [0.5] \n",
            "\n",
            "training data size (24421, 18)\n",
            "dataset feature names ['race', 'sex', 'Age (decade)=10', 'Age (decade)=20', 'Age (decade)=30', 'Age (decade)=40', 'Age (decade)=50', 'Age (decade)=60', 'Age (decade)=>=70', 'Education Years=6', 'Education Years=7', 'Education Years=8', 'Education Years=9', 'Education Years=10', 'Education Years=11', 'Education Years=12', 'Education Years=<6', 'Education Years=>12']\n",
            "\n",
            " Welcome to Fold number 0 \n",
            "\n",
            "\n",
            " Welcome to Fold number 1 \n",
            "\n",
            "\n",
            " Welcome to Fold number 2 \n",
            "\n",
            "\n",
            " Welcome to Fold number 3 \n",
            "\n",
            "\n",
            " Welcome to Fold number 4 \n",
            "\n",
            "\n",
            " Welcome to split number [0.6] \n",
            "\n",
            "training data size (29305, 18)\n",
            "dataset feature names ['race', 'sex', 'Age (decade)=10', 'Age (decade)=20', 'Age (decade)=30', 'Age (decade)=40', 'Age (decade)=50', 'Age (decade)=60', 'Age (decade)=>=70', 'Education Years=6', 'Education Years=7', 'Education Years=8', 'Education Years=9', 'Education Years=10', 'Education Years=11', 'Education Years=12', 'Education Years=<6', 'Education Years=>12']\n",
            "\n",
            " Welcome to Fold number 0 \n",
            "\n",
            "\n",
            " Welcome to Fold number 1 \n",
            "\n",
            "\n",
            " Welcome to Fold number 2 \n",
            "\n",
            "\n",
            " Welcome to Fold number 3 \n",
            "\n",
            "\n",
            " Welcome to Fold number 4 \n",
            "\n",
            "\n",
            " Welcome to split number [0.7] \n",
            "\n",
            "training data size (34189, 18)\n",
            "dataset feature names ['race', 'sex', 'Age (decade)=10', 'Age (decade)=20', 'Age (decade)=30', 'Age (decade)=40', 'Age (decade)=50', 'Age (decade)=60', 'Age (decade)=>=70', 'Education Years=6', 'Education Years=7', 'Education Years=8', 'Education Years=9', 'Education Years=10', 'Education Years=11', 'Education Years=12', 'Education Years=<6', 'Education Years=>12']\n",
            "\n",
            " Welcome to Fold number 0 \n",
            "\n",
            "\n",
            " Welcome to Fold number 1 \n",
            "\n",
            "\n",
            " Welcome to Fold number 2 \n",
            "\n",
            "\n",
            " Welcome to Fold number 3 \n",
            "\n",
            "\n",
            " Welcome to Fold number 4 \n",
            "\n",
            "\n",
            " Welcome to split number [0.8] \n",
            "\n",
            "training data size (39073, 18)\n",
            "dataset feature names ['race', 'sex', 'Age (decade)=10', 'Age (decade)=20', 'Age (decade)=30', 'Age (decade)=40', 'Age (decade)=50', 'Age (decade)=60', 'Age (decade)=>=70', 'Education Years=6', 'Education Years=7', 'Education Years=8', 'Education Years=9', 'Education Years=10', 'Education Years=11', 'Education Years=12', 'Education Years=<6', 'Education Years=>12']\n",
            "\n",
            " Welcome to Fold number 0 \n",
            "\n",
            "\n",
            " Welcome to Fold number 1 \n",
            "\n",
            "\n",
            " Welcome to Fold number 2 \n",
            "\n",
            "\n",
            " Welcome to Fold number 3 \n",
            "\n",
            "\n",
            " Welcome to Fold number 4 \n",
            "\n",
            "\n",
            " Welcome to split number [0.9] \n",
            "\n",
            "training data size (43957, 18)\n",
            "dataset feature names ['race', 'sex', 'Age (decade)=10', 'Age (decade)=20', 'Age (decade)=30', 'Age (decade)=40', 'Age (decade)=50', 'Age (decade)=60', 'Age (decade)=>=70', 'Education Years=6', 'Education Years=7', 'Education Years=8', 'Education Years=9', 'Education Years=10', 'Education Years=11', 'Education Years=12', 'Education Years=<6', 'Education Years=>12']\n",
            "\n",
            " Welcome to Fold number 0 \n",
            "\n",
            "\n",
            " Welcome to Fold number 1 \n",
            "\n",
            "\n",
            " Welcome to Fold number 2 \n",
            "\n",
            "\n",
            " Welcome to Fold number 3 \n",
            "\n",
            "\n",
            " Welcome to Fold number 4 \n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from aif360.algorithms.inprocessing import AdversarialDebiasing, PrejudiceRemover, GerryFairClassifier\n",
        "import tensorflow.compat.v1 as tf\n",
        "\n",
        "privileged_groups = [{'sex':1,'race': 1}]\n",
        "unprivileged_groups = [{'sex':0,'race': 0}]\n",
        "dataset_orig = load_preproc_data_adult(['sex','race'])\n",
        "splits = [[0.1],[0.2],[0.3],[0.4],[0.5],[0.6],[0.7],[0.8],[0.9]]\n",
        "\n",
        "results_final = {}\n",
        "#Step 1: get the train and test splits\n",
        "for split in splits:\n",
        "  print('\\n Welcome to split number {} \\n'.format(split))\n",
        "  #dataset_orig_fix = dataset_orig.convert_to_dataframe()[0].drop(['race', 'sex'], axis=1)\n",
        "  #STEP 2: We split between training and test set.\n",
        "  train_, test_ = dataset_orig.split(split, shuffle=True)\n",
        "  print(\"training data size\", train_.features.shape)\n",
        "  print(\"dataset feature names\", train_.feature_names)\n",
        "\n",
        "  kf = KFold(n_splits=5, random_state=41, shuffle=True)\n",
        "  results_ = {}\n",
        "\n",
        "  # Different criterions\n",
        "  criterions = [0.00001,0.001,1,100,10000]\n",
        "  results_occupation_Tree_Extra2 = []\n",
        "  for cri in criterions: \n",
        "    #Step 3: get the criterion\n",
        "    print('\\n Welcome to Criterion number {} \\n'.format(cri))\n",
        "\n",
        "    for fold_num,(train_index, test_index) in enumerate(kf.split(train_.features)):\n",
        "      #Step 4: get the train and test fold subsets from the dataset\n",
        "      trainining_ = train_.subset(train_index)\n",
        "      testing_ = train_.subset(test_index)\n",
        "\n",
        "      tf.disable_v2_behavior()\n",
        "      tf.reset_default_graph() \n",
        "      sess = tf.Session()\n",
        "      #Step 5: Adversarial Debiasing Classifier\n",
        "      learner = AdversarialDebiasing(privileged_groups = privileged_groups,\n",
        "                            unprivileged_groups = unprivileged_groups,\n",
        "                            scope_name='debiased_classifier',adversary_loss_weight=cri,\n",
        "                            num_epochs=50,debias=True,sess=sess)\n",
        "      learner.fit(trainining_)\n",
        "\n",
        "      predictions = learner.predict(testing_)\n",
        "      sess.close()\n",
        "      predictions_ = predictions.labels.ravel().copy()\n",
        "      #Step 6: retrieve the metrics\n",
        "      model_acc = sum(predictions_==testing_.labels.ravel())/len(testing_.labels.ravel())\n",
        "      test_pred = testing_.copy()\n",
        "      test_pred.labels = predictions_.reshape(test_pred.labels.shape)\n",
        "\n",
        "      metric_pred = ClassificationMetric(testing_, test_pred, unprivileged_groups=unprivileged_groups, privileged_groups=privileged_groups)\n",
        "      classified_metric = ClassificationMetric(testing_, test_pred, unprivileged_groups=unprivileged_groups, privileged_groups=privileged_groups)\n",
        "\n",
        "      #retrieving metrics from the classifier. The accuracies include percentage of true pred, statistical parity difference, disparate impact and equal opportunity.\n",
        "      result = {'Accuracy_pre':model_acc,'Accuracy_post':classified_metric.accuracy(),'statistical_parity_difference': metric_pred.statistical_parity_difference(),\n",
        "                'disparate_impact': metric_pred.disparate_impact(),'equal_opportunity_difference': classified_metric.equal_opportunity_difference()}\n",
        "\n",
        "      results_[str(fold_num)+'AdversarialDebiasing_'+str(cri)]=result#saving the results of the decision tree classifier \n",
        "\n",
        "  results_occupation_Tree_Extra2 = results_.copy()\n",
        "  results_final[str(split)] = results_occupation_Tree_Extra2\n",
        "\n",
        "with open('/content/drive/MyDrive/UniSussex/ML/SplitsExtra_AdversarialDebiasingClassifier_adult_.pickle', 'wb') as f:\n",
        "    pickle.dump(results_final, f)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7rsXmOUOj1DX",
        "outputId": "21fbf510-53ba-45af-e008-9ce5a613ffce"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1;30;43mStreaming output truncated to the last 5000 lines.\u001b[0m\n",
            "epoch 16; iter: 0; batch classifier loss: 0.407039; batch adversarial loss: 0.501140\n",
            "epoch 16; iter: 200; batch classifier loss: 0.426677; batch adversarial loss: 0.523875\n",
            "epoch 17; iter: 0; batch classifier loss: 0.398239; batch adversarial loss: 0.538468\n",
            "epoch 17; iter: 200; batch classifier loss: 0.359264; batch adversarial loss: 0.512081\n",
            "epoch 18; iter: 0; batch classifier loss: 0.386477; batch adversarial loss: 0.539867\n",
            "epoch 18; iter: 200; batch classifier loss: 0.384174; batch adversarial loss: 0.565005\n",
            "epoch 19; iter: 0; batch classifier loss: 0.485110; batch adversarial loss: 0.522618\n",
            "epoch 19; iter: 200; batch classifier loss: 0.394808; batch adversarial loss: 0.580436\n",
            "epoch 20; iter: 0; batch classifier loss: 0.470799; batch adversarial loss: 0.559776\n",
            "epoch 20; iter: 200; batch classifier loss: 0.436101; batch adversarial loss: 0.489334\n",
            "epoch 21; iter: 0; batch classifier loss: 0.434288; batch adversarial loss: 0.489410\n",
            "epoch 21; iter: 200; batch classifier loss: 0.409402; batch adversarial loss: 0.536206\n",
            "epoch 22; iter: 0; batch classifier loss: 0.438579; batch adversarial loss: 0.557472\n",
            "epoch 22; iter: 200; batch classifier loss: 0.425468; batch adversarial loss: 0.466698\n",
            "epoch 23; iter: 0; batch classifier loss: 0.399965; batch adversarial loss: 0.532024\n",
            "epoch 23; iter: 200; batch classifier loss: 0.338891; batch adversarial loss: 0.526302\n",
            "epoch 24; iter: 0; batch classifier loss: 0.487366; batch adversarial loss: 0.485759\n",
            "epoch 24; iter: 200; batch classifier loss: 0.360682; batch adversarial loss: 0.526268\n",
            "epoch 25; iter: 0; batch classifier loss: 0.481135; batch adversarial loss: 0.446529\n",
            "epoch 25; iter: 200; batch classifier loss: 0.412343; batch adversarial loss: 0.504665\n",
            "epoch 26; iter: 0; batch classifier loss: 0.279667; batch adversarial loss: 0.484502\n",
            "epoch 26; iter: 200; batch classifier loss: 0.454640; batch adversarial loss: 0.514576\n",
            "epoch 27; iter: 0; batch classifier loss: 0.522029; batch adversarial loss: 0.485027\n",
            "epoch 27; iter: 200; batch classifier loss: 0.445537; batch adversarial loss: 0.421893\n",
            "epoch 28; iter: 0; batch classifier loss: 0.411597; batch adversarial loss: 0.469142\n",
            "epoch 28; iter: 200; batch classifier loss: 0.514284; batch adversarial loss: 0.442102\n",
            "epoch 29; iter: 0; batch classifier loss: 0.482340; batch adversarial loss: 0.456266\n",
            "epoch 29; iter: 200; batch classifier loss: 0.394740; batch adversarial loss: 0.451938\n",
            "epoch 30; iter: 0; batch classifier loss: 0.439973; batch adversarial loss: 0.483384\n",
            "epoch 30; iter: 200; batch classifier loss: 0.486945; batch adversarial loss: 0.470589\n",
            "epoch 31; iter: 0; batch classifier loss: 0.503359; batch adversarial loss: 0.488738\n",
            "epoch 31; iter: 200; batch classifier loss: 0.483254; batch adversarial loss: 0.489320\n",
            "epoch 32; iter: 0; batch classifier loss: 0.452285; batch adversarial loss: 0.457726\n",
            "epoch 32; iter: 200; batch classifier loss: 0.413616; batch adversarial loss: 0.465204\n",
            "epoch 33; iter: 0; batch classifier loss: 0.386341; batch adversarial loss: 0.483036\n",
            "epoch 33; iter: 200; batch classifier loss: 0.379125; batch adversarial loss: 0.507747\n",
            "epoch 34; iter: 0; batch classifier loss: 0.429900; batch adversarial loss: 0.536696\n",
            "epoch 34; iter: 200; batch classifier loss: 0.438467; batch adversarial loss: 0.523134\n",
            "epoch 35; iter: 0; batch classifier loss: 0.403726; batch adversarial loss: 0.531090\n",
            "epoch 35; iter: 200; batch classifier loss: 0.404238; batch adversarial loss: 0.503886\n",
            "epoch 36; iter: 0; batch classifier loss: 0.506299; batch adversarial loss: 0.490565\n",
            "epoch 36; iter: 200; batch classifier loss: 0.442091; batch adversarial loss: 0.639510\n",
            "epoch 37; iter: 0; batch classifier loss: 0.457869; batch adversarial loss: 0.636903\n",
            "epoch 37; iter: 200; batch classifier loss: 0.389491; batch adversarial loss: 0.618506\n",
            "epoch 38; iter: 0; batch classifier loss: 0.338643; batch adversarial loss: 0.651750\n",
            "epoch 38; iter: 200; batch classifier loss: 0.439138; batch adversarial loss: 0.593006\n",
            "epoch 39; iter: 0; batch classifier loss: 0.413343; batch adversarial loss: 0.631894\n",
            "epoch 39; iter: 200; batch classifier loss: 0.397635; batch adversarial loss: 0.551905\n",
            "epoch 40; iter: 0; batch classifier loss: 0.466959; batch adversarial loss: 0.584390\n",
            "epoch 40; iter: 200; batch classifier loss: 0.362861; batch adversarial loss: 0.573961\n",
            "epoch 41; iter: 0; batch classifier loss: 0.442517; batch adversarial loss: 0.609487\n",
            "epoch 41; iter: 200; batch classifier loss: 0.412475; batch adversarial loss: 0.584980\n",
            "epoch 42; iter: 0; batch classifier loss: 0.481062; batch adversarial loss: 0.576062\n",
            "epoch 42; iter: 200; batch classifier loss: 0.402264; batch adversarial loss: 0.555387\n",
            "epoch 43; iter: 0; batch classifier loss: 0.351215; batch adversarial loss: 0.537453\n",
            "epoch 43; iter: 200; batch classifier loss: 0.475066; batch adversarial loss: 0.462709\n",
            "epoch 44; iter: 0; batch classifier loss: 0.397133; batch adversarial loss: 0.499205\n",
            "epoch 44; iter: 200; batch classifier loss: 0.402054; batch adversarial loss: 0.491678\n",
            "epoch 45; iter: 0; batch classifier loss: 0.414949; batch adversarial loss: 0.459794\n",
            "epoch 45; iter: 200; batch classifier loss: 0.461736; batch adversarial loss: 0.534706\n",
            "epoch 46; iter: 0; batch classifier loss: 0.448736; batch adversarial loss: 0.515144\n",
            "epoch 46; iter: 200; batch classifier loss: 0.418599; batch adversarial loss: 0.474459\n",
            "epoch 47; iter: 0; batch classifier loss: 0.434387; batch adversarial loss: 0.555069\n",
            "epoch 47; iter: 200; batch classifier loss: 0.385076; batch adversarial loss: 0.499556\n",
            "epoch 48; iter: 0; batch classifier loss: 0.400022; batch adversarial loss: 0.512248\n",
            "epoch 48; iter: 200; batch classifier loss: 0.357172; batch adversarial loss: 0.572165\n",
            "epoch 49; iter: 0; batch classifier loss: 0.426270; batch adversarial loss: 0.518814\n",
            "epoch 49; iter: 200; batch classifier loss: 0.404454; batch adversarial loss: 0.538064\n",
            "epoch 0; iter: 0; batch classifier loss: 0.664344; batch adversarial loss: 0.644520\n",
            "epoch 0; iter: 200; batch classifier loss: 0.422351; batch adversarial loss: 0.610601\n",
            "epoch 1; iter: 0; batch classifier loss: 0.450928; batch adversarial loss: 0.599544\n",
            "epoch 1; iter: 200; batch classifier loss: 0.401290; batch adversarial loss: 0.596589\n",
            "epoch 2; iter: 0; batch classifier loss: 0.408284; batch adversarial loss: 0.568470\n",
            "epoch 2; iter: 200; batch classifier loss: 0.418229; batch adversarial loss: 0.581117\n",
            "epoch 3; iter: 0; batch classifier loss: 0.389284; batch adversarial loss: 0.610526\n",
            "epoch 3; iter: 200; batch classifier loss: 0.415298; batch adversarial loss: 0.591407\n",
            "epoch 4; iter: 0; batch classifier loss: 0.458626; batch adversarial loss: 0.635222\n",
            "epoch 4; iter: 200; batch classifier loss: 0.414522; batch adversarial loss: 0.604690\n",
            "epoch 5; iter: 0; batch classifier loss: 0.416901; batch adversarial loss: 0.588539\n",
            "epoch 5; iter: 200; batch classifier loss: 0.393432; batch adversarial loss: 0.568653\n",
            "epoch 6; iter: 0; batch classifier loss: 0.424453; batch adversarial loss: 0.615598\n",
            "epoch 6; iter: 200; batch classifier loss: 0.413532; batch adversarial loss: 0.567177\n",
            "epoch 7; iter: 0; batch classifier loss: 0.442132; batch adversarial loss: 0.618060\n",
            "epoch 7; iter: 200; batch classifier loss: 0.430013; batch adversarial loss: 0.662131\n",
            "epoch 8; iter: 0; batch classifier loss: 0.368759; batch adversarial loss: 0.604856\n",
            "epoch 8; iter: 200; batch classifier loss: 0.461758; batch adversarial loss: 0.531614\n",
            "epoch 9; iter: 0; batch classifier loss: 0.364386; batch adversarial loss: 0.563851\n",
            "epoch 9; iter: 200; batch classifier loss: 0.385636; batch adversarial loss: 0.507556\n",
            "epoch 10; iter: 0; batch classifier loss: 0.410594; batch adversarial loss: 0.546456\n",
            "epoch 10; iter: 200; batch classifier loss: 0.532238; batch adversarial loss: 0.566445\n",
            "epoch 11; iter: 0; batch classifier loss: 0.505189; batch adversarial loss: 0.496856\n",
            "epoch 11; iter: 200; batch classifier loss: 0.355916; batch adversarial loss: 0.595846\n",
            "epoch 12; iter: 0; batch classifier loss: 0.442647; batch adversarial loss: 0.496375\n",
            "epoch 12; iter: 200; batch classifier loss: 0.432295; batch adversarial loss: 0.558002\n",
            "epoch 13; iter: 0; batch classifier loss: 0.425051; batch adversarial loss: 0.515432\n",
            "epoch 13; iter: 200; batch classifier loss: 0.369046; batch adversarial loss: 0.512215\n",
            "epoch 14; iter: 0; batch classifier loss: 0.329270; batch adversarial loss: 0.516763\n",
            "epoch 14; iter: 200; batch classifier loss: 0.375769; batch adversarial loss: 0.522202\n",
            "epoch 15; iter: 0; batch classifier loss: 0.408055; batch adversarial loss: 0.508289\n",
            "epoch 15; iter: 200; batch classifier loss: 0.440021; batch adversarial loss: 0.491965\n",
            "epoch 16; iter: 0; batch classifier loss: 0.429262; batch adversarial loss: 0.505331\n",
            "epoch 16; iter: 200; batch classifier loss: 0.429022; batch adversarial loss: 0.547715\n",
            "epoch 17; iter: 0; batch classifier loss: 0.481581; batch adversarial loss: 0.500965\n",
            "epoch 17; iter: 200; batch classifier loss: 0.279972; batch adversarial loss: 0.581049\n",
            "epoch 18; iter: 0; batch classifier loss: 0.474065; batch adversarial loss: 0.517437\n",
            "epoch 18; iter: 200; batch classifier loss: 0.460994; batch adversarial loss: 0.511922\n",
            "epoch 19; iter: 0; batch classifier loss: 0.453099; batch adversarial loss: 0.527013\n",
            "epoch 19; iter: 200; batch classifier loss: 0.455219; batch adversarial loss: 0.521693\n",
            "epoch 20; iter: 0; batch classifier loss: 0.415731; batch adversarial loss: 0.543378\n",
            "epoch 20; iter: 200; batch classifier loss: 0.408758; batch adversarial loss: 0.501305\n",
            "epoch 21; iter: 0; batch classifier loss: 0.495807; batch adversarial loss: 0.511025\n",
            "epoch 21; iter: 200; batch classifier loss: 0.514141; batch adversarial loss: 0.562541\n",
            "epoch 22; iter: 0; batch classifier loss: 0.494560; batch adversarial loss: 0.520708\n",
            "epoch 22; iter: 200; batch classifier loss: 0.377975; batch adversarial loss: 0.573359\n",
            "epoch 23; iter: 0; batch classifier loss: 0.419861; batch adversarial loss: 0.550126\n",
            "epoch 23; iter: 200; batch classifier loss: 0.443348; batch adversarial loss: 0.572030\n",
            "epoch 24; iter: 0; batch classifier loss: 0.459728; batch adversarial loss: 0.606626\n",
            "epoch 24; iter: 200; batch classifier loss: 0.449288; batch adversarial loss: 0.537289\n",
            "epoch 25; iter: 0; batch classifier loss: 0.412148; batch adversarial loss: 0.560359\n",
            "epoch 25; iter: 200; batch classifier loss: 0.471241; batch adversarial loss: 0.510584\n",
            "epoch 26; iter: 0; batch classifier loss: 0.420725; batch adversarial loss: 0.494926\n",
            "epoch 26; iter: 200; batch classifier loss: 0.428053; batch adversarial loss: 0.550518\n",
            "epoch 27; iter: 0; batch classifier loss: 0.409870; batch adversarial loss: 0.545066\n",
            "epoch 27; iter: 200; batch classifier loss: 0.455804; batch adversarial loss: 0.523436\n",
            "epoch 28; iter: 0; batch classifier loss: 0.532868; batch adversarial loss: 0.572719\n",
            "epoch 28; iter: 200; batch classifier loss: 0.431996; batch adversarial loss: 0.560389\n",
            "epoch 29; iter: 0; batch classifier loss: 0.478639; batch adversarial loss: 0.512962\n",
            "epoch 29; iter: 200; batch classifier loss: 0.444717; batch adversarial loss: 0.517970\n",
            "epoch 30; iter: 0; batch classifier loss: 0.443280; batch adversarial loss: 0.510264\n",
            "epoch 30; iter: 200; batch classifier loss: 0.521611; batch adversarial loss: 0.590620\n",
            "epoch 31; iter: 0; batch classifier loss: 0.425893; batch adversarial loss: 0.485093\n",
            "epoch 31; iter: 200; batch classifier loss: 0.447030; batch adversarial loss: 0.535670\n",
            "epoch 32; iter: 0; batch classifier loss: 0.423740; batch adversarial loss: 0.480137\n",
            "epoch 32; iter: 200; batch classifier loss: 0.411390; batch adversarial loss: 0.562788\n",
            "epoch 33; iter: 0; batch classifier loss: 0.478347; batch adversarial loss: 0.511454\n",
            "epoch 33; iter: 200; batch classifier loss: 0.410510; batch adversarial loss: 0.507716\n",
            "epoch 34; iter: 0; batch classifier loss: 0.422758; batch adversarial loss: 0.557938\n",
            "epoch 34; iter: 200; batch classifier loss: 0.444993; batch adversarial loss: 0.609642\n",
            "epoch 35; iter: 0; batch classifier loss: 0.463745; batch adversarial loss: 0.480886\n",
            "epoch 35; iter: 200; batch classifier loss: 0.451165; batch adversarial loss: 0.513371\n",
            "epoch 36; iter: 0; batch classifier loss: 0.482769; batch adversarial loss: 0.452784\n",
            "epoch 36; iter: 200; batch classifier loss: 0.413089; batch adversarial loss: 0.538755\n",
            "epoch 37; iter: 0; batch classifier loss: 0.410281; batch adversarial loss: 0.534543\n",
            "epoch 37; iter: 200; batch classifier loss: 0.340353; batch adversarial loss: 0.570278\n",
            "epoch 38; iter: 0; batch classifier loss: 0.452995; batch adversarial loss: 0.602078\n",
            "epoch 38; iter: 200; batch classifier loss: 0.498732; batch adversarial loss: 0.485720\n",
            "epoch 39; iter: 0; batch classifier loss: 0.453736; batch adversarial loss: 0.488385\n",
            "epoch 39; iter: 200; batch classifier loss: 0.482672; batch adversarial loss: 0.497031\n",
            "epoch 40; iter: 0; batch classifier loss: 0.413966; batch adversarial loss: 0.551263\n",
            "epoch 40; iter: 200; batch classifier loss: 0.403346; batch adversarial loss: 0.471270\n",
            "epoch 41; iter: 0; batch classifier loss: 0.395757; batch adversarial loss: 0.475994\n",
            "epoch 41; iter: 200; batch classifier loss: 0.378134; batch adversarial loss: 0.533940\n",
            "epoch 42; iter: 0; batch classifier loss: 0.363389; batch adversarial loss: 0.518290\n",
            "epoch 42; iter: 200; batch classifier loss: 0.462899; batch adversarial loss: 0.524169\n",
            "epoch 43; iter: 0; batch classifier loss: 0.434440; batch adversarial loss: 0.490040\n",
            "epoch 43; iter: 200; batch classifier loss: 0.367332; batch adversarial loss: 0.569302\n",
            "epoch 44; iter: 0; batch classifier loss: 0.373453; batch adversarial loss: 0.502309\n",
            "epoch 44; iter: 200; batch classifier loss: 0.368526; batch adversarial loss: 0.489570\n",
            "epoch 45; iter: 0; batch classifier loss: 0.469756; batch adversarial loss: 0.470081\n",
            "epoch 45; iter: 200; batch classifier loss: 0.371029; batch adversarial loss: 0.491963\n",
            "epoch 46; iter: 0; batch classifier loss: 0.465614; batch adversarial loss: 0.519594\n",
            "epoch 46; iter: 200; batch classifier loss: 0.448455; batch adversarial loss: 0.451099\n",
            "epoch 47; iter: 0; batch classifier loss: 0.375015; batch adversarial loss: 0.541013\n",
            "epoch 47; iter: 200; batch classifier loss: 0.437261; batch adversarial loss: 0.483619\n",
            "epoch 48; iter: 0; batch classifier loss: 0.408801; batch adversarial loss: 0.441412\n",
            "epoch 48; iter: 200; batch classifier loss: 0.512023; batch adversarial loss: 0.533958\n",
            "epoch 49; iter: 0; batch classifier loss: 0.464562; batch adversarial loss: 0.514478\n",
            "epoch 49; iter: 200; batch classifier loss: 0.531165; batch adversarial loss: 0.405884\n",
            "epoch 0; iter: 0; batch classifier loss: 0.703047; batch adversarial loss: 0.640913\n",
            "epoch 0; iter: 200; batch classifier loss: 0.559836; batch adversarial loss: 0.656217\n",
            "epoch 1; iter: 0; batch classifier loss: 0.430192; batch adversarial loss: 0.659005\n",
            "epoch 1; iter: 200; batch classifier loss: 0.415923; batch adversarial loss: 0.685435\n",
            "epoch 2; iter: 0; batch classifier loss: 0.489399; batch adversarial loss: 0.639829\n",
            "epoch 2; iter: 200; batch classifier loss: 0.569099; batch adversarial loss: 0.633893\n",
            "epoch 3; iter: 0; batch classifier loss: 0.434903; batch adversarial loss: 0.650490\n",
            "epoch 3; iter: 200; batch classifier loss: 0.469294; batch adversarial loss: 0.666952\n",
            "epoch 4; iter: 0; batch classifier loss: 0.534063; batch adversarial loss: 0.618515\n",
            "epoch 4; iter: 200; batch classifier loss: 0.512060; batch adversarial loss: 0.613633\n",
            "epoch 5; iter: 0; batch classifier loss: 0.462864; batch adversarial loss: 0.643715\n",
            "epoch 5; iter: 200; batch classifier loss: 0.528232; batch adversarial loss: 0.591303\n",
            "epoch 6; iter: 0; batch classifier loss: 0.528167; batch adversarial loss: 0.648942\n",
            "epoch 6; iter: 200; batch classifier loss: 0.571313; batch adversarial loss: 0.620658\n",
            "epoch 7; iter: 0; batch classifier loss: 0.482674; batch adversarial loss: 0.609791\n",
            "epoch 7; iter: 200; batch classifier loss: 0.444684; batch adversarial loss: 0.649203\n",
            "epoch 8; iter: 0; batch classifier loss: 0.440876; batch adversarial loss: 0.656640\n",
            "epoch 8; iter: 200; batch classifier loss: 0.467822; batch adversarial loss: 0.629854\n",
            "epoch 9; iter: 0; batch classifier loss: 0.499148; batch adversarial loss: 0.624223\n",
            "epoch 9; iter: 200; batch classifier loss: 0.396027; batch adversarial loss: 0.622492\n",
            "epoch 10; iter: 0; batch classifier loss: 0.403171; batch adversarial loss: 0.593774\n",
            "epoch 10; iter: 200; batch classifier loss: 0.498388; batch adversarial loss: 0.650649\n",
            "epoch 11; iter: 0; batch classifier loss: 0.547684; batch adversarial loss: 0.603170\n",
            "epoch 11; iter: 200; batch classifier loss: 0.467906; batch adversarial loss: 0.641153\n",
            "epoch 12; iter: 0; batch classifier loss: 0.441624; batch adversarial loss: 0.646011\n",
            "epoch 12; iter: 200; batch classifier loss: 0.485770; batch adversarial loss: 0.588981\n",
            "epoch 13; iter: 0; batch classifier loss: 0.490450; batch adversarial loss: 0.587410\n",
            "epoch 13; iter: 200; batch classifier loss: 0.427406; batch adversarial loss: 0.660304\n",
            "epoch 14; iter: 0; batch classifier loss: 0.428844; batch adversarial loss: 0.628359\n",
            "epoch 14; iter: 200; batch classifier loss: 0.517573; batch adversarial loss: 0.631030\n",
            "epoch 15; iter: 0; batch classifier loss: 0.441239; batch adversarial loss: 0.608696\n",
            "epoch 15; iter: 200; batch classifier loss: 0.427002; batch adversarial loss: 0.663726\n",
            "epoch 16; iter: 0; batch classifier loss: 0.523885; batch adversarial loss: 0.598168\n",
            "epoch 16; iter: 200; batch classifier loss: 0.442976; batch adversarial loss: 0.643431\n",
            "epoch 17; iter: 0; batch classifier loss: 0.406566; batch adversarial loss: 0.631761\n",
            "epoch 17; iter: 200; batch classifier loss: 0.504481; batch adversarial loss: 0.610516\n",
            "epoch 18; iter: 0; batch classifier loss: 0.516842; batch adversarial loss: 0.580495\n",
            "epoch 18; iter: 200; batch classifier loss: 0.421715; batch adversarial loss: 0.684798\n",
            "epoch 19; iter: 0; batch classifier loss: 0.418514; batch adversarial loss: 0.614399\n",
            "epoch 19; iter: 200; batch classifier loss: 0.458655; batch adversarial loss: 0.621881\n",
            "epoch 20; iter: 0; batch classifier loss: 0.476680; batch adversarial loss: 0.565557\n",
            "epoch 20; iter: 200; batch classifier loss: 0.405360; batch adversarial loss: 0.575308\n",
            "epoch 21; iter: 0; batch classifier loss: 0.480516; batch adversarial loss: 0.591769\n",
            "epoch 21; iter: 200; batch classifier loss: 0.348603; batch adversarial loss: 0.663639\n",
            "epoch 22; iter: 0; batch classifier loss: 0.460475; batch adversarial loss: 0.629003\n",
            "epoch 22; iter: 200; batch classifier loss: 0.502777; batch adversarial loss: 0.563405\n",
            "epoch 23; iter: 0; batch classifier loss: 0.500553; batch adversarial loss: 0.614339\n",
            "epoch 23; iter: 200; batch classifier loss: 0.440359; batch adversarial loss: 0.588665\n",
            "epoch 24; iter: 0; batch classifier loss: 0.520322; batch adversarial loss: 0.697381\n",
            "epoch 24; iter: 200; batch classifier loss: 0.503159; batch adversarial loss: 0.619243\n",
            "epoch 25; iter: 0; batch classifier loss: 0.584341; batch adversarial loss: 0.609847\n",
            "epoch 25; iter: 200; batch classifier loss: 0.440182; batch adversarial loss: 0.657295\n",
            "epoch 26; iter: 0; batch classifier loss: 0.395329; batch adversarial loss: 0.641289\n",
            "epoch 26; iter: 200; batch classifier loss: 0.496583; batch adversarial loss: 0.650978\n",
            "epoch 27; iter: 0; batch classifier loss: 0.396932; batch adversarial loss: 0.621304\n",
            "epoch 27; iter: 200; batch classifier loss: 0.447737; batch adversarial loss: 0.647258\n",
            "epoch 28; iter: 0; batch classifier loss: 0.471746; batch adversarial loss: 0.669267\n",
            "epoch 28; iter: 200; batch classifier loss: 0.406782; batch adversarial loss: 0.641784\n",
            "epoch 29; iter: 0; batch classifier loss: 0.366119; batch adversarial loss: 0.622456\n",
            "epoch 29; iter: 200; batch classifier loss: 0.524308; batch adversarial loss: 0.596641\n",
            "epoch 30; iter: 0; batch classifier loss: 0.531582; batch adversarial loss: 0.568547\n",
            "epoch 30; iter: 200; batch classifier loss: 0.469607; batch adversarial loss: 0.628689\n",
            "epoch 31; iter: 0; batch classifier loss: 0.550631; batch adversarial loss: 0.646631\n",
            "epoch 31; iter: 200; batch classifier loss: 0.404109; batch adversarial loss: 0.596980\n",
            "epoch 32; iter: 0; batch classifier loss: 0.450244; batch adversarial loss: 0.597834\n",
            "epoch 32; iter: 200; batch classifier loss: 0.553585; batch adversarial loss: 0.635029\n",
            "epoch 33; iter: 0; batch classifier loss: 0.341819; batch adversarial loss: 0.636579\n",
            "epoch 33; iter: 200; batch classifier loss: 0.517593; batch adversarial loss: 0.641045\n",
            "epoch 34; iter: 0; batch classifier loss: 0.535884; batch adversarial loss: 0.636840\n",
            "epoch 34; iter: 200; batch classifier loss: 0.314994; batch adversarial loss: 0.713778\n",
            "epoch 35; iter: 0; batch classifier loss: 0.443389; batch adversarial loss: 0.686266\n",
            "epoch 35; iter: 200; batch classifier loss: 0.487090; batch adversarial loss: 0.625805\n",
            "epoch 36; iter: 0; batch classifier loss: 0.609888; batch adversarial loss: 0.598970\n",
            "epoch 36; iter: 200; batch classifier loss: 0.383061; batch adversarial loss: 0.580330\n",
            "epoch 37; iter: 0; batch classifier loss: 0.444819; batch adversarial loss: 0.630512\n",
            "epoch 37; iter: 200; batch classifier loss: 0.498161; batch adversarial loss: 0.599918\n",
            "epoch 38; iter: 0; batch classifier loss: 0.467610; batch adversarial loss: 0.640432\n",
            "epoch 38; iter: 200; batch classifier loss: 0.504614; batch adversarial loss: 0.587552\n",
            "epoch 39; iter: 0; batch classifier loss: 0.456309; batch adversarial loss: 0.647944\n",
            "epoch 39; iter: 200; batch classifier loss: 0.509568; batch adversarial loss: 0.633742\n",
            "epoch 40; iter: 0; batch classifier loss: 0.459667; batch adversarial loss: 0.598551\n",
            "epoch 40; iter: 200; batch classifier loss: 0.433209; batch adversarial loss: 0.678472\n",
            "epoch 41; iter: 0; batch classifier loss: 0.458171; batch adversarial loss: 0.653929\n",
            "epoch 41; iter: 200; batch classifier loss: 0.457720; batch adversarial loss: 0.619231\n",
            "epoch 42; iter: 0; batch classifier loss: 0.483464; batch adversarial loss: 0.605989\n",
            "epoch 42; iter: 200; batch classifier loss: 0.461220; batch adversarial loss: 0.646536\n",
            "epoch 43; iter: 0; batch classifier loss: 0.419572; batch adversarial loss: 0.619999\n",
            "epoch 43; iter: 200; batch classifier loss: 0.451568; batch adversarial loss: 0.684431\n",
            "epoch 44; iter: 0; batch classifier loss: 0.535882; batch adversarial loss: 0.602866\n",
            "epoch 44; iter: 200; batch classifier loss: 0.387901; batch adversarial loss: 0.634480\n",
            "epoch 45; iter: 0; batch classifier loss: 0.524012; batch adversarial loss: 0.635706\n",
            "epoch 45; iter: 200; batch classifier loss: 0.441381; batch adversarial loss: 0.594927\n",
            "epoch 46; iter: 0; batch classifier loss: 0.445322; batch adversarial loss: 0.648593\n",
            "epoch 46; iter: 200; batch classifier loss: 0.532771; batch adversarial loss: 0.603750\n",
            "epoch 47; iter: 0; batch classifier loss: 0.403859; batch adversarial loss: 0.660740\n",
            "epoch 47; iter: 200; batch classifier loss: 0.509086; batch adversarial loss: 0.596229\n",
            "epoch 48; iter: 0; batch classifier loss: 0.429322; batch adversarial loss: 0.634476\n",
            "epoch 48; iter: 200; batch classifier loss: 0.427956; batch adversarial loss: 0.600252\n",
            "epoch 49; iter: 0; batch classifier loss: 0.417651; batch adversarial loss: 0.618814\n",
            "epoch 49; iter: 200; batch classifier loss: 0.402445; batch adversarial loss: 0.613350\n",
            "epoch 0; iter: 0; batch classifier loss: 0.831335; batch adversarial loss: 0.662015\n",
            "epoch 0; iter: 200; batch classifier loss: 1.329337; batch adversarial loss: 0.723290\n",
            "epoch 1; iter: 0; batch classifier loss: 1.145704; batch adversarial loss: 0.689646\n",
            "epoch 1; iter: 200; batch classifier loss: 1.776672; batch adversarial loss: 0.656322\n",
            "epoch 2; iter: 0; batch classifier loss: 1.396960; batch adversarial loss: 0.635382\n",
            "epoch 2; iter: 200; batch classifier loss: 0.777218; batch adversarial loss: 0.618295\n",
            "epoch 3; iter: 0; batch classifier loss: 0.622511; batch adversarial loss: 0.582524\n",
            "epoch 3; iter: 200; batch classifier loss: 0.617357; batch adversarial loss: 0.658389\n",
            "epoch 4; iter: 0; batch classifier loss: 0.696455; batch adversarial loss: 0.638420\n",
            "epoch 4; iter: 200; batch classifier loss: 0.626594; batch adversarial loss: 0.633048\n",
            "epoch 5; iter: 0; batch classifier loss: 0.703192; batch adversarial loss: 0.601146\n",
            "epoch 5; iter: 200; batch classifier loss: 0.541865; batch adversarial loss: 0.644405\n",
            "epoch 6; iter: 0; batch classifier loss: 0.525606; batch adversarial loss: 0.648392\n",
            "epoch 6; iter: 200; batch classifier loss: 0.654726; batch adversarial loss: 0.606264\n",
            "epoch 7; iter: 0; batch classifier loss: 0.486192; batch adversarial loss: 0.632753\n",
            "epoch 7; iter: 200; batch classifier loss: 0.512243; batch adversarial loss: 0.610114\n",
            "epoch 8; iter: 0; batch classifier loss: 0.650930; batch adversarial loss: 0.631613\n",
            "epoch 8; iter: 200; batch classifier loss: 0.554312; batch adversarial loss: 0.611309\n",
            "epoch 9; iter: 0; batch classifier loss: 0.557544; batch adversarial loss: 0.660447\n",
            "epoch 9; iter: 200; batch classifier loss: 0.693161; batch adversarial loss: 0.627210\n",
            "epoch 10; iter: 0; batch classifier loss: 0.575908; batch adversarial loss: 0.568810\n",
            "epoch 10; iter: 200; batch classifier loss: 0.652565; batch adversarial loss: 0.627923\n",
            "epoch 11; iter: 0; batch classifier loss: 0.559852; batch adversarial loss: 0.616513\n",
            "epoch 11; iter: 200; batch classifier loss: 0.574821; batch adversarial loss: 0.691873\n",
            "epoch 12; iter: 0; batch classifier loss: 0.524521; batch adversarial loss: 0.616401\n",
            "epoch 12; iter: 200; batch classifier loss: 0.599431; batch adversarial loss: 0.645022\n",
            "epoch 13; iter: 0; batch classifier loss: 0.627047; batch adversarial loss: 0.605772\n",
            "epoch 13; iter: 200; batch classifier loss: 0.787397; batch adversarial loss: 0.622975\n",
            "epoch 14; iter: 0; batch classifier loss: 0.663313; batch adversarial loss: 0.631310\n",
            "epoch 14; iter: 200; batch classifier loss: 0.642049; batch adversarial loss: 0.669473\n",
            "epoch 15; iter: 0; batch classifier loss: 0.702906; batch adversarial loss: 0.655233\n",
            "epoch 15; iter: 200; batch classifier loss: 0.575313; batch adversarial loss: 0.645393\n",
            "epoch 16; iter: 0; batch classifier loss: 0.780449; batch adversarial loss: 0.582103\n",
            "epoch 16; iter: 200; batch classifier loss: 0.511375; batch adversarial loss: 0.663134\n",
            "epoch 17; iter: 0; batch classifier loss: 0.506357; batch adversarial loss: 0.617228\n",
            "epoch 17; iter: 200; batch classifier loss: 0.565806; batch adversarial loss: 0.647674\n",
            "epoch 18; iter: 0; batch classifier loss: 0.594568; batch adversarial loss: 0.649135\n",
            "epoch 18; iter: 200; batch classifier loss: 0.534986; batch adversarial loss: 0.649556\n",
            "epoch 19; iter: 0; batch classifier loss: 0.635864; batch adversarial loss: 0.656412\n",
            "epoch 19; iter: 200; batch classifier loss: 0.644250; batch adversarial loss: 0.622979\n",
            "epoch 20; iter: 0; batch classifier loss: 0.495949; batch adversarial loss: 0.644159\n",
            "epoch 20; iter: 200; batch classifier loss: 0.819797; batch adversarial loss: 0.676584\n",
            "epoch 21; iter: 0; batch classifier loss: 0.622527; batch adversarial loss: 0.661787\n",
            "epoch 21; iter: 200; batch classifier loss: 0.685169; batch adversarial loss: 0.657117\n",
            "epoch 22; iter: 0; batch classifier loss: 0.582697; batch adversarial loss: 0.694096\n",
            "epoch 22; iter: 200; batch classifier loss: 0.575321; batch adversarial loss: 0.669492\n",
            "epoch 23; iter: 0; batch classifier loss: 0.564005; batch adversarial loss: 0.601981\n",
            "epoch 23; iter: 200; batch classifier loss: 0.599486; batch adversarial loss: 0.594597\n",
            "epoch 24; iter: 0; batch classifier loss: 0.583900; batch adversarial loss: 0.638806\n",
            "epoch 24; iter: 200; batch classifier loss: 0.848711; batch adversarial loss: 0.618102\n",
            "epoch 25; iter: 0; batch classifier loss: 0.817368; batch adversarial loss: 0.697812\n",
            "epoch 25; iter: 200; batch classifier loss: 0.711522; batch adversarial loss: 0.616095\n",
            "epoch 26; iter: 0; batch classifier loss: 0.611610; batch adversarial loss: 0.604078\n",
            "epoch 26; iter: 200; batch classifier loss: 0.647165; batch adversarial loss: 0.626781\n",
            "epoch 27; iter: 0; batch classifier loss: 0.590990; batch adversarial loss: 0.686286\n",
            "epoch 27; iter: 200; batch classifier loss: 0.883222; batch adversarial loss: 0.671233\n",
            "epoch 28; iter: 0; batch classifier loss: 0.962132; batch adversarial loss: 0.574659\n",
            "epoch 28; iter: 200; batch classifier loss: 0.664865; batch adversarial loss: 0.708150\n",
            "epoch 29; iter: 0; batch classifier loss: 0.625369; batch adversarial loss: 0.596328\n",
            "epoch 29; iter: 200; batch classifier loss: 0.577298; batch adversarial loss: 0.654682\n",
            "epoch 30; iter: 0; batch classifier loss: 0.643183; batch adversarial loss: 0.626385\n",
            "epoch 30; iter: 200; batch classifier loss: 0.759709; batch adversarial loss: 0.595979\n",
            "epoch 31; iter: 0; batch classifier loss: 0.582964; batch adversarial loss: 0.585555\n",
            "epoch 31; iter: 200; batch classifier loss: 0.563893; batch adversarial loss: 0.685917\n",
            "epoch 32; iter: 0; batch classifier loss: 0.636979; batch adversarial loss: 0.647788\n",
            "epoch 32; iter: 200; batch classifier loss: 0.439496; batch adversarial loss: 0.666676\n",
            "epoch 33; iter: 0; batch classifier loss: 0.506673; batch adversarial loss: 0.643979\n",
            "epoch 33; iter: 200; batch classifier loss: 0.658020; batch adversarial loss: 0.650710\n",
            "epoch 34; iter: 0; batch classifier loss: 0.802116; batch adversarial loss: 0.610883\n",
            "epoch 34; iter: 200; batch classifier loss: 0.494649; batch adversarial loss: 0.676406\n",
            "epoch 35; iter: 0; batch classifier loss: 0.564642; batch adversarial loss: 0.647049\n",
            "epoch 35; iter: 200; batch classifier loss: 0.775713; batch adversarial loss: 0.611326\n",
            "epoch 36; iter: 0; batch classifier loss: 0.671707; batch adversarial loss: 0.610110\n",
            "epoch 36; iter: 200; batch classifier loss: 0.488056; batch adversarial loss: 0.609895\n",
            "epoch 37; iter: 0; batch classifier loss: 0.621568; batch adversarial loss: 0.569077\n",
            "epoch 37; iter: 200; batch classifier loss: 0.390109; batch adversarial loss: 0.662176\n",
            "epoch 38; iter: 0; batch classifier loss: 0.562392; batch adversarial loss: 0.600964\n",
            "epoch 38; iter: 200; batch classifier loss: 0.524905; batch adversarial loss: 0.642846\n",
            "epoch 39; iter: 0; batch classifier loss: 0.635099; batch adversarial loss: 0.612485\n",
            "epoch 39; iter: 200; batch classifier loss: 0.627798; batch adversarial loss: 0.599104\n",
            "epoch 40; iter: 0; batch classifier loss: 0.543082; batch adversarial loss: 0.603293\n",
            "epoch 40; iter: 200; batch classifier loss: 0.749982; batch adversarial loss: 0.611757\n",
            "epoch 41; iter: 0; batch classifier loss: 0.662058; batch adversarial loss: 0.648404\n",
            "epoch 41; iter: 200; batch classifier loss: 0.924188; batch adversarial loss: 0.607152\n",
            "epoch 42; iter: 0; batch classifier loss: 0.591639; batch adversarial loss: 0.632323\n",
            "epoch 42; iter: 200; batch classifier loss: 0.579640; batch adversarial loss: 0.644903\n",
            "epoch 43; iter: 0; batch classifier loss: 0.677202; batch adversarial loss: 0.607010\n",
            "epoch 43; iter: 200; batch classifier loss: 0.651371; batch adversarial loss: 0.667213\n",
            "epoch 44; iter: 0; batch classifier loss: 0.805274; batch adversarial loss: 0.660772\n",
            "epoch 44; iter: 200; batch classifier loss: 0.738764; batch adversarial loss: 0.649349\n",
            "epoch 45; iter: 0; batch classifier loss: 0.735794; batch adversarial loss: 0.603277\n",
            "epoch 45; iter: 200; batch classifier loss: 0.511057; batch adversarial loss: 0.658708\n",
            "epoch 46; iter: 0; batch classifier loss: 0.597048; batch adversarial loss: 0.568010\n",
            "epoch 46; iter: 200; batch classifier loss: 0.700711; batch adversarial loss: 0.600030\n",
            "epoch 47; iter: 0; batch classifier loss: 0.657168; batch adversarial loss: 0.696577\n",
            "epoch 47; iter: 200; batch classifier loss: 0.642835; batch adversarial loss: 0.623530\n",
            "epoch 48; iter: 0; batch classifier loss: 0.550705; batch adversarial loss: 0.623946\n",
            "epoch 48; iter: 200; batch classifier loss: 0.798642; batch adversarial loss: 0.666301\n",
            "epoch 49; iter: 0; batch classifier loss: 0.862614; batch adversarial loss: 0.684700\n",
            "epoch 49; iter: 200; batch classifier loss: 0.598476; batch adversarial loss: 0.644408\n",
            "epoch 0; iter: 0; batch classifier loss: 0.853863; batch adversarial loss: 0.714769\n",
            "epoch 0; iter: 200; batch classifier loss: 2.485520; batch adversarial loss: 0.661954\n",
            "epoch 1; iter: 0; batch classifier loss: 2.852448; batch adversarial loss: 0.699528\n",
            "epoch 1; iter: 200; batch classifier loss: 3.740911; batch adversarial loss: 0.660527\n",
            "epoch 2; iter: 0; batch classifier loss: 3.842328; batch adversarial loss: 0.642936\n",
            "epoch 2; iter: 200; batch classifier loss: 2.947853; batch adversarial loss: 0.585799\n",
            "epoch 3; iter: 0; batch classifier loss: 2.610755; batch adversarial loss: 0.608602\n",
            "epoch 3; iter: 200; batch classifier loss: 2.006090; batch adversarial loss: 0.650792\n",
            "epoch 4; iter: 0; batch classifier loss: 1.489631; batch adversarial loss: 0.635037\n",
            "epoch 4; iter: 200; batch classifier loss: 1.962708; batch adversarial loss: 0.644004\n",
            "epoch 5; iter: 0; batch classifier loss: 1.687256; batch adversarial loss: 0.597994\n",
            "epoch 5; iter: 200; batch classifier loss: 1.605874; batch adversarial loss: 0.668464\n",
            "epoch 6; iter: 0; batch classifier loss: 1.979245; batch adversarial loss: 0.683652\n",
            "epoch 6; iter: 200; batch classifier loss: 1.560267; batch adversarial loss: 0.643556\n",
            "epoch 7; iter: 0; batch classifier loss: 1.463958; batch adversarial loss: 0.654093\n",
            "epoch 7; iter: 200; batch classifier loss: 1.431393; batch adversarial loss: 0.664839\n",
            "epoch 8; iter: 0; batch classifier loss: 1.704991; batch adversarial loss: 0.590146\n",
            "epoch 8; iter: 200; batch classifier loss: 1.654117; batch adversarial loss: 0.627473\n",
            "epoch 9; iter: 0; batch classifier loss: 1.349159; batch adversarial loss: 0.627481\n",
            "epoch 9; iter: 200; batch classifier loss: 1.276238; batch adversarial loss: 0.638326\n",
            "epoch 10; iter: 0; batch classifier loss: 1.419866; batch adversarial loss: 0.616606\n",
            "epoch 10; iter: 200; batch classifier loss: 1.440337; batch adversarial loss: 0.704115\n",
            "epoch 11; iter: 0; batch classifier loss: 1.538314; batch adversarial loss: 0.611066\n",
            "epoch 11; iter: 200; batch classifier loss: 1.308134; batch adversarial loss: 0.643797\n",
            "epoch 12; iter: 0; batch classifier loss: 1.138374; batch adversarial loss: 0.632884\n",
            "epoch 12; iter: 200; batch classifier loss: 1.235907; batch adversarial loss: 0.638332\n",
            "epoch 13; iter: 0; batch classifier loss: 1.094621; batch adversarial loss: 0.621959\n",
            "epoch 13; iter: 200; batch classifier loss: 0.936682; batch adversarial loss: 0.676283\n",
            "epoch 14; iter: 0; batch classifier loss: 0.952313; batch adversarial loss: 0.703869\n",
            "epoch 14; iter: 200; batch classifier loss: 0.961333; batch adversarial loss: 0.654755\n",
            "epoch 15; iter: 0; batch classifier loss: 0.575190; batch adversarial loss: 0.590945\n",
            "epoch 15; iter: 200; batch classifier loss: 1.562554; batch adversarial loss: 0.643843\n",
            "epoch 16; iter: 0; batch classifier loss: 1.815797; batch adversarial loss: 0.616458\n",
            "epoch 16; iter: 200; batch classifier loss: 1.860448; batch adversarial loss: 0.638317\n",
            "epoch 17; iter: 0; batch classifier loss: 1.893452; batch adversarial loss: 0.611084\n",
            "epoch 17; iter: 200; batch classifier loss: 1.788934; batch adversarial loss: 0.616580\n",
            "epoch 18; iter: 0; batch classifier loss: 1.157292; batch adversarial loss: 0.665442\n",
            "epoch 18; iter: 200; batch classifier loss: 2.107574; batch adversarial loss: 0.578117\n",
            "epoch 19; iter: 0; batch classifier loss: 1.171542; batch adversarial loss: 0.632893\n",
            "epoch 19; iter: 200; batch classifier loss: 1.036642; batch adversarial loss: 0.665474\n",
            "epoch 20; iter: 0; batch classifier loss: 1.490066; batch adversarial loss: 0.632887\n",
            "epoch 20; iter: 200; batch classifier loss: 1.339625; batch adversarial loss: 0.610919\n",
            "epoch 21; iter: 0; batch classifier loss: 1.650182; batch adversarial loss: 0.649257\n",
            "epoch 21; iter: 200; batch classifier loss: 1.583744; batch adversarial loss: 0.605563\n",
            "epoch 22; iter: 0; batch classifier loss: 1.367616; batch adversarial loss: 0.643770\n",
            "epoch 22; iter: 200; batch classifier loss: 1.578165; batch adversarial loss: 0.573213\n",
            "epoch 23; iter: 0; batch classifier loss: 1.203994; batch adversarial loss: 0.622061\n",
            "epoch 23; iter: 200; batch classifier loss: 1.107221; batch adversarial loss: 0.708892\n",
            "epoch 24; iter: 0; batch classifier loss: 1.127335; batch adversarial loss: 0.632923\n",
            "epoch 24; iter: 200; batch classifier loss: 1.612910; batch adversarial loss: 0.605619\n",
            "epoch 25; iter: 0; batch classifier loss: 1.791022; batch adversarial loss: 0.665497\n",
            "epoch 25; iter: 200; batch classifier loss: 1.517356; batch adversarial loss: 0.719820\n",
            "epoch 26; iter: 0; batch classifier loss: 1.662910; batch adversarial loss: 0.676427\n",
            "epoch 26; iter: 200; batch classifier loss: 1.810318; batch adversarial loss: 0.665373\n",
            "epoch 27; iter: 0; batch classifier loss: 1.832376; batch adversarial loss: 0.611145\n",
            "epoch 27; iter: 200; batch classifier loss: 1.799865; batch adversarial loss: 0.627448\n",
            "epoch 28; iter: 0; batch classifier loss: 1.609880; batch adversarial loss: 0.594897\n",
            "epoch 28; iter: 200; batch classifier loss: 1.982289; batch adversarial loss: 0.638338\n",
            "epoch 29; iter: 0; batch classifier loss: 1.345520; batch adversarial loss: 0.649193\n",
            "epoch 29; iter: 200; batch classifier loss: 1.538987; batch adversarial loss: 0.687497\n",
            "epoch 30; iter: 0; batch classifier loss: 1.544933; batch adversarial loss: 0.616623\n",
            "epoch 30; iter: 200; batch classifier loss: 1.345108; batch adversarial loss: 0.643734\n",
            "epoch 31; iter: 0; batch classifier loss: 1.519555; batch adversarial loss: 0.665578\n",
            "epoch 31; iter: 200; batch classifier loss: 1.398859; batch adversarial loss: 0.660085\n",
            "epoch 32; iter: 0; batch classifier loss: 1.204557; batch adversarial loss: 0.638929\n",
            "epoch 32; iter: 200; batch classifier loss: 1.964795; batch adversarial loss: 0.568080\n",
            "epoch 33; iter: 0; batch classifier loss: 2.265524; batch adversarial loss: 0.681792\n",
            "epoch 33; iter: 200; batch classifier loss: 1.542194; batch adversarial loss: 0.709517\n",
            "epoch 34; iter: 0; batch classifier loss: 1.680753; batch adversarial loss: 0.622026\n",
            "epoch 34; iter: 200; batch classifier loss: 1.389656; batch adversarial loss: 0.594723\n",
            "epoch 35; iter: 0; batch classifier loss: 1.521391; batch adversarial loss: 0.681952\n",
            "epoch 35; iter: 200; batch classifier loss: 1.790010; batch adversarial loss: 0.616666\n",
            "epoch 36; iter: 0; batch classifier loss: 1.774612; batch adversarial loss: 0.600107\n",
            "epoch 36; iter: 200; batch classifier loss: 1.459647; batch adversarial loss: 0.660123\n",
            "epoch 37; iter: 0; batch classifier loss: 1.297009; batch adversarial loss: 0.627431\n",
            "epoch 37; iter: 200; batch classifier loss: 1.052185; batch adversarial loss: 0.638434\n",
            "epoch 38; iter: 0; batch classifier loss: 0.714648; batch adversarial loss: 0.673096\n",
            "epoch 38; iter: 200; batch classifier loss: 1.977918; batch adversarial loss: 0.681996\n",
            "epoch 39; iter: 0; batch classifier loss: 1.652044; batch adversarial loss: 0.594819\n",
            "epoch 39; iter: 200; batch classifier loss: 1.868534; batch adversarial loss: 0.654732\n",
            "epoch 40; iter: 0; batch classifier loss: 2.011663; batch adversarial loss: 0.632874\n",
            "epoch 40; iter: 200; batch classifier loss: 2.012379; batch adversarial loss: 0.638365\n",
            "epoch 41; iter: 0; batch classifier loss: 1.878883; batch adversarial loss: 0.632879\n",
            "epoch 41; iter: 200; batch classifier loss: 1.773871; batch adversarial loss: 0.632886\n",
            "epoch 42; iter: 0; batch classifier loss: 1.313712; batch adversarial loss: 0.654675\n",
            "epoch 42; iter: 200; batch classifier loss: 1.610995; batch adversarial loss: 0.671225\n",
            "epoch 43; iter: 0; batch classifier loss: 2.047894; batch adversarial loss: 0.594707\n",
            "epoch 43; iter: 200; batch classifier loss: 2.115630; batch adversarial loss: 0.627390\n",
            "epoch 44; iter: 0; batch classifier loss: 1.661551; batch adversarial loss: 0.638329\n",
            "epoch 44; iter: 200; batch classifier loss: 1.261539; batch adversarial loss: 0.692939\n",
            "epoch 45; iter: 0; batch classifier loss: 1.662271; batch adversarial loss: 0.632871\n",
            "epoch 45; iter: 200; batch classifier loss: 1.623463; batch adversarial loss: 0.600158\n",
            "epoch 46; iter: 0; batch classifier loss: 1.502773; batch adversarial loss: 0.605592\n",
            "epoch 46; iter: 200; batch classifier loss: 1.096015; batch adversarial loss: 0.649279\n",
            "epoch 47; iter: 0; batch classifier loss: 1.221318; batch adversarial loss: 0.660128\n",
            "epoch 47; iter: 200; batch classifier loss: 2.081170; batch adversarial loss: 0.654602\n",
            "epoch 48; iter: 0; batch classifier loss: 2.084522; batch adversarial loss: 0.643778\n",
            "epoch 48; iter: 200; batch classifier loss: 2.314545; batch adversarial loss: 0.687406\n",
            "epoch 49; iter: 0; batch classifier loss: 1.586461; batch adversarial loss: 0.627403\n",
            "epoch 49; iter: 200; batch classifier loss: 2.093936; batch adversarial loss: 0.638349\n",
            "\n",
            " Welcome to Fold number 1 \n",
            "\n",
            "epoch 0; iter: 0; batch classifier loss: 0.725329; batch adversarial loss: 0.977032\n",
            "epoch 0; iter: 200; batch classifier loss: 0.443499; batch adversarial loss: 0.827504\n",
            "epoch 1; iter: 0; batch classifier loss: 0.432677; batch adversarial loss: 0.822406\n",
            "epoch 1; iter: 200; batch classifier loss: 0.481694; batch adversarial loss: 0.773703\n",
            "epoch 2; iter: 0; batch classifier loss: 0.454785; batch adversarial loss: 0.727007\n",
            "epoch 2; iter: 200; batch classifier loss: 0.354829; batch adversarial loss: 0.702142\n",
            "epoch 3; iter: 0; batch classifier loss: 0.526149; batch adversarial loss: 0.671853\n",
            "epoch 3; iter: 200; batch classifier loss: 0.456628; batch adversarial loss: 0.688162\n",
            "epoch 4; iter: 0; batch classifier loss: 0.510718; batch adversarial loss: 0.679303\n",
            "epoch 4; iter: 200; batch classifier loss: 0.478928; batch adversarial loss: 0.583943\n",
            "epoch 5; iter: 0; batch classifier loss: 0.443968; batch adversarial loss: 0.716059\n",
            "epoch 5; iter: 200; batch classifier loss: 0.386057; batch adversarial loss: 0.669879\n",
            "epoch 6; iter: 0; batch classifier loss: 0.503728; batch adversarial loss: 0.646625\n",
            "epoch 6; iter: 200; batch classifier loss: 0.392343; batch adversarial loss: 0.644562\n",
            "epoch 7; iter: 0; batch classifier loss: 0.411492; batch adversarial loss: 0.625048\n",
            "epoch 7; iter: 200; batch classifier loss: 0.504990; batch adversarial loss: 0.619143\n",
            "epoch 8; iter: 0; batch classifier loss: 0.416010; batch adversarial loss: 0.624227\n",
            "epoch 8; iter: 200; batch classifier loss: 0.408828; batch adversarial loss: 0.659011\n",
            "epoch 9; iter: 0; batch classifier loss: 0.538512; batch adversarial loss: 0.673076\n",
            "epoch 9; iter: 200; batch classifier loss: 0.475054; batch adversarial loss: 0.656761\n",
            "epoch 10; iter: 0; batch classifier loss: 0.483562; batch adversarial loss: 0.652044\n",
            "epoch 10; iter: 200; batch classifier loss: 0.398283; batch adversarial loss: 0.609502\n",
            "epoch 11; iter: 0; batch classifier loss: 0.534751; batch adversarial loss: 0.642489\n",
            "epoch 11; iter: 200; batch classifier loss: 0.381292; batch adversarial loss: 0.609263\n",
            "epoch 12; iter: 0; batch classifier loss: 0.409233; batch adversarial loss: 0.592986\n",
            "epoch 12; iter: 200; batch classifier loss: 0.444222; batch adversarial loss: 0.651191\n",
            "epoch 13; iter: 0; batch classifier loss: 0.524768; batch adversarial loss: 0.649145\n",
            "epoch 13; iter: 200; batch classifier loss: 0.464541; batch adversarial loss: 0.688009\n",
            "epoch 14; iter: 0; batch classifier loss: 0.400741; batch adversarial loss: 0.611075\n",
            "epoch 14; iter: 200; batch classifier loss: 0.445033; batch adversarial loss: 0.602821\n",
            "epoch 15; iter: 0; batch classifier loss: 0.355456; batch adversarial loss: 0.581208\n",
            "epoch 15; iter: 200; batch classifier loss: 0.431798; batch adversarial loss: 0.628201\n",
            "epoch 16; iter: 0; batch classifier loss: 0.534579; batch adversarial loss: 0.570218\n",
            "epoch 16; iter: 200; batch classifier loss: 0.437234; batch adversarial loss: 0.558494\n",
            "epoch 17; iter: 0; batch classifier loss: 0.406606; batch adversarial loss: 0.627466\n",
            "epoch 17; iter: 200; batch classifier loss: 0.383055; batch adversarial loss: 0.606191\n",
            "epoch 18; iter: 0; batch classifier loss: 0.495066; batch adversarial loss: 0.657669\n",
            "epoch 18; iter: 200; batch classifier loss: 0.393713; batch adversarial loss: 0.645496\n",
            "epoch 19; iter: 0; batch classifier loss: 0.455010; batch adversarial loss: 0.590380\n",
            "epoch 19; iter: 200; batch classifier loss: 0.438514; batch adversarial loss: 0.614638\n",
            "epoch 20; iter: 0; batch classifier loss: 0.373530; batch adversarial loss: 0.606123\n",
            "epoch 20; iter: 200; batch classifier loss: 0.416465; batch adversarial loss: 0.555597\n",
            "epoch 21; iter: 0; batch classifier loss: 0.363607; batch adversarial loss: 0.528080\n",
            "epoch 21; iter: 200; batch classifier loss: 0.467190; batch adversarial loss: 0.586908\n",
            "epoch 22; iter: 0; batch classifier loss: 0.478661; batch adversarial loss: 0.577679\n",
            "epoch 22; iter: 200; batch classifier loss: 0.312823; batch adversarial loss: 0.594464\n",
            "epoch 23; iter: 0; batch classifier loss: 0.429152; batch adversarial loss: 0.610579\n",
            "epoch 23; iter: 200; batch classifier loss: 0.415611; batch adversarial loss: 0.556681\n",
            "epoch 24; iter: 0; batch classifier loss: 0.311903; batch adversarial loss: 0.584778\n",
            "epoch 24; iter: 200; batch classifier loss: 0.435927; batch adversarial loss: 0.485041\n",
            "epoch 25; iter: 0; batch classifier loss: 0.448288; batch adversarial loss: 0.513877\n",
            "epoch 25; iter: 200; batch classifier loss: 0.426703; batch adversarial loss: 0.531179\n",
            "epoch 26; iter: 0; batch classifier loss: 0.430157; batch adversarial loss: 0.603434\n",
            "epoch 26; iter: 200; batch classifier loss: 0.342951; batch adversarial loss: 0.570257\n",
            "epoch 27; iter: 0; batch classifier loss: 0.432892; batch adversarial loss: 0.556602\n",
            "epoch 27; iter: 200; batch classifier loss: 0.408400; batch adversarial loss: 0.458596\n",
            "epoch 28; iter: 0; batch classifier loss: 0.444580; batch adversarial loss: 0.532883\n",
            "epoch 28; iter: 200; batch classifier loss: 0.331635; batch adversarial loss: 0.507629\n",
            "epoch 29; iter: 0; batch classifier loss: 0.344873; batch adversarial loss: 0.506367\n",
            "epoch 29; iter: 200; batch classifier loss: 0.436254; batch adversarial loss: 0.469841\n",
            "epoch 30; iter: 0; batch classifier loss: 0.386986; batch adversarial loss: 0.528328\n",
            "epoch 30; iter: 200; batch classifier loss: 0.430774; batch adversarial loss: 0.461691\n",
            "epoch 31; iter: 0; batch classifier loss: 0.410115; batch adversarial loss: 0.540722\n",
            "epoch 31; iter: 200; batch classifier loss: 0.435097; batch adversarial loss: 0.519273\n",
            "epoch 32; iter: 0; batch classifier loss: 0.337490; batch adversarial loss: 0.574587\n",
            "epoch 32; iter: 200; batch classifier loss: 0.465373; batch adversarial loss: 0.481950\n",
            "epoch 33; iter: 0; batch classifier loss: 0.368546; batch adversarial loss: 0.542013\n",
            "epoch 33; iter: 200; batch classifier loss: 0.389860; batch adversarial loss: 0.523411\n",
            "epoch 34; iter: 0; batch classifier loss: 0.406420; batch adversarial loss: 0.518237\n",
            "epoch 34; iter: 200; batch classifier loss: 0.459013; batch adversarial loss: 0.548092\n",
            "epoch 35; iter: 0; batch classifier loss: 0.423868; batch adversarial loss: 0.494661\n",
            "epoch 35; iter: 200; batch classifier loss: 0.440745; batch adversarial loss: 0.535862\n",
            "epoch 36; iter: 0; batch classifier loss: 0.437910; batch adversarial loss: 0.538529\n",
            "epoch 36; iter: 200; batch classifier loss: 0.426901; batch adversarial loss: 0.551915\n",
            "epoch 37; iter: 0; batch classifier loss: 0.312502; batch adversarial loss: 0.551868\n",
            "epoch 37; iter: 200; batch classifier loss: 0.407268; batch adversarial loss: 0.564985\n",
            "epoch 38; iter: 0; batch classifier loss: 0.354353; batch adversarial loss: 0.576865\n",
            "epoch 38; iter: 200; batch classifier loss: 0.415247; batch adversarial loss: 0.500009\n",
            "epoch 39; iter: 0; batch classifier loss: 0.406086; batch adversarial loss: 0.500132\n",
            "epoch 39; iter: 200; batch classifier loss: 0.426205; batch adversarial loss: 0.520977\n",
            "epoch 40; iter: 0; batch classifier loss: 0.419101; batch adversarial loss: 0.522993\n",
            "epoch 40; iter: 200; batch classifier loss: 0.466970; batch adversarial loss: 0.564889\n",
            "epoch 41; iter: 0; batch classifier loss: 0.452914; batch adversarial loss: 0.608637\n",
            "epoch 41; iter: 200; batch classifier loss: 0.429090; batch adversarial loss: 0.528117\n",
            "epoch 42; iter: 0; batch classifier loss: 0.435346; batch adversarial loss: 0.481162\n",
            "epoch 42; iter: 200; batch classifier loss: 0.432200; batch adversarial loss: 0.564182\n",
            "epoch 43; iter: 0; batch classifier loss: 0.447743; batch adversarial loss: 0.520377\n",
            "epoch 43; iter: 200; batch classifier loss: 0.365494; batch adversarial loss: 0.544702\n",
            "epoch 44; iter: 0; batch classifier loss: 0.382781; batch adversarial loss: 0.447980\n",
            "epoch 44; iter: 200; batch classifier loss: 0.390402; batch adversarial loss: 0.478358\n",
            "epoch 45; iter: 0; batch classifier loss: 0.476109; batch adversarial loss: 0.523994\n",
            "epoch 45; iter: 200; batch classifier loss: 0.437994; batch adversarial loss: 0.598884\n",
            "epoch 46; iter: 0; batch classifier loss: 0.431416; batch adversarial loss: 0.562625\n",
            "epoch 46; iter: 200; batch classifier loss: 0.462988; batch adversarial loss: 0.451460\n",
            "epoch 47; iter: 0; batch classifier loss: 0.443881; batch adversarial loss: 0.407852\n",
            "epoch 47; iter: 200; batch classifier loss: 0.401304; batch adversarial loss: 0.462316\n",
            "epoch 48; iter: 0; batch classifier loss: 0.437296; batch adversarial loss: 0.514396\n",
            "epoch 48; iter: 200; batch classifier loss: 0.437696; batch adversarial loss: 0.498272\n",
            "epoch 49; iter: 0; batch classifier loss: 0.457683; batch adversarial loss: 0.509155\n",
            "epoch 49; iter: 200; batch classifier loss: 0.467167; batch adversarial loss: 0.496931\n",
            "epoch 0; iter: 0; batch classifier loss: 0.672501; batch adversarial loss: 0.729467\n",
            "epoch 0; iter: 200; batch classifier loss: 0.422986; batch adversarial loss: 0.677266\n",
            "epoch 1; iter: 0; batch classifier loss: 0.514094; batch adversarial loss: 0.655382\n",
            "epoch 1; iter: 200; batch classifier loss: 0.477268; batch adversarial loss: 0.610134\n",
            "epoch 2; iter: 0; batch classifier loss: 0.405780; batch adversarial loss: 0.596877\n",
            "epoch 2; iter: 200; batch classifier loss: 0.446875; batch adversarial loss: 0.513906\n",
            "epoch 3; iter: 0; batch classifier loss: 0.446178; batch adversarial loss: 0.479984\n",
            "epoch 3; iter: 200; batch classifier loss: 0.507090; batch adversarial loss: 0.501164\n",
            "epoch 4; iter: 0; batch classifier loss: 0.464762; batch adversarial loss: 0.500779\n",
            "epoch 4; iter: 200; batch classifier loss: 0.407121; batch adversarial loss: 0.536133\n",
            "epoch 5; iter: 0; batch classifier loss: 0.413155; batch adversarial loss: 0.536807\n",
            "epoch 5; iter: 200; batch classifier loss: 0.524179; batch adversarial loss: 0.508802\n",
            "epoch 6; iter: 0; batch classifier loss: 0.473792; batch adversarial loss: 0.530465\n",
            "epoch 6; iter: 200; batch classifier loss: 0.402550; batch adversarial loss: 0.559762\n",
            "epoch 7; iter: 0; batch classifier loss: 0.385877; batch adversarial loss: 0.596477\n",
            "epoch 7; iter: 200; batch classifier loss: 0.455071; batch adversarial loss: 0.626411\n",
            "epoch 8; iter: 0; batch classifier loss: 0.456494; batch adversarial loss: 0.585222\n",
            "epoch 8; iter: 200; batch classifier loss: 0.420041; batch adversarial loss: 0.671499\n",
            "epoch 9; iter: 0; batch classifier loss: 0.446230; batch adversarial loss: 0.546477\n",
            "epoch 9; iter: 200; batch classifier loss: 0.412889; batch adversarial loss: 0.660441\n",
            "epoch 10; iter: 0; batch classifier loss: 0.463790; batch adversarial loss: 0.636496\n",
            "epoch 10; iter: 200; batch classifier loss: 0.461688; batch adversarial loss: 0.592960\n",
            "epoch 11; iter: 0; batch classifier loss: 0.417953; batch adversarial loss: 0.675151\n",
            "epoch 11; iter: 200; batch classifier loss: 0.439348; batch adversarial loss: 0.652898\n",
            "epoch 12; iter: 0; batch classifier loss: 0.463568; batch adversarial loss: 0.636234\n",
            "epoch 12; iter: 200; batch classifier loss: 0.470625; batch adversarial loss: 0.661686\n",
            "epoch 13; iter: 0; batch classifier loss: 0.413599; batch adversarial loss: 0.716032\n",
            "epoch 13; iter: 200; batch classifier loss: 0.478585; batch adversarial loss: 0.624078\n",
            "epoch 14; iter: 0; batch classifier loss: 0.415744; batch adversarial loss: 0.683578\n",
            "epoch 14; iter: 200; batch classifier loss: 0.462895; batch adversarial loss: 0.642482\n",
            "epoch 15; iter: 0; batch classifier loss: 0.411655; batch adversarial loss: 0.632500\n",
            "epoch 15; iter: 200; batch classifier loss: 0.480072; batch adversarial loss: 0.562030\n",
            "epoch 16; iter: 0; batch classifier loss: 0.492177; batch adversarial loss: 0.619259\n",
            "epoch 16; iter: 200; batch classifier loss: 0.501444; batch adversarial loss: 0.631844\n",
            "epoch 17; iter: 0; batch classifier loss: 0.419278; batch adversarial loss: 0.626398\n",
            "epoch 17; iter: 200; batch classifier loss: 0.471764; batch adversarial loss: 0.612832\n",
            "epoch 18; iter: 0; batch classifier loss: 0.346552; batch adversarial loss: 0.652186\n",
            "epoch 18; iter: 200; batch classifier loss: 0.496465; batch adversarial loss: 0.641560\n",
            "epoch 19; iter: 0; batch classifier loss: 0.353906; batch adversarial loss: 0.646323\n",
            "epoch 19; iter: 200; batch classifier loss: 0.504795; batch adversarial loss: 0.593678\n",
            "epoch 20; iter: 0; batch classifier loss: 0.400786; batch adversarial loss: 0.586766\n",
            "epoch 20; iter: 200; batch classifier loss: 0.492092; batch adversarial loss: 0.604313\n",
            "epoch 21; iter: 0; batch classifier loss: 0.390378; batch adversarial loss: 0.613043\n",
            "epoch 21; iter: 200; batch classifier loss: 0.397432; batch adversarial loss: 0.553012\n",
            "epoch 22; iter: 0; batch classifier loss: 0.409807; batch adversarial loss: 0.548907\n",
            "epoch 22; iter: 200; batch classifier loss: 0.470613; batch adversarial loss: 0.528572\n",
            "epoch 23; iter: 0; batch classifier loss: 0.349398; batch adversarial loss: 0.554245\n",
            "epoch 23; iter: 200; batch classifier loss: 0.444749; batch adversarial loss: 0.547616\n",
            "epoch 24; iter: 0; batch classifier loss: 0.475793; batch adversarial loss: 0.514633\n",
            "epoch 24; iter: 200; batch classifier loss: 0.398159; batch adversarial loss: 0.560075\n",
            "epoch 25; iter: 0; batch classifier loss: 0.476287; batch adversarial loss: 0.581231\n",
            "epoch 25; iter: 200; batch classifier loss: 0.410206; batch adversarial loss: 0.499593\n",
            "epoch 26; iter: 0; batch classifier loss: 0.406948; batch adversarial loss: 0.507533\n",
            "epoch 26; iter: 200; batch classifier loss: 0.364494; batch adversarial loss: 0.504187\n",
            "epoch 27; iter: 0; batch classifier loss: 0.389762; batch adversarial loss: 0.453124\n",
            "epoch 27; iter: 200; batch classifier loss: 0.332520; batch adversarial loss: 0.542903\n",
            "epoch 28; iter: 0; batch classifier loss: 0.339621; batch adversarial loss: 0.520360\n",
            "epoch 28; iter: 200; batch classifier loss: 0.369468; batch adversarial loss: 0.478022\n",
            "epoch 29; iter: 0; batch classifier loss: 0.441151; batch adversarial loss: 0.461113\n",
            "epoch 29; iter: 200; batch classifier loss: 0.391522; batch adversarial loss: 0.487271\n",
            "epoch 30; iter: 0; batch classifier loss: 0.470284; batch adversarial loss: 0.485827\n",
            "epoch 30; iter: 200; batch classifier loss: 0.492348; batch adversarial loss: 0.531987\n",
            "epoch 31; iter: 0; batch classifier loss: 0.423021; batch adversarial loss: 0.467787\n",
            "epoch 31; iter: 200; batch classifier loss: 0.423242; batch adversarial loss: 0.418106\n",
            "epoch 32; iter: 0; batch classifier loss: 0.341159; batch adversarial loss: 0.519401\n",
            "epoch 32; iter: 200; batch classifier loss: 0.467777; batch adversarial loss: 0.452675\n",
            "epoch 33; iter: 0; batch classifier loss: 0.431836; batch adversarial loss: 0.461956\n",
            "epoch 33; iter: 200; batch classifier loss: 0.416996; batch adversarial loss: 0.492989\n",
            "epoch 34; iter: 0; batch classifier loss: 0.483365; batch adversarial loss: 0.437771\n",
            "epoch 34; iter: 200; batch classifier loss: 0.406325; batch adversarial loss: 0.484392\n",
            "epoch 35; iter: 0; batch classifier loss: 0.449502; batch adversarial loss: 0.508745\n",
            "epoch 35; iter: 200; batch classifier loss: 0.445363; batch adversarial loss: 0.436505\n",
            "epoch 36; iter: 0; batch classifier loss: 0.436998; batch adversarial loss: 0.529085\n",
            "epoch 36; iter: 200; batch classifier loss: 0.437519; batch adversarial loss: 0.649622\n",
            "epoch 37; iter: 0; batch classifier loss: 0.402134; batch adversarial loss: 0.539579\n",
            "epoch 37; iter: 200; batch classifier loss: 0.360379; batch adversarial loss: 0.511367\n",
            "epoch 38; iter: 0; batch classifier loss: 0.458722; batch adversarial loss: 0.512736\n",
            "epoch 38; iter: 200; batch classifier loss: 0.397424; batch adversarial loss: 0.506315\n",
            "epoch 39; iter: 0; batch classifier loss: 0.403771; batch adversarial loss: 0.470574\n",
            "epoch 39; iter: 200; batch classifier loss: 0.408431; batch adversarial loss: 0.488403\n",
            "epoch 40; iter: 0; batch classifier loss: 0.442783; batch adversarial loss: 0.574857\n",
            "epoch 40; iter: 200; batch classifier loss: 0.418282; batch adversarial loss: 0.550346\n",
            "epoch 41; iter: 0; batch classifier loss: 0.398517; batch adversarial loss: 0.542499\n",
            "epoch 41; iter: 200; batch classifier loss: 0.448579; batch adversarial loss: 0.551234\n",
            "epoch 42; iter: 0; batch classifier loss: 0.412566; batch adversarial loss: 0.576301\n",
            "epoch 42; iter: 200; batch classifier loss: 0.422191; batch adversarial loss: 0.501319\n",
            "epoch 43; iter: 0; batch classifier loss: 0.532791; batch adversarial loss: 0.535565\n",
            "epoch 43; iter: 200; batch classifier loss: 0.406070; batch adversarial loss: 0.539748\n",
            "epoch 44; iter: 0; batch classifier loss: 0.422184; batch adversarial loss: 0.637524\n",
            "epoch 44; iter: 200; batch classifier loss: 0.389153; batch adversarial loss: 0.560262\n",
            "epoch 45; iter: 0; batch classifier loss: 0.479927; batch adversarial loss: 0.591200\n",
            "epoch 45; iter: 200; batch classifier loss: 0.410566; batch adversarial loss: 0.560656\n",
            "epoch 46; iter: 0; batch classifier loss: 0.368306; batch adversarial loss: 0.666781\n",
            "epoch 46; iter: 200; batch classifier loss: 0.507698; batch adversarial loss: 0.604596\n",
            "epoch 47; iter: 0; batch classifier loss: 0.330657; batch adversarial loss: 0.622145\n",
            "epoch 47; iter: 200; batch classifier loss: 0.490697; batch adversarial loss: 0.630370\n",
            "epoch 48; iter: 0; batch classifier loss: 0.445848; batch adversarial loss: 0.553689\n",
            "epoch 48; iter: 200; batch classifier loss: 0.484635; batch adversarial loss: 0.507108\n",
            "epoch 49; iter: 0; batch classifier loss: 0.355929; batch adversarial loss: 0.531399\n",
            "epoch 49; iter: 200; batch classifier loss: 0.389553; batch adversarial loss: 0.585029\n",
            "epoch 0; iter: 0; batch classifier loss: 0.652341; batch adversarial loss: 0.712285\n",
            "epoch 0; iter: 200; batch classifier loss: 0.693934; batch adversarial loss: 0.672002\n",
            "epoch 1; iter: 0; batch classifier loss: 0.477841; batch adversarial loss: 0.684614\n",
            "epoch 1; iter: 200; batch classifier loss: 0.429317; batch adversarial loss: 0.646457\n",
            "epoch 2; iter: 0; batch classifier loss: 0.480618; batch adversarial loss: 0.605339\n",
            "epoch 2; iter: 200; batch classifier loss: 0.424926; batch adversarial loss: 0.629972\n",
            "epoch 3; iter: 0; batch classifier loss: 0.449023; batch adversarial loss: 0.602179\n",
            "epoch 3; iter: 200; batch classifier loss: 0.411981; batch adversarial loss: 0.642871\n",
            "epoch 4; iter: 0; batch classifier loss: 0.473354; batch adversarial loss: 0.598063\n",
            "epoch 4; iter: 200; batch classifier loss: 0.467672; batch adversarial loss: 0.576542\n",
            "epoch 5; iter: 0; batch classifier loss: 0.568705; batch adversarial loss: 0.606012\n",
            "epoch 5; iter: 200; batch classifier loss: 0.370933; batch adversarial loss: 0.586217\n",
            "epoch 6; iter: 0; batch classifier loss: 0.525907; batch adversarial loss: 0.625086\n",
            "epoch 6; iter: 200; batch classifier loss: 0.463726; batch adversarial loss: 0.660268\n",
            "epoch 7; iter: 0; batch classifier loss: 0.506521; batch adversarial loss: 0.613595\n",
            "epoch 7; iter: 200; batch classifier loss: 0.420798; batch adversarial loss: 0.606872\n",
            "epoch 8; iter: 0; batch classifier loss: 0.468096; batch adversarial loss: 0.605479\n",
            "epoch 8; iter: 200; batch classifier loss: 0.489357; batch adversarial loss: 0.664150\n",
            "epoch 9; iter: 0; batch classifier loss: 0.498570; batch adversarial loss: 0.636213\n",
            "epoch 9; iter: 200; batch classifier loss: 0.459015; batch adversarial loss: 0.624880\n",
            "epoch 10; iter: 0; batch classifier loss: 0.553476; batch adversarial loss: 0.625489\n",
            "epoch 10; iter: 200; batch classifier loss: 0.433809; batch adversarial loss: 0.628093\n",
            "epoch 11; iter: 0; batch classifier loss: 0.539906; batch adversarial loss: 0.598807\n",
            "epoch 11; iter: 200; batch classifier loss: 0.438605; batch adversarial loss: 0.694823\n",
            "epoch 12; iter: 0; batch classifier loss: 0.472878; batch adversarial loss: 0.623580\n",
            "epoch 12; iter: 200; batch classifier loss: 0.476748; batch adversarial loss: 0.620355\n",
            "epoch 13; iter: 0; batch classifier loss: 0.365373; batch adversarial loss: 0.548156\n",
            "epoch 13; iter: 200; batch classifier loss: 0.452542; batch adversarial loss: 0.628734\n",
            "epoch 14; iter: 0; batch classifier loss: 0.406642; batch adversarial loss: 0.628605\n",
            "epoch 14; iter: 200; batch classifier loss: 0.481003; batch adversarial loss: 0.625894\n",
            "epoch 15; iter: 0; batch classifier loss: 0.467431; batch adversarial loss: 0.616088\n",
            "epoch 15; iter: 200; batch classifier loss: 0.517832; batch adversarial loss: 0.654083\n",
            "epoch 16; iter: 0; batch classifier loss: 0.380866; batch adversarial loss: 0.558296\n",
            "epoch 16; iter: 200; batch classifier loss: 0.471413; batch adversarial loss: 0.666021\n",
            "epoch 17; iter: 0; batch classifier loss: 0.495834; batch adversarial loss: 0.599487\n",
            "epoch 17; iter: 200; batch classifier loss: 0.504125; batch adversarial loss: 0.595883\n",
            "epoch 18; iter: 0; batch classifier loss: 0.468612; batch adversarial loss: 0.595969\n",
            "epoch 18; iter: 200; batch classifier loss: 0.545435; batch adversarial loss: 0.605504\n",
            "epoch 19; iter: 0; batch classifier loss: 0.402667; batch adversarial loss: 0.647230\n",
            "epoch 19; iter: 200; batch classifier loss: 0.506206; batch adversarial loss: 0.641236\n",
            "epoch 20; iter: 0; batch classifier loss: 0.473554; batch adversarial loss: 0.620098\n",
            "epoch 20; iter: 200; batch classifier loss: 0.354727; batch adversarial loss: 0.712009\n",
            "epoch 21; iter: 0; batch classifier loss: 0.506348; batch adversarial loss: 0.594380\n",
            "epoch 21; iter: 200; batch classifier loss: 0.416905; batch adversarial loss: 0.622546\n",
            "epoch 22; iter: 0; batch classifier loss: 0.560189; batch adversarial loss: 0.583409\n",
            "epoch 22; iter: 200; batch classifier loss: 0.441418; batch adversarial loss: 0.616884\n",
            "epoch 23; iter: 0; batch classifier loss: 0.421989; batch adversarial loss: 0.666646\n",
            "epoch 23; iter: 200; batch classifier loss: 0.341563; batch adversarial loss: 0.636465\n",
            "epoch 24; iter: 0; batch classifier loss: 0.417129; batch adversarial loss: 0.621517\n",
            "epoch 24; iter: 200; batch classifier loss: 0.479378; batch adversarial loss: 0.579595\n",
            "epoch 25; iter: 0; batch classifier loss: 0.514649; batch adversarial loss: 0.629536\n",
            "epoch 25; iter: 200; batch classifier loss: 0.426550; batch adversarial loss: 0.611684\n",
            "epoch 26; iter: 0; batch classifier loss: 0.551577; batch adversarial loss: 0.633969\n",
            "epoch 26; iter: 200; batch classifier loss: 0.534989; batch adversarial loss: 0.580504\n",
            "epoch 27; iter: 0; batch classifier loss: 0.420818; batch adversarial loss: 0.639777\n",
            "epoch 27; iter: 200; batch classifier loss: 0.505843; batch adversarial loss: 0.591193\n",
            "epoch 28; iter: 0; batch classifier loss: 0.483254; batch adversarial loss: 0.593954\n",
            "epoch 28; iter: 200; batch classifier loss: 0.520792; batch adversarial loss: 0.599235\n",
            "epoch 29; iter: 0; batch classifier loss: 0.515452; batch adversarial loss: 0.605867\n",
            "epoch 29; iter: 200; batch classifier loss: 0.507895; batch adversarial loss: 0.625222\n",
            "epoch 30; iter: 0; batch classifier loss: 0.409426; batch adversarial loss: 0.564728\n",
            "epoch 30; iter: 200; batch classifier loss: 0.436456; batch adversarial loss: 0.675777\n",
            "epoch 31; iter: 0; batch classifier loss: 0.395497; batch adversarial loss: 0.681734\n",
            "epoch 31; iter: 200; batch classifier loss: 0.504802; batch adversarial loss: 0.587861\n",
            "epoch 32; iter: 0; batch classifier loss: 0.504236; batch adversarial loss: 0.570887\n",
            "epoch 32; iter: 200; batch classifier loss: 0.513001; batch adversarial loss: 0.655977\n",
            "epoch 33; iter: 0; batch classifier loss: 0.441007; batch adversarial loss: 0.651910\n",
            "epoch 33; iter: 200; batch classifier loss: 0.554219; batch adversarial loss: 0.643218\n",
            "epoch 34; iter: 0; batch classifier loss: 0.509382; batch adversarial loss: 0.685386\n",
            "epoch 34; iter: 200; batch classifier loss: 0.573804; batch adversarial loss: 0.627839\n",
            "epoch 35; iter: 0; batch classifier loss: 0.463463; batch adversarial loss: 0.614060\n",
            "epoch 35; iter: 200; batch classifier loss: 0.470775; batch adversarial loss: 0.652078\n",
            "epoch 36; iter: 0; batch classifier loss: 0.519244; batch adversarial loss: 0.653835\n",
            "epoch 36; iter: 200; batch classifier loss: 0.508376; batch adversarial loss: 0.652237\n",
            "epoch 37; iter: 0; batch classifier loss: 0.472985; batch adversarial loss: 0.586942\n",
            "epoch 37; iter: 200; batch classifier loss: 0.513421; batch adversarial loss: 0.613143\n",
            "epoch 38; iter: 0; batch classifier loss: 0.531517; batch adversarial loss: 0.511761\n",
            "epoch 38; iter: 200; batch classifier loss: 0.467745; batch adversarial loss: 0.617112\n",
            "epoch 39; iter: 0; batch classifier loss: 0.482053; batch adversarial loss: 0.633298\n",
            "epoch 39; iter: 200; batch classifier loss: 0.482944; batch adversarial loss: 0.604266\n",
            "epoch 40; iter: 0; batch classifier loss: 0.466801; batch adversarial loss: 0.664230\n",
            "epoch 40; iter: 200; batch classifier loss: 0.470089; batch adversarial loss: 0.644438\n",
            "epoch 41; iter: 0; batch classifier loss: 0.443151; batch adversarial loss: 0.617641\n",
            "epoch 41; iter: 200; batch classifier loss: 0.432952; batch adversarial loss: 0.640546\n",
            "epoch 42; iter: 0; batch classifier loss: 0.681762; batch adversarial loss: 0.569819\n",
            "epoch 42; iter: 200; batch classifier loss: 0.452501; batch adversarial loss: 0.664210\n",
            "epoch 43; iter: 0; batch classifier loss: 0.548864; batch adversarial loss: 0.630405\n",
            "epoch 43; iter: 200; batch classifier loss: 0.450172; batch adversarial loss: 0.619920\n",
            "epoch 44; iter: 0; batch classifier loss: 0.499532; batch adversarial loss: 0.609966\n",
            "epoch 44; iter: 200; batch classifier loss: 0.506961; batch adversarial loss: 0.572617\n",
            "epoch 45; iter: 0; batch classifier loss: 0.555107; batch adversarial loss: 0.626254\n",
            "epoch 45; iter: 200; batch classifier loss: 0.515883; batch adversarial loss: 0.602350\n",
            "epoch 46; iter: 0; batch classifier loss: 0.468579; batch adversarial loss: 0.568703\n",
            "epoch 46; iter: 200; batch classifier loss: 0.417051; batch adversarial loss: 0.627527\n",
            "epoch 47; iter: 0; batch classifier loss: 0.349112; batch adversarial loss: 0.644059\n",
            "epoch 47; iter: 200; batch classifier loss: 0.467562; batch adversarial loss: 0.599167\n",
            "epoch 48; iter: 0; batch classifier loss: 0.446975; batch adversarial loss: 0.658436\n",
            "epoch 48; iter: 200; batch classifier loss: 0.523195; batch adversarial loss: 0.614688\n",
            "epoch 49; iter: 0; batch classifier loss: 0.477418; batch adversarial loss: 0.615150\n",
            "epoch 49; iter: 200; batch classifier loss: 0.442651; batch adversarial loss: 0.610902\n",
            "epoch 0; iter: 0; batch classifier loss: 0.670517; batch adversarial loss: 0.749037\n",
            "epoch 0; iter: 200; batch classifier loss: 1.633378; batch adversarial loss: 0.723282\n",
            "epoch 1; iter: 0; batch classifier loss: 1.474115; batch adversarial loss: 0.707869\n",
            "epoch 1; iter: 200; batch classifier loss: 1.097529; batch adversarial loss: 0.667035\n",
            "epoch 2; iter: 0; batch classifier loss: 1.175607; batch adversarial loss: 0.684706\n",
            "epoch 2; iter: 200; batch classifier loss: 0.802195; batch adversarial loss: 0.657553\n",
            "epoch 3; iter: 0; batch classifier loss: 0.661167; batch adversarial loss: 0.632605\n",
            "epoch 3; iter: 200; batch classifier loss: 0.513645; batch adversarial loss: 0.616318\n",
            "epoch 4; iter: 0; batch classifier loss: 0.515665; batch adversarial loss: 0.670871\n",
            "epoch 4; iter: 200; batch classifier loss: 0.605348; batch adversarial loss: 0.588639\n",
            "epoch 5; iter: 0; batch classifier loss: 0.528446; batch adversarial loss: 0.667021\n",
            "epoch 5; iter: 200; batch classifier loss: 0.512798; batch adversarial loss: 0.685219\n",
            "epoch 6; iter: 0; batch classifier loss: 0.718377; batch adversarial loss: 0.580091\n",
            "epoch 6; iter: 200; batch classifier loss: 0.739672; batch adversarial loss: 0.605825\n",
            "epoch 7; iter: 0; batch classifier loss: 0.729517; batch adversarial loss: 0.605497\n",
            "epoch 7; iter: 200; batch classifier loss: 0.640923; batch adversarial loss: 0.616084\n",
            "epoch 8; iter: 0; batch classifier loss: 0.544536; batch adversarial loss: 0.610519\n",
            "epoch 8; iter: 200; batch classifier loss: 0.873683; batch adversarial loss: 0.643643\n",
            "epoch 9; iter: 0; batch classifier loss: 0.789351; batch adversarial loss: 0.588240\n",
            "epoch 9; iter: 200; batch classifier loss: 0.653199; batch adversarial loss: 0.660630\n",
            "epoch 10; iter: 0; batch classifier loss: 0.550395; batch adversarial loss: 0.593708\n",
            "epoch 10; iter: 200; batch classifier loss: 0.695910; batch adversarial loss: 0.576595\n",
            "epoch 11; iter: 0; batch classifier loss: 0.552296; batch adversarial loss: 0.666832\n",
            "epoch 11; iter: 200; batch classifier loss: 0.562413; batch adversarial loss: 0.600550\n",
            "epoch 12; iter: 0; batch classifier loss: 0.623118; batch adversarial loss: 0.600468\n",
            "epoch 12; iter: 200; batch classifier loss: 0.833513; batch adversarial loss: 0.594380\n",
            "epoch 13; iter: 0; batch classifier loss: 0.761136; batch adversarial loss: 0.648791\n",
            "epoch 13; iter: 200; batch classifier loss: 0.850450; batch adversarial loss: 0.595926\n",
            "epoch 14; iter: 0; batch classifier loss: 0.563307; batch adversarial loss: 0.594916\n",
            "epoch 14; iter: 200; batch classifier loss: 0.722587; batch adversarial loss: 0.582574\n",
            "epoch 15; iter: 0; batch classifier loss: 0.558691; batch adversarial loss: 0.600363\n",
            "epoch 15; iter: 200; batch classifier loss: 0.577722; batch adversarial loss: 0.637244\n",
            "epoch 16; iter: 0; batch classifier loss: 0.669475; batch adversarial loss: 0.689719\n",
            "epoch 16; iter: 200; batch classifier loss: 0.686267; batch adversarial loss: 0.651423\n",
            "epoch 17; iter: 0; batch classifier loss: 0.754744; batch adversarial loss: 0.593647\n",
            "epoch 17; iter: 200; batch classifier loss: 0.839966; batch adversarial loss: 0.694156\n",
            "epoch 18; iter: 0; batch classifier loss: 0.631176; batch adversarial loss: 0.648391\n",
            "epoch 18; iter: 200; batch classifier loss: 0.734374; batch adversarial loss: 0.600101\n",
            "epoch 19; iter: 0; batch classifier loss: 0.684327; batch adversarial loss: 0.583683\n",
            "epoch 19; iter: 200; batch classifier loss: 0.981803; batch adversarial loss: 0.627345\n",
            "epoch 20; iter: 0; batch classifier loss: 0.620917; batch adversarial loss: 0.686928\n",
            "epoch 20; iter: 200; batch classifier loss: 0.692853; batch adversarial loss: 0.674105\n",
            "epoch 21; iter: 0; batch classifier loss: 0.600134; batch adversarial loss: 0.666329\n",
            "epoch 21; iter: 200; batch classifier loss: 0.708509; batch adversarial loss: 0.637521\n",
            "epoch 22; iter: 0; batch classifier loss: 0.575186; batch adversarial loss: 0.650675\n",
            "epoch 22; iter: 200; batch classifier loss: 0.760423; batch adversarial loss: 0.655942\n",
            "epoch 23; iter: 0; batch classifier loss: 0.632818; batch adversarial loss: 0.632478\n",
            "epoch 23; iter: 200; batch classifier loss: 0.974632; batch adversarial loss: 0.597999\n",
            "epoch 24; iter: 0; batch classifier loss: 0.580126; batch adversarial loss: 0.649224\n",
            "epoch 24; iter: 200; batch classifier loss: 0.882688; batch adversarial loss: 0.672400\n",
            "epoch 25; iter: 0; batch classifier loss: 0.626268; batch adversarial loss: 0.639445\n",
            "epoch 25; iter: 200; batch classifier loss: 0.697008; batch adversarial loss: 0.623207\n",
            "epoch 26; iter: 0; batch classifier loss: 0.683563; batch adversarial loss: 0.621933\n",
            "epoch 26; iter: 200; batch classifier loss: 0.520407; batch adversarial loss: 0.637951\n",
            "epoch 27; iter: 0; batch classifier loss: 0.591106; batch adversarial loss: 0.593437\n",
            "epoch 27; iter: 200; batch classifier loss: 0.526391; batch adversarial loss: 0.673281\n",
            "epoch 28; iter: 0; batch classifier loss: 0.532961; batch adversarial loss: 0.632430\n",
            "epoch 28; iter: 200; batch classifier loss: 0.490922; batch adversarial loss: 0.606086\n",
            "epoch 29; iter: 0; batch classifier loss: 0.584181; batch adversarial loss: 0.648396\n",
            "epoch 29; iter: 200; batch classifier loss: 0.648529; batch adversarial loss: 0.620053\n",
            "epoch 30; iter: 0; batch classifier loss: 0.683723; batch adversarial loss: 0.620454\n",
            "epoch 30; iter: 200; batch classifier loss: 0.693613; batch adversarial loss: 0.677050\n",
            "epoch 31; iter: 0; batch classifier loss: 0.728164; batch adversarial loss: 0.614397\n",
            "epoch 31; iter: 200; batch classifier loss: 0.779871; batch adversarial loss: 0.680055\n",
            "epoch 32; iter: 0; batch classifier loss: 0.861309; batch adversarial loss: 0.644820\n",
            "epoch 32; iter: 200; batch classifier loss: 0.726405; batch adversarial loss: 0.614597\n",
            "epoch 33; iter: 0; batch classifier loss: 0.628045; batch adversarial loss: 0.677941\n",
            "epoch 33; iter: 200; batch classifier loss: 0.580361; batch adversarial loss: 0.579729\n",
            "epoch 34; iter: 0; batch classifier loss: 0.538440; batch adversarial loss: 0.614345\n",
            "epoch 34; iter: 200; batch classifier loss: 0.582381; batch adversarial loss: 0.622565\n",
            "epoch 35; iter: 0; batch classifier loss: 0.675858; batch adversarial loss: 0.681376\n",
            "epoch 35; iter: 200; batch classifier loss: 0.632245; batch adversarial loss: 0.617788\n",
            "epoch 36; iter: 0; batch classifier loss: 0.801777; batch adversarial loss: 0.606242\n",
            "epoch 36; iter: 200; batch classifier loss: 0.633649; batch adversarial loss: 0.597297\n",
            "epoch 37; iter: 0; batch classifier loss: 0.691434; batch adversarial loss: 0.649948\n",
            "epoch 37; iter: 200; batch classifier loss: 0.531702; batch adversarial loss: 0.647105\n",
            "epoch 38; iter: 0; batch classifier loss: 0.589593; batch adversarial loss: 0.602387\n",
            "epoch 38; iter: 200; batch classifier loss: 0.727115; batch adversarial loss: 0.660134\n",
            "epoch 39; iter: 0; batch classifier loss: 0.664233; batch adversarial loss: 0.599424\n",
            "epoch 39; iter: 200; batch classifier loss: 0.812129; batch adversarial loss: 0.627638\n",
            "epoch 40; iter: 0; batch classifier loss: 0.653328; batch adversarial loss: 0.604177\n",
            "epoch 40; iter: 200; batch classifier loss: 0.648269; batch adversarial loss: 0.669501\n",
            "epoch 41; iter: 0; batch classifier loss: 0.833089; batch adversarial loss: 0.619305\n",
            "epoch 41; iter: 200; batch classifier loss: 0.605476; batch adversarial loss: 0.639451\n",
            "epoch 42; iter: 0; batch classifier loss: 0.570548; batch adversarial loss: 0.546573\n",
            "epoch 42; iter: 200; batch classifier loss: 0.670367; batch adversarial loss: 0.627692\n",
            "epoch 43; iter: 0; batch classifier loss: 0.571095; batch adversarial loss: 0.611167\n",
            "epoch 43; iter: 200; batch classifier loss: 0.919776; batch adversarial loss: 0.695443\n",
            "epoch 44; iter: 0; batch classifier loss: 0.558864; batch adversarial loss: 0.638866\n",
            "epoch 44; iter: 200; batch classifier loss: 0.659814; batch adversarial loss: 0.656678\n",
            "epoch 45; iter: 0; batch classifier loss: 0.625223; batch adversarial loss: 0.666208\n",
            "epoch 45; iter: 200; batch classifier loss: 0.629195; batch adversarial loss: 0.587397\n",
            "epoch 46; iter: 0; batch classifier loss: 0.549567; batch adversarial loss: 0.632152\n",
            "epoch 46; iter: 200; batch classifier loss: 0.560305; batch adversarial loss: 0.611413\n",
            "epoch 47; iter: 0; batch classifier loss: 0.562172; batch adversarial loss: 0.628466\n",
            "epoch 47; iter: 200; batch classifier loss: 0.655754; batch adversarial loss: 0.634436\n",
            "epoch 48; iter: 0; batch classifier loss: 0.664604; batch adversarial loss: 0.582694\n",
            "epoch 48; iter: 200; batch classifier loss: 0.552096; batch adversarial loss: 0.701589\n",
            "epoch 49; iter: 0; batch classifier loss: 0.535076; batch adversarial loss: 0.609619\n",
            "epoch 49; iter: 200; batch classifier loss: 0.618630; batch adversarial loss: 0.601435\n",
            "epoch 0; iter: 0; batch classifier loss: 0.708113; batch adversarial loss: 0.572368\n",
            "epoch 0; iter: 200; batch classifier loss: 1.268338; batch adversarial loss: 0.806532\n",
            "epoch 1; iter: 0; batch classifier loss: 1.508593; batch adversarial loss: 0.818223\n",
            "epoch 1; iter: 200; batch classifier loss: 1.715003; batch adversarial loss: 0.731868\n",
            "epoch 2; iter: 0; batch classifier loss: 1.619839; batch adversarial loss: 0.704151\n",
            "epoch 2; iter: 200; batch classifier loss: 1.844282; batch adversarial loss: 0.602836\n",
            "epoch 3; iter: 0; batch classifier loss: 1.678634; batch adversarial loss: 0.583428\n",
            "epoch 3; iter: 200; batch classifier loss: 2.454147; batch adversarial loss: 0.658602\n",
            "epoch 4; iter: 0; batch classifier loss: 1.994526; batch adversarial loss: 0.617907\n",
            "epoch 4; iter: 200; batch classifier loss: 1.889808; batch adversarial loss: 0.670449\n",
            "epoch 5; iter: 0; batch classifier loss: 2.160203; batch adversarial loss: 0.665185\n",
            "epoch 5; iter: 200; batch classifier loss: 2.402626; batch adversarial loss: 0.572180\n",
            "epoch 6; iter: 0; batch classifier loss: 2.099712; batch adversarial loss: 0.583334\n",
            "epoch 6; iter: 200; batch classifier loss: 2.305462; batch adversarial loss: 0.571858\n",
            "epoch 7; iter: 0; batch classifier loss: 2.038088; batch adversarial loss: 0.643933\n",
            "epoch 7; iter: 200; batch classifier loss: 3.116174; batch adversarial loss: 0.660816\n",
            "epoch 8; iter: 0; batch classifier loss: 2.435204; batch adversarial loss: 0.610564\n",
            "epoch 8; iter: 200; batch classifier loss: 2.360905; batch adversarial loss: 0.627299\n",
            "epoch 9; iter: 0; batch classifier loss: 2.280662; batch adversarial loss: 0.660718\n",
            "epoch 9; iter: 200; batch classifier loss: 2.604893; batch adversarial loss: 0.638402\n",
            "epoch 10; iter: 0; batch classifier loss: 2.470279; batch adversarial loss: 0.604981\n",
            "epoch 10; iter: 200; batch classifier loss: 2.625180; batch adversarial loss: 0.616027\n",
            "epoch 11; iter: 0; batch classifier loss: 2.679961; batch adversarial loss: 0.621713\n",
            "epoch 11; iter: 200; batch classifier loss: 3.249960; batch adversarial loss: 0.632845\n",
            "epoch 12; iter: 0; batch classifier loss: 2.180273; batch adversarial loss: 0.666188\n",
            "epoch 12; iter: 200; batch classifier loss: 1.820996; batch adversarial loss: 0.638437\n",
            "epoch 13; iter: 0; batch classifier loss: 2.938543; batch adversarial loss: 0.655049\n",
            "epoch 13; iter: 200; batch classifier loss: 2.814148; batch adversarial loss: 0.610624\n",
            "epoch 14; iter: 0; batch classifier loss: 2.580728; batch adversarial loss: 0.655121\n",
            "epoch 14; iter: 200; batch classifier loss: 2.048162; batch adversarial loss: 0.616240\n",
            "epoch 15; iter: 0; batch classifier loss: 3.040223; batch adversarial loss: 0.616077\n",
            "epoch 15; iter: 200; batch classifier loss: 2.017470; batch adversarial loss: 0.610326\n",
            "epoch 16; iter: 0; batch classifier loss: 2.026607; batch adversarial loss: 0.616010\n",
            "epoch 16; iter: 200; batch classifier loss: 1.807978; batch adversarial loss: 0.644071\n",
            "epoch 17; iter: 0; batch classifier loss: 2.052084; batch adversarial loss: 0.677819\n",
            "epoch 17; iter: 200; batch classifier loss: 1.853101; batch adversarial loss: 0.672350\n",
            "epoch 18; iter: 0; batch classifier loss: 2.403960; batch adversarial loss: 0.632842\n",
            "epoch 18; iter: 200; batch classifier loss: 1.930948; batch adversarial loss: 0.621656\n",
            "epoch 19; iter: 0; batch classifier loss: 2.148964; batch adversarial loss: 0.666261\n",
            "epoch 19; iter: 200; batch classifier loss: 2.044690; batch adversarial loss: 0.616143\n",
            "epoch 20; iter: 0; batch classifier loss: 2.272011; batch adversarial loss: 0.627304\n",
            "epoch 20; iter: 200; batch classifier loss: 1.961815; batch adversarial loss: 0.604904\n",
            "epoch 21; iter: 0; batch classifier loss: 2.517115; batch adversarial loss: 0.588523\n",
            "epoch 21; iter: 200; batch classifier loss: 1.561524; batch adversarial loss: 0.666159\n",
            "epoch 22; iter: 0; batch classifier loss: 1.915907; batch adversarial loss: 0.660682\n",
            "epoch 22; iter: 200; batch classifier loss: 1.436521; batch adversarial loss: 0.616055\n",
            "epoch 23; iter: 0; batch classifier loss: 1.212475; batch adversarial loss: 0.677396\n",
            "epoch 23; iter: 200; batch classifier loss: 1.261407; batch adversarial loss: 0.660534\n",
            "epoch 24; iter: 0; batch classifier loss: 1.932658; batch adversarial loss: 0.644004\n",
            "epoch 24; iter: 200; batch classifier loss: 1.414703; batch adversarial loss: 0.632827\n",
            "epoch 25; iter: 0; batch classifier loss: 1.342755; batch adversarial loss: 0.610414\n",
            "epoch 25; iter: 200; batch classifier loss: 0.826469; batch adversarial loss: 0.666701\n",
            "epoch 26; iter: 0; batch classifier loss: 0.413503; batch adversarial loss: 0.691651\n",
            "epoch 26; iter: 200; batch classifier loss: 1.822804; batch adversarial loss: 0.587899\n",
            "epoch 27; iter: 0; batch classifier loss: 1.838696; batch adversarial loss: 0.610382\n",
            "epoch 27; iter: 200; batch classifier loss: 1.898310; batch adversarial loss: 0.632834\n",
            "epoch 28; iter: 0; batch classifier loss: 2.134404; batch adversarial loss: 0.666250\n",
            "epoch 28; iter: 200; batch classifier loss: 1.679854; batch adversarial loss: 0.621648\n",
            "epoch 29; iter: 0; batch classifier loss: 2.221958; batch adversarial loss: 0.671939\n",
            "epoch 29; iter: 200; batch classifier loss: 2.107152; batch adversarial loss: 0.666451\n",
            "epoch 30; iter: 0; batch classifier loss: 2.050568; batch adversarial loss: 0.582589\n",
            "epoch 30; iter: 200; batch classifier loss: 2.241743; batch adversarial loss: 0.616096\n",
            "epoch 31; iter: 0; batch classifier loss: 2.002031; batch adversarial loss: 0.632838\n",
            "epoch 31; iter: 200; batch classifier loss: 1.895488; batch adversarial loss: 0.616135\n",
            "epoch 32; iter: 0; batch classifier loss: 2.222380; batch adversarial loss: 0.610570\n",
            "epoch 32; iter: 200; batch classifier loss: 1.859388; batch adversarial loss: 0.610630\n",
            "epoch 33; iter: 0; batch classifier loss: 1.841296; batch adversarial loss: 0.621692\n",
            "epoch 33; iter: 200; batch classifier loss: 2.479045; batch adversarial loss: 0.599292\n",
            "epoch 34; iter: 0; batch classifier loss: 1.461417; batch adversarial loss: 0.610574\n",
            "epoch 34; iter: 200; batch classifier loss: 1.323607; batch adversarial loss: 0.605222\n",
            "epoch 35; iter: 0; batch classifier loss: 1.957607; batch adversarial loss: 0.632844\n",
            "epoch 35; iter: 200; batch classifier loss: 1.874110; batch adversarial loss: 0.610278\n",
            "epoch 36; iter: 0; batch classifier loss: 1.096612; batch adversarial loss: 0.666260\n",
            "epoch 36; iter: 200; batch classifier loss: 1.452384; batch adversarial loss: 0.638439\n",
            "epoch 37; iter: 0; batch classifier loss: 1.673534; batch adversarial loss: 0.649556\n",
            "epoch 37; iter: 200; batch classifier loss: 1.688057; batch adversarial loss: 0.627251\n",
            "epoch 38; iter: 0; batch classifier loss: 1.751609; batch adversarial loss: 0.610516\n",
            "epoch 38; iter: 200; batch classifier loss: 1.227396; batch adversarial loss: 0.655015\n",
            "epoch 39; iter: 0; batch classifier loss: 0.981940; batch adversarial loss: 0.649723\n",
            "epoch 39; iter: 200; batch classifier loss: 2.905525; batch adversarial loss: 0.644028\n",
            "epoch 40; iter: 0; batch classifier loss: 1.801843; batch adversarial loss: 0.655116\n",
            "epoch 40; iter: 200; batch classifier loss: 2.387280; batch adversarial loss: 0.666357\n",
            "epoch 41; iter: 0; batch classifier loss: 2.898881; batch adversarial loss: 0.599430\n",
            "epoch 41; iter: 200; batch classifier loss: 3.378725; batch adversarial loss: 0.610409\n",
            "epoch 42; iter: 0; batch classifier loss: 2.352871; batch adversarial loss: 0.643955\n",
            "epoch 42; iter: 200; batch classifier loss: 2.499001; batch adversarial loss: 0.722799\n",
            "epoch 43; iter: 0; batch classifier loss: 2.733413; batch adversarial loss: 0.655113\n",
            "epoch 43; iter: 200; batch classifier loss: 2.355271; batch adversarial loss: 0.655230\n",
            "epoch 44; iter: 0; batch classifier loss: 2.150764; batch adversarial loss: 0.621651\n",
            "epoch 44; iter: 200; batch classifier loss: 3.217968; batch adversarial loss: 0.615930\n",
            "epoch 45; iter: 0; batch classifier loss: 2.777790; batch adversarial loss: 0.627256\n",
            "epoch 45; iter: 200; batch classifier loss: 1.688916; batch adversarial loss: 0.605069\n",
            "epoch 46; iter: 0; batch classifier loss: 2.433981; batch adversarial loss: 0.588227\n",
            "epoch 46; iter: 200; batch classifier loss: 1.450567; batch adversarial loss: 0.694197\n",
            "epoch 47; iter: 0; batch classifier loss: 1.558102; batch adversarial loss: 0.649622\n",
            "epoch 47; iter: 200; batch classifier loss: 1.507974; batch adversarial loss: 0.616052\n",
            "epoch 48; iter: 0; batch classifier loss: 1.635416; batch adversarial loss: 0.655293\n",
            "epoch 48; iter: 200; batch classifier loss: 1.445833; batch adversarial loss: 0.632849\n",
            "epoch 49; iter: 0; batch classifier loss: 1.215950; batch adversarial loss: 0.671867\n",
            "epoch 49; iter: 200; batch classifier loss: 1.351085; batch adversarial loss: 0.649510\n",
            "\n",
            " Welcome to Fold number 2 \n",
            "\n",
            "epoch 0; iter: 0; batch classifier loss: 0.677510; batch adversarial loss: 0.645985\n",
            "epoch 0; iter: 200; batch classifier loss: 0.419928; batch adversarial loss: 0.616161\n",
            "epoch 1; iter: 0; batch classifier loss: 0.496850; batch adversarial loss: 0.604542\n",
            "epoch 1; iter: 200; batch classifier loss: 0.453362; batch adversarial loss: 0.663492\n",
            "epoch 2; iter: 0; batch classifier loss: 0.398110; batch adversarial loss: 0.587403\n",
            "epoch 2; iter: 200; batch classifier loss: 0.466080; batch adversarial loss: 0.596105\n",
            "epoch 3; iter: 0; batch classifier loss: 0.433425; batch adversarial loss: 0.575873\n",
            "epoch 3; iter: 200; batch classifier loss: 0.369267; batch adversarial loss: 0.680120\n",
            "epoch 4; iter: 0; batch classifier loss: 0.413880; batch adversarial loss: 0.538414\n",
            "epoch 4; iter: 200; batch classifier loss: 0.395287; batch adversarial loss: 0.588215\n",
            "epoch 5; iter: 0; batch classifier loss: 0.489883; batch adversarial loss: 0.502945\n",
            "epoch 5; iter: 200; batch classifier loss: 0.496084; batch adversarial loss: 0.571731\n",
            "epoch 6; iter: 0; batch classifier loss: 0.533825; batch adversarial loss: 0.538901\n",
            "epoch 6; iter: 200; batch classifier loss: 0.437285; batch adversarial loss: 0.569212\n",
            "epoch 7; iter: 0; batch classifier loss: 0.406463; batch adversarial loss: 0.537038\n",
            "epoch 7; iter: 200; batch classifier loss: 0.363838; batch adversarial loss: 0.524281\n",
            "epoch 8; iter: 0; batch classifier loss: 0.401001; batch adversarial loss: 0.543084\n",
            "epoch 8; iter: 200; batch classifier loss: 0.509180; batch adversarial loss: 0.530399\n",
            "epoch 9; iter: 0; batch classifier loss: 0.430174; batch adversarial loss: 0.573801\n",
            "epoch 9; iter: 200; batch classifier loss: 0.435158; batch adversarial loss: 0.521442\n",
            "epoch 10; iter: 0; batch classifier loss: 0.420571; batch adversarial loss: 0.507157\n",
            "epoch 10; iter: 200; batch classifier loss: 0.433630; batch adversarial loss: 0.482033\n",
            "epoch 11; iter: 0; batch classifier loss: 0.302811; batch adversarial loss: 0.533702\n",
            "epoch 11; iter: 200; batch classifier loss: 0.516154; batch adversarial loss: 0.510918\n",
            "epoch 12; iter: 0; batch classifier loss: 0.413636; batch adversarial loss: 0.511999\n",
            "epoch 12; iter: 200; batch classifier loss: 0.411587; batch adversarial loss: 0.503161\n",
            "epoch 13; iter: 0; batch classifier loss: 0.402558; batch adversarial loss: 0.545746\n",
            "epoch 13; iter: 200; batch classifier loss: 0.400153; batch adversarial loss: 0.512659\n",
            "epoch 14; iter: 0; batch classifier loss: 0.502798; batch adversarial loss: 0.502605\n",
            "epoch 14; iter: 200; batch classifier loss: 0.414839; batch adversarial loss: 0.485152\n",
            "epoch 15; iter: 0; batch classifier loss: 0.426562; batch adversarial loss: 0.509432\n",
            "epoch 15; iter: 200; batch classifier loss: 0.423393; batch adversarial loss: 0.569731\n",
            "epoch 16; iter: 0; batch classifier loss: 0.363622; batch adversarial loss: 0.537089\n",
            "epoch 16; iter: 200; batch classifier loss: 0.387275; batch adversarial loss: 0.553358\n",
            "epoch 17; iter: 0; batch classifier loss: 0.382621; batch adversarial loss: 0.474632\n",
            "epoch 17; iter: 200; batch classifier loss: 0.404977; batch adversarial loss: 0.514312\n",
            "epoch 18; iter: 0; batch classifier loss: 0.407297; batch adversarial loss: 0.435449\n",
            "epoch 18; iter: 200; batch classifier loss: 0.338743; batch adversarial loss: 0.532516\n",
            "epoch 19; iter: 0; batch classifier loss: 0.385404; batch adversarial loss: 0.481630\n",
            "epoch 19; iter: 200; batch classifier loss: 0.506598; batch adversarial loss: 0.453459\n",
            "epoch 20; iter: 0; batch classifier loss: 0.442295; batch adversarial loss: 0.472790\n",
            "epoch 20; iter: 200; batch classifier loss: 0.444500; batch adversarial loss: 0.507085\n",
            "epoch 21; iter: 0; batch classifier loss: 0.469050; batch adversarial loss: 0.570642\n",
            "epoch 21; iter: 200; batch classifier loss: 0.348729; batch adversarial loss: 0.558219\n",
            "epoch 22; iter: 0; batch classifier loss: 0.378667; batch adversarial loss: 0.547388\n",
            "epoch 22; iter: 200; batch classifier loss: 0.406686; batch adversarial loss: 0.589105\n",
            "epoch 23; iter: 0; batch classifier loss: 0.399169; batch adversarial loss: 0.564188\n",
            "epoch 23; iter: 200; batch classifier loss: 0.464787; batch adversarial loss: 0.560969\n",
            "epoch 24; iter: 0; batch classifier loss: 0.462294; batch adversarial loss: 0.526382\n",
            "epoch 24; iter: 200; batch classifier loss: 0.403031; batch adversarial loss: 0.604006\n",
            "epoch 25; iter: 0; batch classifier loss: 0.413904; batch adversarial loss: 0.544531\n",
            "epoch 25; iter: 200; batch classifier loss: 0.397691; batch adversarial loss: 0.517032\n",
            "epoch 26; iter: 0; batch classifier loss: 0.298648; batch adversarial loss: 0.539668\n",
            "epoch 26; iter: 200; batch classifier loss: 0.406311; batch adversarial loss: 0.543768\n",
            "epoch 27; iter: 0; batch classifier loss: 0.471809; batch adversarial loss: 0.603652\n",
            "epoch 27; iter: 200; batch classifier loss: 0.510928; batch adversarial loss: 0.540106\n",
            "epoch 28; iter: 0; batch classifier loss: 0.524542; batch adversarial loss: 0.484783\n",
            "epoch 28; iter: 200; batch classifier loss: 0.415142; batch adversarial loss: 0.476051\n",
            "epoch 29; iter: 0; batch classifier loss: 0.425315; batch adversarial loss: 0.471039\n",
            "epoch 29; iter: 200; batch classifier loss: 0.360898; batch adversarial loss: 0.493389\n",
            "epoch 30; iter: 0; batch classifier loss: 0.402948; batch adversarial loss: 0.472159\n",
            "epoch 30; iter: 200; batch classifier loss: 0.480409; batch adversarial loss: 0.502270\n",
            "epoch 31; iter: 0; batch classifier loss: 0.404252; batch adversarial loss: 0.473914\n",
            "epoch 31; iter: 200; batch classifier loss: 0.333126; batch adversarial loss: 0.541870\n",
            "epoch 32; iter: 0; batch classifier loss: 0.336990; batch adversarial loss: 0.517218\n",
            "epoch 32; iter: 200; batch classifier loss: 0.477830; batch adversarial loss: 0.486243\n",
            "epoch 33; iter: 0; batch classifier loss: 0.361635; batch adversarial loss: 0.510895\n",
            "epoch 33; iter: 200; batch classifier loss: 0.403383; batch adversarial loss: 0.428982\n",
            "epoch 34; iter: 0; batch classifier loss: 0.397767; batch adversarial loss: 0.531384\n",
            "epoch 34; iter: 200; batch classifier loss: 0.384770; batch adversarial loss: 0.529918\n",
            "epoch 35; iter: 0; batch classifier loss: 0.343318; batch adversarial loss: 0.572929\n",
            "epoch 35; iter: 200; batch classifier loss: 0.441540; batch adversarial loss: 0.552438\n",
            "epoch 36; iter: 0; batch classifier loss: 0.486722; batch adversarial loss: 0.490769\n",
            "epoch 36; iter: 200; batch classifier loss: 0.381957; batch adversarial loss: 0.569624\n",
            "epoch 37; iter: 0; batch classifier loss: 0.477824; batch adversarial loss: 0.498537\n",
            "epoch 37; iter: 200; batch classifier loss: 0.422042; batch adversarial loss: 0.531917\n",
            "epoch 38; iter: 0; batch classifier loss: 0.429133; batch adversarial loss: 0.558168\n",
            "epoch 38; iter: 200; batch classifier loss: 0.400758; batch adversarial loss: 0.504006\n",
            "epoch 39; iter: 0; batch classifier loss: 0.423016; batch adversarial loss: 0.488158\n",
            "epoch 39; iter: 200; batch classifier loss: 0.416352; batch adversarial loss: 0.536896\n",
            "epoch 40; iter: 0; batch classifier loss: 0.425680; batch adversarial loss: 0.466221\n",
            "epoch 40; iter: 200; batch classifier loss: 0.417109; batch adversarial loss: 0.523788\n",
            "epoch 41; iter: 0; batch classifier loss: 0.456128; batch adversarial loss: 0.466142\n",
            "epoch 41; iter: 200; batch classifier loss: 0.410781; batch adversarial loss: 0.486966\n",
            "epoch 42; iter: 0; batch classifier loss: 0.483490; batch adversarial loss: 0.506956\n",
            "epoch 42; iter: 200; batch classifier loss: 0.367054; batch adversarial loss: 0.474479\n",
            "epoch 43; iter: 0; batch classifier loss: 0.508123; batch adversarial loss: 0.504544\n",
            "epoch 43; iter: 200; batch classifier loss: 0.376619; batch adversarial loss: 0.506077\n",
            "epoch 44; iter: 0; batch classifier loss: 0.417365; batch adversarial loss: 0.462953\n",
            "epoch 44; iter: 200; batch classifier loss: 0.411474; batch adversarial loss: 0.517525\n",
            "epoch 45; iter: 0; batch classifier loss: 0.499073; batch adversarial loss: 0.556337\n",
            "epoch 45; iter: 200; batch classifier loss: 0.429273; batch adversarial loss: 0.509105\n",
            "epoch 46; iter: 0; batch classifier loss: 0.441202; batch adversarial loss: 0.550084\n",
            "epoch 46; iter: 200; batch classifier loss: 0.356723; batch adversarial loss: 0.480895\n",
            "epoch 47; iter: 0; batch classifier loss: 0.367102; batch adversarial loss: 0.479272\n",
            "epoch 47; iter: 200; batch classifier loss: 0.350023; batch adversarial loss: 0.496661\n",
            "epoch 48; iter: 0; batch classifier loss: 0.399456; batch adversarial loss: 0.579016\n",
            "epoch 48; iter: 200; batch classifier loss: 0.346568; batch adversarial loss: 0.523956\n",
            "epoch 49; iter: 0; batch classifier loss: 0.403134; batch adversarial loss: 0.580246\n",
            "epoch 49; iter: 200; batch classifier loss: 0.395998; batch adversarial loss: 0.455328\n",
            "epoch 0; iter: 0; batch classifier loss: 0.692873; batch adversarial loss: 0.671669\n",
            "epoch 0; iter: 200; batch classifier loss: 0.467132; batch adversarial loss: 0.588633\n",
            "epoch 1; iter: 0; batch classifier loss: 0.526351; batch adversarial loss: 0.620767\n",
            "epoch 1; iter: 200; batch classifier loss: 0.421953; batch adversarial loss: 0.637987\n",
            "epoch 2; iter: 0; batch classifier loss: 0.370491; batch adversarial loss: 0.612814\n",
            "epoch 2; iter: 200; batch classifier loss: 0.479467; batch adversarial loss: 0.610498\n",
            "epoch 3; iter: 0; batch classifier loss: 0.428425; batch adversarial loss: 0.640037\n",
            "epoch 3; iter: 200; batch classifier loss: 0.438891; batch adversarial loss: 0.570249\n",
            "epoch 4; iter: 0; batch classifier loss: 0.407477; batch adversarial loss: 0.577552\n",
            "epoch 4; iter: 200; batch classifier loss: 0.491887; batch adversarial loss: 0.548449\n",
            "epoch 5; iter: 0; batch classifier loss: 0.487555; batch adversarial loss: 0.531250\n",
            "epoch 5; iter: 200; batch classifier loss: 0.425690; batch adversarial loss: 0.555651\n",
            "epoch 6; iter: 0; batch classifier loss: 0.476933; batch adversarial loss: 0.495396\n",
            "epoch 6; iter: 200; batch classifier loss: 0.447414; batch adversarial loss: 0.595288\n",
            "epoch 7; iter: 0; batch classifier loss: 0.311655; batch adversarial loss: 0.613768\n",
            "epoch 7; iter: 200; batch classifier loss: 0.335524; batch adversarial loss: 0.561795\n",
            "epoch 8; iter: 0; batch classifier loss: 0.407873; batch adversarial loss: 0.573915\n",
            "epoch 8; iter: 200; batch classifier loss: 0.470794; batch adversarial loss: 0.471257\n",
            "epoch 9; iter: 0; batch classifier loss: 0.468695; batch adversarial loss: 0.568537\n",
            "epoch 9; iter: 200; batch classifier loss: 0.480605; batch adversarial loss: 0.528500\n",
            "epoch 10; iter: 0; batch classifier loss: 0.480928; batch adversarial loss: 0.544120\n",
            "epoch 10; iter: 200; batch classifier loss: 0.464076; batch adversarial loss: 0.477664\n",
            "epoch 11; iter: 0; batch classifier loss: 0.369252; batch adversarial loss: 0.490008\n",
            "epoch 11; iter: 200; batch classifier loss: 0.431083; batch adversarial loss: 0.541335\n",
            "epoch 12; iter: 0; batch classifier loss: 0.478206; batch adversarial loss: 0.524391\n",
            "epoch 12; iter: 200; batch classifier loss: 0.414543; batch adversarial loss: 0.561481\n",
            "epoch 13; iter: 0; batch classifier loss: 0.377813; batch adversarial loss: 0.480555\n",
            "epoch 13; iter: 200; batch classifier loss: 0.410470; batch adversarial loss: 0.497951\n",
            "epoch 14; iter: 0; batch classifier loss: 0.422365; batch adversarial loss: 0.484921\n",
            "epoch 14; iter: 200; batch classifier loss: 0.350949; batch adversarial loss: 0.532440\n",
            "epoch 15; iter: 0; batch classifier loss: 0.410333; batch adversarial loss: 0.494477\n",
            "epoch 15; iter: 200; batch classifier loss: 0.369847; batch adversarial loss: 0.555047\n",
            "epoch 16; iter: 0; batch classifier loss: 0.373230; batch adversarial loss: 0.558047\n",
            "epoch 16; iter: 200; batch classifier loss: 0.485141; batch adversarial loss: 0.455002\n",
            "epoch 17; iter: 0; batch classifier loss: 0.421525; batch adversarial loss: 0.527950\n",
            "epoch 17; iter: 200; batch classifier loss: 0.449973; batch adversarial loss: 0.551308\n",
            "epoch 18; iter: 0; batch classifier loss: 0.481563; batch adversarial loss: 0.517060\n",
            "epoch 18; iter: 200; batch classifier loss: 0.351310; batch adversarial loss: 0.578220\n",
            "epoch 19; iter: 0; batch classifier loss: 0.422752; batch adversarial loss: 0.621415\n",
            "epoch 19; iter: 200; batch classifier loss: 0.456301; batch adversarial loss: 0.503959\n",
            "epoch 20; iter: 0; batch classifier loss: 0.354392; batch adversarial loss: 0.543757\n",
            "epoch 20; iter: 200; batch classifier loss: 0.410181; batch adversarial loss: 0.515012\n",
            "epoch 21; iter: 0; batch classifier loss: 0.498070; batch adversarial loss: 0.475555\n",
            "epoch 21; iter: 200; batch classifier loss: 0.468026; batch adversarial loss: 0.430165\n",
            "epoch 22; iter: 0; batch classifier loss: 0.537075; batch adversarial loss: 0.505024\n",
            "epoch 22; iter: 200; batch classifier loss: 0.430939; batch adversarial loss: 0.473171\n",
            "epoch 23; iter: 0; batch classifier loss: 0.415721; batch adversarial loss: 0.514802\n",
            "epoch 23; iter: 200; batch classifier loss: 0.375512; batch adversarial loss: 0.461872\n",
            "epoch 24; iter: 0; batch classifier loss: 0.502587; batch adversarial loss: 0.489081\n",
            "epoch 24; iter: 200; batch classifier loss: 0.471898; batch adversarial loss: 0.487627\n",
            "epoch 25; iter: 0; batch classifier loss: 0.393500; batch adversarial loss: 0.535763\n",
            "epoch 25; iter: 200; batch classifier loss: 0.378166; batch adversarial loss: 0.503376\n",
            "epoch 26; iter: 0; batch classifier loss: 0.464553; batch adversarial loss: 0.507854\n",
            "epoch 26; iter: 200; batch classifier loss: 0.405493; batch adversarial loss: 0.487654\n",
            "epoch 27; iter: 0; batch classifier loss: 0.428209; batch adversarial loss: 0.494861\n",
            "epoch 27; iter: 200; batch classifier loss: 0.362968; batch adversarial loss: 0.520521\n",
            "epoch 28; iter: 0; batch classifier loss: 0.458559; batch adversarial loss: 0.455141\n",
            "epoch 28; iter: 200; batch classifier loss: 0.458856; batch adversarial loss: 0.493114\n",
            "epoch 29; iter: 0; batch classifier loss: 0.346847; batch adversarial loss: 0.513489\n",
            "epoch 29; iter: 200; batch classifier loss: 0.500382; batch adversarial loss: 0.552755\n",
            "epoch 30; iter: 0; batch classifier loss: 0.456958; batch adversarial loss: 0.515158\n",
            "epoch 30; iter: 200; batch classifier loss: 0.485963; batch adversarial loss: 0.563375\n",
            "epoch 31; iter: 0; batch classifier loss: 0.464618; batch adversarial loss: 0.527951\n",
            "epoch 31; iter: 200; batch classifier loss: 0.372404; batch adversarial loss: 0.526633\n",
            "epoch 32; iter: 0; batch classifier loss: 0.437182; batch adversarial loss: 0.510227\n",
            "epoch 32; iter: 200; batch classifier loss: 0.405398; batch adversarial loss: 0.502845\n",
            "epoch 33; iter: 0; batch classifier loss: 0.417860; batch adversarial loss: 0.492034\n",
            "epoch 33; iter: 200; batch classifier loss: 0.362217; batch adversarial loss: 0.599919\n",
            "epoch 34; iter: 0; batch classifier loss: 0.407442; batch adversarial loss: 0.583261\n",
            "epoch 34; iter: 200; batch classifier loss: 0.387995; batch adversarial loss: 0.492780\n",
            "epoch 35; iter: 0; batch classifier loss: 0.364554; batch adversarial loss: 0.598564\n",
            "epoch 35; iter: 200; batch classifier loss: 0.373027; batch adversarial loss: 0.505187\n",
            "epoch 36; iter: 0; batch classifier loss: 0.473521; batch adversarial loss: 0.439373\n",
            "epoch 36; iter: 200; batch classifier loss: 0.389912; batch adversarial loss: 0.580244\n",
            "epoch 37; iter: 0; batch classifier loss: 0.433528; batch adversarial loss: 0.474086\n",
            "epoch 37; iter: 200; batch classifier loss: 0.413194; batch adversarial loss: 0.466725\n",
            "epoch 38; iter: 0; batch classifier loss: 0.479227; batch adversarial loss: 0.466093\n",
            "epoch 38; iter: 200; batch classifier loss: 0.461120; batch adversarial loss: 0.541249\n",
            "epoch 39; iter: 0; batch classifier loss: 0.435381; batch adversarial loss: 0.479574\n",
            "epoch 39; iter: 200; batch classifier loss: 0.431058; batch adversarial loss: 0.478560\n",
            "epoch 40; iter: 0; batch classifier loss: 0.412334; batch adversarial loss: 0.526415\n",
            "epoch 40; iter: 200; batch classifier loss: 0.457436; batch adversarial loss: 0.568253\n",
            "epoch 41; iter: 0; batch classifier loss: 0.370030; batch adversarial loss: 0.502766\n",
            "epoch 41; iter: 200; batch classifier loss: 0.468080; batch adversarial loss: 0.484045\n",
            "epoch 42; iter: 0; batch classifier loss: 0.418224; batch adversarial loss: 0.564035\n",
            "epoch 42; iter: 200; batch classifier loss: 0.411382; batch adversarial loss: 0.549885\n",
            "epoch 43; iter: 0; batch classifier loss: 0.343827; batch adversarial loss: 0.538934\n",
            "epoch 43; iter: 200; batch classifier loss: 0.594725; batch adversarial loss: 0.516307\n",
            "epoch 44; iter: 0; batch classifier loss: 0.453768; batch adversarial loss: 0.497860\n",
            "epoch 44; iter: 200; batch classifier loss: 0.466419; batch adversarial loss: 0.479799\n",
            "epoch 45; iter: 0; batch classifier loss: 0.356097; batch adversarial loss: 0.538986\n",
            "epoch 45; iter: 200; batch classifier loss: 0.369161; batch adversarial loss: 0.502809\n",
            "epoch 46; iter: 0; batch classifier loss: 0.481668; batch adversarial loss: 0.501126\n",
            "epoch 46; iter: 200; batch classifier loss: 0.471188; batch adversarial loss: 0.479114\n",
            "epoch 47; iter: 0; batch classifier loss: 0.422267; batch adversarial loss: 0.506931\n",
            "epoch 47; iter: 200; batch classifier loss: 0.437133; batch adversarial loss: 0.528549\n",
            "epoch 48; iter: 0; batch classifier loss: 0.399573; batch adversarial loss: 0.523000\n",
            "epoch 48; iter: 200; batch classifier loss: 0.426986; batch adversarial loss: 0.532320\n",
            "epoch 49; iter: 0; batch classifier loss: 0.427391; batch adversarial loss: 0.492065\n",
            "epoch 49; iter: 200; batch classifier loss: 0.432186; batch adversarial loss: 0.550635\n",
            "epoch 0; iter: 0; batch classifier loss: 0.673204; batch adversarial loss: 0.632265\n",
            "epoch 0; iter: 200; batch classifier loss: 0.643629; batch adversarial loss: 0.702733\n",
            "epoch 1; iter: 0; batch classifier loss: 0.821758; batch adversarial loss: 0.703095\n",
            "epoch 1; iter: 200; batch classifier loss: 0.455997; batch adversarial loss: 0.614449\n",
            "epoch 2; iter: 0; batch classifier loss: 0.385864; batch adversarial loss: 0.652308\n",
            "epoch 2; iter: 200; batch classifier loss: 0.490964; batch adversarial loss: 0.639705\n",
            "epoch 3; iter: 0; batch classifier loss: 0.423235; batch adversarial loss: 0.658978\n",
            "epoch 3; iter: 200; batch classifier loss: 0.496867; batch adversarial loss: 0.611490\n",
            "epoch 4; iter: 0; batch classifier loss: 0.488227; batch adversarial loss: 0.583134\n",
            "epoch 4; iter: 200; batch classifier loss: 0.432080; batch adversarial loss: 0.687234\n",
            "epoch 5; iter: 0; batch classifier loss: 0.479892; batch adversarial loss: 0.605996\n",
            "epoch 5; iter: 200; batch classifier loss: 0.560235; batch adversarial loss: 0.632989\n",
            "epoch 6; iter: 0; batch classifier loss: 0.436782; batch adversarial loss: 0.599117\n",
            "epoch 6; iter: 200; batch classifier loss: 0.401770; batch adversarial loss: 0.646845\n",
            "epoch 7; iter: 0; batch classifier loss: 0.384401; batch adversarial loss: 0.627253\n",
            "epoch 7; iter: 200; batch classifier loss: 0.548341; batch adversarial loss: 0.607360\n",
            "epoch 8; iter: 0; batch classifier loss: 0.411658; batch adversarial loss: 0.630500\n",
            "epoch 8; iter: 200; batch classifier loss: 0.478048; batch adversarial loss: 0.561677\n",
            "epoch 9; iter: 0; batch classifier loss: 0.430927; batch adversarial loss: 0.631176\n",
            "epoch 9; iter: 200; batch classifier loss: 0.432351; batch adversarial loss: 0.625781\n",
            "epoch 10; iter: 0; batch classifier loss: 0.485091; batch adversarial loss: 0.586651\n",
            "epoch 10; iter: 200; batch classifier loss: 0.454030; batch adversarial loss: 0.592402\n",
            "epoch 11; iter: 0; batch classifier loss: 0.568213; batch adversarial loss: 0.616740\n",
            "epoch 11; iter: 200; batch classifier loss: 0.503420; batch adversarial loss: 0.583360\n",
            "epoch 12; iter: 0; batch classifier loss: 0.484906; batch adversarial loss: 0.636240\n",
            "epoch 12; iter: 200; batch classifier loss: 0.442505; batch adversarial loss: 0.605822\n",
            "epoch 13; iter: 0; batch classifier loss: 0.480581; batch adversarial loss: 0.664180\n",
            "epoch 13; iter: 200; batch classifier loss: 0.373116; batch adversarial loss: 0.593673\n",
            "epoch 14; iter: 0; batch classifier loss: 0.468445; batch adversarial loss: 0.585104\n",
            "epoch 14; iter: 200; batch classifier loss: 0.338840; batch adversarial loss: 0.665986\n",
            "epoch 15; iter: 0; batch classifier loss: 0.516491; batch adversarial loss: 0.623750\n",
            "epoch 15; iter: 200; batch classifier loss: 0.470986; batch adversarial loss: 0.631021\n",
            "epoch 16; iter: 0; batch classifier loss: 0.555971; batch adversarial loss: 0.640471\n",
            "epoch 16; iter: 200; batch classifier loss: 0.454638; batch adversarial loss: 0.592086\n",
            "epoch 17; iter: 0; batch classifier loss: 0.430033; batch adversarial loss: 0.586213\n",
            "epoch 17; iter: 200; batch classifier loss: 0.427565; batch adversarial loss: 0.600651\n",
            "epoch 18; iter: 0; batch classifier loss: 0.501874; batch adversarial loss: 0.562594\n",
            "epoch 18; iter: 200; batch classifier loss: 0.468368; batch adversarial loss: 0.636157\n",
            "epoch 19; iter: 0; batch classifier loss: 0.434529; batch adversarial loss: 0.684573\n",
            "epoch 19; iter: 200; batch classifier loss: 0.540551; batch adversarial loss: 0.585760\n",
            "epoch 20; iter: 0; batch classifier loss: 0.397546; batch adversarial loss: 0.664678\n",
            "epoch 20; iter: 200; batch classifier loss: 0.456948; batch adversarial loss: 0.670160\n",
            "epoch 21; iter: 0; batch classifier loss: 0.524035; batch adversarial loss: 0.538643\n",
            "epoch 21; iter: 200; batch classifier loss: 0.506503; batch adversarial loss: 0.618531\n",
            "epoch 22; iter: 0; batch classifier loss: 0.564780; batch adversarial loss: 0.589440\n",
            "epoch 22; iter: 200; batch classifier loss: 0.429850; batch adversarial loss: 0.619170\n",
            "epoch 23; iter: 0; batch classifier loss: 0.626324; batch adversarial loss: 0.570282\n",
            "epoch 23; iter: 200; batch classifier loss: 0.426202; batch adversarial loss: 0.651401\n",
            "epoch 24; iter: 0; batch classifier loss: 0.445646; batch adversarial loss: 0.636509\n",
            "epoch 24; iter: 200; batch classifier loss: 0.456701; batch adversarial loss: 0.632311\n",
            "epoch 25; iter: 0; batch classifier loss: 0.444657; batch adversarial loss: 0.572339\n",
            "epoch 25; iter: 200; batch classifier loss: 0.556819; batch adversarial loss: 0.666625\n",
            "epoch 26; iter: 0; batch classifier loss: 0.625625; batch adversarial loss: 0.636020\n",
            "epoch 26; iter: 200; batch classifier loss: 0.550046; batch adversarial loss: 0.606788\n",
            "epoch 27; iter: 0; batch classifier loss: 0.455010; batch adversarial loss: 0.644465\n",
            "epoch 27; iter: 200; batch classifier loss: 0.506823; batch adversarial loss: 0.638696\n",
            "epoch 28; iter: 0; batch classifier loss: 0.463895; batch adversarial loss: 0.618123\n",
            "epoch 28; iter: 200; batch classifier loss: 0.549973; batch adversarial loss: 0.627976\n",
            "epoch 29; iter: 0; batch classifier loss: 0.433147; batch adversarial loss: 0.721845\n",
            "epoch 29; iter: 200; batch classifier loss: 0.554651; batch adversarial loss: 0.539009\n",
            "epoch 30; iter: 0; batch classifier loss: 0.499171; batch adversarial loss: 0.645745\n",
            "epoch 30; iter: 200; batch classifier loss: 0.475490; batch adversarial loss: 0.707122\n",
            "epoch 31; iter: 0; batch classifier loss: 0.458271; batch adversarial loss: 0.573157\n",
            "epoch 31; iter: 200; batch classifier loss: 0.527682; batch adversarial loss: 0.609198\n",
            "epoch 32; iter: 0; batch classifier loss: 0.615798; batch adversarial loss: 0.623029\n",
            "epoch 32; iter: 200; batch classifier loss: 0.444629; batch adversarial loss: 0.645791\n",
            "epoch 33; iter: 0; batch classifier loss: 0.545599; batch adversarial loss: 0.590252\n",
            "epoch 33; iter: 200; batch classifier loss: 0.553370; batch adversarial loss: 0.588862\n",
            "epoch 34; iter: 0; batch classifier loss: 0.436067; batch adversarial loss: 0.571520\n",
            "epoch 34; iter: 200; batch classifier loss: 0.420296; batch adversarial loss: 0.561477\n",
            "epoch 35; iter: 0; batch classifier loss: 0.485563; batch adversarial loss: 0.622931\n",
            "epoch 35; iter: 200; batch classifier loss: 0.504061; batch adversarial loss: 0.533620\n",
            "epoch 36; iter: 0; batch classifier loss: 0.522735; batch adversarial loss: 0.623053\n",
            "epoch 36; iter: 200; batch classifier loss: 0.485071; batch adversarial loss: 0.625508\n",
            "epoch 37; iter: 0; batch classifier loss: 0.469200; batch adversarial loss: 0.602047\n",
            "epoch 37; iter: 200; batch classifier loss: 0.427303; batch adversarial loss: 0.625452\n",
            "epoch 38; iter: 0; batch classifier loss: 0.483789; batch adversarial loss: 0.596428\n",
            "epoch 38; iter: 200; batch classifier loss: 0.452300; batch adversarial loss: 0.660450\n",
            "epoch 39; iter: 0; batch classifier loss: 0.527332; batch adversarial loss: 0.668835\n",
            "epoch 39; iter: 200; batch classifier loss: 0.488639; batch adversarial loss: 0.626168\n",
            "epoch 40; iter: 0; batch classifier loss: 0.495341; batch adversarial loss: 0.630448\n",
            "epoch 40; iter: 200; batch classifier loss: 0.402643; batch adversarial loss: 0.649588\n",
            "epoch 41; iter: 0; batch classifier loss: 0.582452; batch adversarial loss: 0.602316\n",
            "epoch 41; iter: 200; batch classifier loss: 0.521708; batch adversarial loss: 0.628394\n",
            "epoch 42; iter: 0; batch classifier loss: 0.456631; batch adversarial loss: 0.627556\n",
            "epoch 42; iter: 200; batch classifier loss: 0.495343; batch adversarial loss: 0.590157\n",
            "epoch 43; iter: 0; batch classifier loss: 0.573642; batch adversarial loss: 0.616194\n",
            "epoch 43; iter: 200; batch classifier loss: 0.405061; batch adversarial loss: 0.655876\n",
            "epoch 44; iter: 0; batch classifier loss: 0.488569; batch adversarial loss: 0.643897\n",
            "epoch 44; iter: 200; batch classifier loss: 0.621453; batch adversarial loss: 0.625982\n",
            "epoch 45; iter: 0; batch classifier loss: 0.458806; batch adversarial loss: 0.586934\n",
            "epoch 45; iter: 200; batch classifier loss: 0.476923; batch adversarial loss: 0.637747\n",
            "epoch 46; iter: 0; batch classifier loss: 0.409092; batch adversarial loss: 0.651579\n",
            "epoch 46; iter: 200; batch classifier loss: 0.475276; batch adversarial loss: 0.635407\n",
            "epoch 47; iter: 0; batch classifier loss: 0.477508; batch adversarial loss: 0.591435\n",
            "epoch 47; iter: 200; batch classifier loss: 0.545650; batch adversarial loss: 0.618993\n",
            "epoch 48; iter: 0; batch classifier loss: 0.542954; batch adversarial loss: 0.594428\n",
            "epoch 48; iter: 200; batch classifier loss: 0.523232; batch adversarial loss: 0.564405\n",
            "epoch 49; iter: 0; batch classifier loss: 0.463272; batch adversarial loss: 0.601935\n",
            "epoch 49; iter: 200; batch classifier loss: 0.518276; batch adversarial loss: 0.614598\n",
            "epoch 0; iter: 0; batch classifier loss: 0.747524; batch adversarial loss: 0.863486\n",
            "epoch 0; iter: 200; batch classifier loss: 1.317106; batch adversarial loss: 0.857811\n",
            "epoch 1; iter: 0; batch classifier loss: 1.396651; batch adversarial loss: 0.831536\n",
            "epoch 1; iter: 200; batch classifier loss: 1.426887; batch adversarial loss: 0.673534\n",
            "epoch 2; iter: 0; batch classifier loss: 1.169032; batch adversarial loss: 0.676690\n",
            "epoch 2; iter: 200; batch classifier loss: 1.118008; batch adversarial loss: 0.602108\n",
            "epoch 3; iter: 0; batch classifier loss: 1.004779; batch adversarial loss: 0.634107\n",
            "epoch 3; iter: 200; batch classifier loss: 0.761638; batch adversarial loss: 0.645559\n",
            "epoch 4; iter: 0; batch classifier loss: 0.945244; batch adversarial loss: 0.624714\n",
            "epoch 4; iter: 200; batch classifier loss: 0.848947; batch adversarial loss: 0.630466\n",
            "epoch 5; iter: 0; batch classifier loss: 0.860845; batch adversarial loss: 0.671362\n",
            "epoch 5; iter: 200; batch classifier loss: 1.301358; batch adversarial loss: 0.633716\n",
            "epoch 6; iter: 0; batch classifier loss: 1.044292; batch adversarial loss: 0.653370\n",
            "epoch 6; iter: 200; batch classifier loss: 0.933911; batch adversarial loss: 0.643474\n",
            "epoch 7; iter: 0; batch classifier loss: 0.757381; batch adversarial loss: 0.653904\n",
            "epoch 7; iter: 200; batch classifier loss: 0.774783; batch adversarial loss: 0.646418\n",
            "epoch 8; iter: 0; batch classifier loss: 0.791989; batch adversarial loss: 0.658343\n",
            "epoch 8; iter: 200; batch classifier loss: 0.821739; batch adversarial loss: 0.635169\n",
            "epoch 9; iter: 0; batch classifier loss: 0.584814; batch adversarial loss: 0.668141\n",
            "epoch 9; iter: 200; batch classifier loss: 0.838092; batch adversarial loss: 0.643401\n",
            "epoch 10; iter: 0; batch classifier loss: 0.770457; batch adversarial loss: 0.644527\n",
            "epoch 10; iter: 200; batch classifier loss: 0.906534; batch adversarial loss: 0.629388\n",
            "epoch 11; iter: 0; batch classifier loss: 0.810510; batch adversarial loss: 0.620244\n",
            "epoch 11; iter: 200; batch classifier loss: 0.801435; batch adversarial loss: 0.689618\n",
            "epoch 12; iter: 0; batch classifier loss: 1.187121; batch adversarial loss: 0.613277\n",
            "epoch 12; iter: 200; batch classifier loss: 1.090157; batch adversarial loss: 0.643115\n",
            "epoch 13; iter: 0; batch classifier loss: 0.898651; batch adversarial loss: 0.632500\n",
            "epoch 13; iter: 200; batch classifier loss: 0.913993; batch adversarial loss: 0.615567\n",
            "epoch 14; iter: 0; batch classifier loss: 0.776927; batch adversarial loss: 0.645852\n",
            "epoch 14; iter: 200; batch classifier loss: 0.736630; batch adversarial loss: 0.650784\n",
            "epoch 15; iter: 0; batch classifier loss: 0.667469; batch adversarial loss: 0.661103\n",
            "epoch 15; iter: 200; batch classifier loss: 0.652631; batch adversarial loss: 0.649598\n",
            "epoch 16; iter: 0; batch classifier loss: 0.678637; batch adversarial loss: 0.660359\n",
            "epoch 16; iter: 200; batch classifier loss: 0.491784; batch adversarial loss: 0.610950\n",
            "epoch 17; iter: 0; batch classifier loss: 0.512753; batch adversarial loss: 0.649572\n",
            "epoch 17; iter: 200; batch classifier loss: 0.588394; batch adversarial loss: 0.621768\n",
            "epoch 18; iter: 0; batch classifier loss: 0.628746; batch adversarial loss: 0.666225\n",
            "epoch 18; iter: 200; batch classifier loss: 0.761935; batch adversarial loss: 0.685128\n",
            "epoch 19; iter: 0; batch classifier loss: 0.912726; batch adversarial loss: 0.557614\n",
            "epoch 19; iter: 200; batch classifier loss: 0.791468; batch adversarial loss: 0.626968\n",
            "epoch 20; iter: 0; batch classifier loss: 0.571383; batch adversarial loss: 0.599021\n",
            "epoch 20; iter: 200; batch classifier loss: 0.745372; batch adversarial loss: 0.599771\n",
            "epoch 21; iter: 0; batch classifier loss: 0.623354; batch adversarial loss: 0.627608\n",
            "epoch 21; iter: 200; batch classifier loss: 0.544716; batch adversarial loss: 0.623439\n",
            "epoch 22; iter: 0; batch classifier loss: 0.580700; batch adversarial loss: 0.624232\n",
            "epoch 22; iter: 200; batch classifier loss: 0.520534; batch adversarial loss: 0.643191\n",
            "epoch 23; iter: 0; batch classifier loss: 0.854424; batch adversarial loss: 0.645397\n",
            "epoch 23; iter: 200; batch classifier loss: 0.468082; batch adversarial loss: 0.592491\n",
            "epoch 24; iter: 0; batch classifier loss: 0.772871; batch adversarial loss: 0.617331\n",
            "epoch 24; iter: 200; batch classifier loss: 0.668685; batch adversarial loss: 0.616421\n",
            "epoch 25; iter: 0; batch classifier loss: 0.783378; batch adversarial loss: 0.681902\n",
            "epoch 25; iter: 200; batch classifier loss: 0.584261; batch adversarial loss: 0.644902\n",
            "epoch 26; iter: 0; batch classifier loss: 0.811627; batch adversarial loss: 0.649616\n",
            "epoch 26; iter: 200; batch classifier loss: 0.544511; batch adversarial loss: 0.667467\n",
            "epoch 27; iter: 0; batch classifier loss: 0.825812; batch adversarial loss: 0.595129\n",
            "epoch 27; iter: 200; batch classifier loss: 0.609435; batch adversarial loss: 0.667489\n",
            "epoch 28; iter: 0; batch classifier loss: 0.715614; batch adversarial loss: 0.618536\n",
            "epoch 28; iter: 200; batch classifier loss: 0.511939; batch adversarial loss: 0.673292\n",
            "epoch 29; iter: 0; batch classifier loss: 0.843708; batch adversarial loss: 0.593910\n",
            "epoch 29; iter: 200; batch classifier loss: 0.556542; batch adversarial loss: 0.659207\n",
            "epoch 30; iter: 0; batch classifier loss: 0.744394; batch adversarial loss: 0.650763\n",
            "epoch 30; iter: 200; batch classifier loss: 0.558091; batch adversarial loss: 0.692696\n",
            "epoch 31; iter: 0; batch classifier loss: 0.624263; batch adversarial loss: 0.621537\n",
            "epoch 31; iter: 200; batch classifier loss: 0.591068; batch adversarial loss: 0.600742\n",
            "epoch 32; iter: 0; batch classifier loss: 0.672535; batch adversarial loss: 0.649062\n",
            "epoch 32; iter: 200; batch classifier loss: 0.542086; batch adversarial loss: 0.655383\n",
            "epoch 33; iter: 0; batch classifier loss: 0.623811; batch adversarial loss: 0.665321\n",
            "epoch 33; iter: 200; batch classifier loss: 0.589718; batch adversarial loss: 0.649118\n",
            "epoch 34; iter: 0; batch classifier loss: 0.549738; batch adversarial loss: 0.633911\n",
            "epoch 34; iter: 200; batch classifier loss: 0.792214; batch adversarial loss: 0.638144\n",
            "epoch 35; iter: 0; batch classifier loss: 0.643120; batch adversarial loss: 0.600284\n",
            "epoch 35; iter: 200; batch classifier loss: 0.700988; batch adversarial loss: 0.639367\n",
            "epoch 36; iter: 0; batch classifier loss: 0.681731; batch adversarial loss: 0.605489\n",
            "epoch 36; iter: 200; batch classifier loss: 0.696321; batch adversarial loss: 0.660366\n",
            "epoch 37; iter: 0; batch classifier loss: 0.815417; batch adversarial loss: 0.634076\n",
            "epoch 37; iter: 200; batch classifier loss: 0.578189; batch adversarial loss: 0.615531\n",
            "epoch 38; iter: 0; batch classifier loss: 0.611427; batch adversarial loss: 0.637881\n",
            "epoch 38; iter: 200; batch classifier loss: 0.601191; batch adversarial loss: 0.589758\n",
            "epoch 39; iter: 0; batch classifier loss: 0.705336; batch adversarial loss: 0.632944\n",
            "epoch 39; iter: 200; batch classifier loss: 0.661391; batch adversarial loss: 0.588585\n",
            "epoch 40; iter: 0; batch classifier loss: 0.591942; batch adversarial loss: 0.666801\n",
            "epoch 40; iter: 200; batch classifier loss: 0.549151; batch adversarial loss: 0.672686\n",
            "epoch 41; iter: 0; batch classifier loss: 0.571387; batch adversarial loss: 0.621756\n",
            "epoch 41; iter: 200; batch classifier loss: 0.505036; batch adversarial loss: 0.595182\n",
            "epoch 42; iter: 0; batch classifier loss: 0.430813; batch adversarial loss: 0.629632\n",
            "epoch 42; iter: 200; batch classifier loss: 0.535818; batch adversarial loss: 0.666634\n",
            "epoch 43; iter: 0; batch classifier loss: 0.661831; batch adversarial loss: 0.648943\n",
            "epoch 43; iter: 200; batch classifier loss: 0.526594; batch adversarial loss: 0.639212\n",
            "epoch 44; iter: 0; batch classifier loss: 0.672898; batch adversarial loss: 0.616008\n",
            "epoch 44; iter: 200; batch classifier loss: 0.601947; batch adversarial loss: 0.640461\n",
            "epoch 45; iter: 0; batch classifier loss: 0.500215; batch adversarial loss: 0.695172\n",
            "epoch 45; iter: 200; batch classifier loss: 0.711733; batch adversarial loss: 0.659301\n",
            "epoch 46; iter: 0; batch classifier loss: 0.692845; batch adversarial loss: 0.615818\n",
            "epoch 46; iter: 200; batch classifier loss: 0.577547; batch adversarial loss: 0.655070\n",
            "epoch 47; iter: 0; batch classifier loss: 0.610898; batch adversarial loss: 0.666004\n",
            "epoch 47; iter: 200; batch classifier loss: 0.731180; batch adversarial loss: 0.593861\n",
            "epoch 48; iter: 0; batch classifier loss: 0.830489; batch adversarial loss: 0.584200\n",
            "epoch 48; iter: 200; batch classifier loss: 0.600569; batch adversarial loss: 0.627370\n",
            "epoch 49; iter: 0; batch classifier loss: 0.894154; batch adversarial loss: 0.583029\n",
            "epoch 49; iter: 200; batch classifier loss: 0.557784; batch adversarial loss: 0.654424\n",
            "epoch 0; iter: 0; batch classifier loss: 0.665558; batch adversarial loss: 0.712204\n",
            "epoch 0; iter: 200; batch classifier loss: 1.168483; batch adversarial loss: 0.665112\n",
            "epoch 1; iter: 0; batch classifier loss: 1.183597; batch adversarial loss: 0.673278\n",
            "epoch 1; iter: 200; batch classifier loss: 1.674928; batch adversarial loss: 0.648715\n",
            "epoch 2; iter: 0; batch classifier loss: 1.711412; batch adversarial loss: 0.629427\n",
            "epoch 2; iter: 200; batch classifier loss: 1.887880; batch adversarial loss: 0.602197\n",
            "epoch 3; iter: 0; batch classifier loss: 1.825049; batch adversarial loss: 0.615104\n",
            "epoch 3; iter: 200; batch classifier loss: 1.854696; batch adversarial loss: 0.595105\n",
            "epoch 4; iter: 0; batch classifier loss: 1.964295; batch adversarial loss: 0.616486\n",
            "epoch 4; iter: 200; batch classifier loss: 1.594125; batch adversarial loss: 0.614007\n",
            "epoch 5; iter: 0; batch classifier loss: 1.360672; batch adversarial loss: 0.588306\n",
            "epoch 5; iter: 200; batch classifier loss: 1.493211; batch adversarial loss: 0.612179\n",
            "epoch 6; iter: 0; batch classifier loss: 1.435760; batch adversarial loss: 0.643593\n",
            "epoch 6; iter: 200; batch classifier loss: 1.958572; batch adversarial loss: 0.562721\n",
            "epoch 7; iter: 0; batch classifier loss: 1.823490; batch adversarial loss: 0.622046\n",
            "epoch 7; iter: 200; batch classifier loss: 1.502675; batch adversarial loss: 0.627401\n",
            "epoch 8; iter: 0; batch classifier loss: 1.504629; batch adversarial loss: 0.632853\n",
            "epoch 8; iter: 200; batch classifier loss: 1.397508; batch adversarial loss: 0.599600\n",
            "epoch 9; iter: 0; batch classifier loss: 1.386663; batch adversarial loss: 0.621789\n",
            "epoch 9; iter: 200; batch classifier loss: 1.296960; batch adversarial loss: 0.671635\n",
            "epoch 10; iter: 0; batch classifier loss: 1.482303; batch adversarial loss: 0.616245\n",
            "epoch 10; iter: 200; batch classifier loss: 1.746903; batch adversarial loss: 0.610641\n",
            "epoch 11; iter: 0; batch classifier loss: 1.790539; batch adversarial loss: 0.643859\n",
            "epoch 11; iter: 200; batch classifier loss: 1.530423; batch adversarial loss: 0.605185\n",
            "epoch 12; iter: 0; batch classifier loss: 1.412777; batch adversarial loss: 0.649431\n",
            "epoch 12; iter: 200; batch classifier loss: 2.113639; batch adversarial loss: 0.627314\n",
            "epoch 13; iter: 0; batch classifier loss: 1.473608; batch adversarial loss: 0.627295\n",
            "epoch 13; iter: 200; batch classifier loss: 1.435386; batch adversarial loss: 0.694215\n",
            "epoch 14; iter: 0; batch classifier loss: 1.641300; batch adversarial loss: 0.599600\n",
            "epoch 14; iter: 200; batch classifier loss: 1.171776; batch adversarial loss: 0.649489\n",
            "epoch 15; iter: 0; batch classifier loss: 1.149172; batch adversarial loss: 0.616277\n",
            "epoch 15; iter: 200; batch classifier loss: 1.443709; batch adversarial loss: 0.627270\n",
            "epoch 16; iter: 0; batch classifier loss: 1.155986; batch adversarial loss: 0.638410\n",
            "epoch 16; iter: 200; batch classifier loss: 1.123504; batch adversarial loss: 0.621777\n",
            "epoch 17; iter: 0; batch classifier loss: 0.887708; batch adversarial loss: 0.644265\n",
            "epoch 17; iter: 200; batch classifier loss: 1.958593; batch adversarial loss: 0.655113\n",
            "epoch 18; iter: 0; batch classifier loss: 1.834684; batch adversarial loss: 0.621665\n",
            "epoch 18; iter: 200; batch classifier loss: 1.791485; batch adversarial loss: 0.654962\n",
            "epoch 19; iter: 0; batch classifier loss: 2.351995; batch adversarial loss: 0.588147\n",
            "epoch 19; iter: 200; batch classifier loss: 1.895459; batch adversarial loss: 0.643881\n",
            "epoch 20; iter: 0; batch classifier loss: 1.871752; batch adversarial loss: 0.627289\n",
            "epoch 20; iter: 200; batch classifier loss: 1.586061; batch adversarial loss: 0.683290\n",
            "epoch 21; iter: 0; batch classifier loss: 1.844682; batch adversarial loss: 0.638372\n",
            "epoch 21; iter: 200; batch classifier loss: 1.587681; batch adversarial loss: 0.599822\n",
            "epoch 22; iter: 0; batch classifier loss: 1.016412; batch adversarial loss: 0.632852\n",
            "epoch 22; iter: 200; batch classifier loss: 1.543870; batch adversarial loss: 0.643981\n",
            "epoch 23; iter: 0; batch classifier loss: 1.476352; batch adversarial loss: 0.627282\n",
            "epoch 23; iter: 200; batch classifier loss: 0.474793; batch adversarial loss: 0.674317\n",
            "epoch 24; iter: 0; batch classifier loss: 1.220842; batch adversarial loss: 0.638368\n",
            "epoch 24; iter: 200; batch classifier loss: 2.316253; batch adversarial loss: 0.649640\n",
            "epoch 25; iter: 0; batch classifier loss: 2.506072; batch adversarial loss: 0.610676\n",
            "epoch 25; iter: 200; batch classifier loss: 2.255411; batch adversarial loss: 0.638357\n",
            "epoch 26; iter: 0; batch classifier loss: 1.667890; batch adversarial loss: 0.638415\n",
            "epoch 26; iter: 200; batch classifier loss: 2.193751; batch adversarial loss: 0.610654\n",
            "epoch 27; iter: 0; batch classifier loss: 1.839894; batch adversarial loss: 0.632845\n",
            "epoch 27; iter: 200; batch classifier loss: 2.142818; batch adversarial loss: 0.649477\n",
            "epoch 28; iter: 0; batch classifier loss: 2.456540; batch adversarial loss: 0.566333\n",
            "epoch 28; iter: 200; batch classifier loss: 2.428675; batch adversarial loss: 0.621820\n",
            "epoch 29; iter: 0; batch classifier loss: 2.256812; batch adversarial loss: 0.610647\n",
            "epoch 29; iter: 200; batch classifier loss: 1.616226; batch adversarial loss: 0.604804\n",
            "epoch 30; iter: 0; batch classifier loss: 1.933839; batch adversarial loss: 0.627294\n",
            "epoch 30; iter: 200; batch classifier loss: 2.037398; batch adversarial loss: 0.593911\n",
            "epoch 31; iter: 0; batch classifier loss: 1.881989; batch adversarial loss: 0.588505\n",
            "epoch 31; iter: 200; batch classifier loss: 1.936220; batch adversarial loss: 0.610533\n",
            "epoch 32; iter: 0; batch classifier loss: 2.292816; batch adversarial loss: 0.616250\n",
            "epoch 32; iter: 200; batch classifier loss: 1.733481; batch adversarial loss: 0.610562\n",
            "epoch 33; iter: 0; batch classifier loss: 1.753137; batch adversarial loss: 0.616202\n",
            "epoch 33; iter: 200; batch classifier loss: 1.238074; batch adversarial loss: 0.660578\n",
            "epoch 34; iter: 0; batch classifier loss: 1.268070; batch adversarial loss: 0.649521\n",
            "epoch 34; iter: 200; batch classifier loss: 2.274111; batch adversarial loss: 0.643055\n",
            "epoch 35; iter: 0; batch classifier loss: 2.686891; batch adversarial loss: 0.583150\n",
            "epoch 35; iter: 200; batch classifier loss: 2.133859; batch adversarial loss: 0.693232\n",
            "epoch 36; iter: 0; batch classifier loss: 2.198018; batch adversarial loss: 0.610703\n",
            "epoch 36; iter: 200; batch classifier loss: 2.189753; batch adversarial loss: 0.582738\n",
            "epoch 37; iter: 0; batch classifier loss: 1.730925; batch adversarial loss: 0.660547\n",
            "epoch 37; iter: 200; batch classifier loss: 1.545343; batch adversarial loss: 0.627279\n",
            "epoch 38; iter: 0; batch classifier loss: 2.107332; batch adversarial loss: 0.632819\n",
            "epoch 38; iter: 200; batch classifier loss: 1.731021; batch adversarial loss: 0.621790\n",
            "epoch 39; iter: 0; batch classifier loss: 2.277663; batch adversarial loss: 0.627289\n",
            "epoch 39; iter: 200; batch classifier loss: 2.115262; batch adversarial loss: 0.616122\n",
            "epoch 40; iter: 0; batch classifier loss: 3.472577; batch adversarial loss: 0.566351\n",
            "epoch 40; iter: 200; batch classifier loss: 2.383121; batch adversarial loss: 0.677100\n",
            "epoch 41; iter: 0; batch classifier loss: 2.760423; batch adversarial loss: 0.627299\n",
            "epoch 41; iter: 200; batch classifier loss: 2.271527; batch adversarial loss: 0.666272\n",
            "epoch 42; iter: 0; batch classifier loss: 1.893711; batch adversarial loss: 0.660638\n",
            "epoch 42; iter: 200; batch classifier loss: 2.454885; batch adversarial loss: 0.616135\n",
            "epoch 43; iter: 0; batch classifier loss: 1.878233; batch adversarial loss: 0.632845\n",
            "epoch 43; iter: 200; batch classifier loss: 1.746696; batch adversarial loss: 0.633463\n",
            "epoch 44; iter: 0; batch classifier loss: 1.683858; batch adversarial loss: 0.619615\n",
            "epoch 44; iter: 200; batch classifier loss: 2.973378; batch adversarial loss: 0.588195\n",
            "epoch 45; iter: 0; batch classifier loss: 3.162674; batch adversarial loss: 0.643975\n",
            "epoch 45; iter: 200; batch classifier loss: 3.096029; batch adversarial loss: 0.632838\n",
            "epoch 46; iter: 0; batch classifier loss: 2.731093; batch adversarial loss: 0.621698\n",
            "epoch 46; iter: 200; batch classifier loss: 2.048668; batch adversarial loss: 0.638488\n",
            "epoch 47; iter: 0; batch classifier loss: 3.157186; batch adversarial loss: 0.638441\n",
            "epoch 47; iter: 200; batch classifier loss: 2.481861; batch adversarial loss: 0.688149\n",
            "epoch 48; iter: 0; batch classifier loss: 2.485077; batch adversarial loss: 0.627293\n",
            "epoch 48; iter: 200; batch classifier loss: 2.356263; batch adversarial loss: 0.627463\n",
            "epoch 49; iter: 0; batch classifier loss: 1.711385; batch adversarial loss: 0.618629\n",
            "epoch 49; iter: 200; batch classifier loss: 1.788399; batch adversarial loss: 0.648775\n",
            "\n",
            " Welcome to Fold number 3 \n",
            "\n",
            "epoch 0; iter: 0; batch classifier loss: 0.661801; batch adversarial loss: 0.771872\n",
            "epoch 0; iter: 200; batch classifier loss: 0.541612; batch adversarial loss: 0.683933\n",
            "epoch 1; iter: 0; batch classifier loss: 0.440043; batch adversarial loss: 0.666724\n",
            "epoch 1; iter: 200; batch classifier loss: 0.425032; batch adversarial loss: 0.668467\n",
            "epoch 2; iter: 0; batch classifier loss: 0.463994; batch adversarial loss: 0.627422\n",
            "epoch 2; iter: 200; batch classifier loss: 0.451991; batch adversarial loss: 0.635963\n",
            "epoch 3; iter: 0; batch classifier loss: 0.425191; batch adversarial loss: 0.558569\n",
            "epoch 3; iter: 200; batch classifier loss: 0.560484; batch adversarial loss: 0.527732\n",
            "epoch 4; iter: 0; batch classifier loss: 0.483848; batch adversarial loss: 0.526164\n",
            "epoch 4; iter: 200; batch classifier loss: 0.398827; batch adversarial loss: 0.579600\n",
            "epoch 5; iter: 0; batch classifier loss: 0.372585; batch adversarial loss: 0.557806\n",
            "epoch 5; iter: 200; batch classifier loss: 0.402557; batch adversarial loss: 0.546914\n",
            "epoch 6; iter: 0; batch classifier loss: 0.477556; batch adversarial loss: 0.515950\n",
            "epoch 6; iter: 200; batch classifier loss: 0.426335; batch adversarial loss: 0.522676\n",
            "epoch 7; iter: 0; batch classifier loss: 0.366907; batch adversarial loss: 0.555350\n",
            "epoch 7; iter: 200; batch classifier loss: 0.356406; batch adversarial loss: 0.612240\n",
            "epoch 8; iter: 0; batch classifier loss: 0.506477; batch adversarial loss: 0.625648\n",
            "epoch 8; iter: 200; batch classifier loss: 0.418111; batch adversarial loss: 0.575402\n",
            "epoch 9; iter: 0; batch classifier loss: 0.394764; batch adversarial loss: 0.594208\n",
            "epoch 9; iter: 200; batch classifier loss: 0.491718; batch adversarial loss: 0.635718\n",
            "epoch 10; iter: 0; batch classifier loss: 0.458588; batch adversarial loss: 0.593135\n",
            "epoch 10; iter: 200; batch classifier loss: 0.393301; batch adversarial loss: 0.672132\n",
            "epoch 11; iter: 0; batch classifier loss: 0.415459; batch adversarial loss: 0.653714\n",
            "epoch 11; iter: 200; batch classifier loss: 0.412391; batch adversarial loss: 0.616095\n",
            "epoch 12; iter: 0; batch classifier loss: 0.426195; batch adversarial loss: 0.632370\n",
            "epoch 12; iter: 200; batch classifier loss: 0.434753; batch adversarial loss: 0.695796\n",
            "epoch 13; iter: 0; batch classifier loss: 0.433313; batch adversarial loss: 0.635126\n",
            "epoch 13; iter: 200; batch classifier loss: 0.438704; batch adversarial loss: 0.671052\n",
            "epoch 14; iter: 0; batch classifier loss: 0.450170; batch adversarial loss: 0.675417\n",
            "epoch 14; iter: 200; batch classifier loss: 0.362187; batch adversarial loss: 0.684765\n",
            "epoch 15; iter: 0; batch classifier loss: 0.331891; batch adversarial loss: 0.725985\n",
            "epoch 15; iter: 200; batch classifier loss: 0.490480; batch adversarial loss: 0.663080\n",
            "epoch 16; iter: 0; batch classifier loss: 0.416609; batch adversarial loss: 0.653973\n",
            "epoch 16; iter: 200; batch classifier loss: 0.337748; batch adversarial loss: 0.628923\n",
            "epoch 17; iter: 0; batch classifier loss: 0.393303; batch adversarial loss: 0.609870\n",
            "epoch 17; iter: 200; batch classifier loss: 0.433471; batch adversarial loss: 0.622545\n",
            "epoch 18; iter: 0; batch classifier loss: 0.406307; batch adversarial loss: 0.681133\n",
            "epoch 18; iter: 200; batch classifier loss: 0.400736; batch adversarial loss: 0.632040\n",
            "epoch 19; iter: 0; batch classifier loss: 0.436775; batch adversarial loss: 0.598607\n",
            "epoch 19; iter: 200; batch classifier loss: 0.422789; batch adversarial loss: 0.633448\n",
            "epoch 20; iter: 0; batch classifier loss: 0.389972; batch adversarial loss: 0.583268\n",
            "epoch 20; iter: 200; batch classifier loss: 0.425648; batch adversarial loss: 0.647043\n",
            "epoch 21; iter: 0; batch classifier loss: 0.490288; batch adversarial loss: 0.605847\n",
            "epoch 21; iter: 200; batch classifier loss: 0.490051; batch adversarial loss: 0.555032\n",
            "epoch 22; iter: 0; batch classifier loss: 0.435326; batch adversarial loss: 0.587520\n",
            "epoch 22; iter: 200; batch classifier loss: 0.374118; batch adversarial loss: 0.560266\n",
            "epoch 23; iter: 0; batch classifier loss: 0.483716; batch adversarial loss: 0.533368\n",
            "epoch 23; iter: 200; batch classifier loss: 0.413414; batch adversarial loss: 0.558719\n",
            "epoch 24; iter: 0; batch classifier loss: 0.459011; batch adversarial loss: 0.528813\n",
            "epoch 24; iter: 200; batch classifier loss: 0.414793; batch adversarial loss: 0.581731\n",
            "epoch 25; iter: 0; batch classifier loss: 0.443963; batch adversarial loss: 0.525768\n",
            "epoch 25; iter: 200; batch classifier loss: 0.456328; batch adversarial loss: 0.536363\n",
            "epoch 26; iter: 0; batch classifier loss: 0.406795; batch adversarial loss: 0.550389\n",
            "epoch 26; iter: 200; batch classifier loss: 0.377582; batch adversarial loss: 0.547843\n",
            "epoch 27; iter: 0; batch classifier loss: 0.455009; batch adversarial loss: 0.552050\n",
            "epoch 27; iter: 200; batch classifier loss: 0.358774; batch adversarial loss: 0.541219\n",
            "epoch 28; iter: 0; batch classifier loss: 0.418680; batch adversarial loss: 0.505336\n",
            "epoch 28; iter: 200; batch classifier loss: 0.474995; batch adversarial loss: 0.521323\n",
            "epoch 29; iter: 0; batch classifier loss: 0.465494; batch adversarial loss: 0.486862\n",
            "epoch 29; iter: 200; batch classifier loss: 0.466694; batch adversarial loss: 0.507239\n",
            "epoch 30; iter: 0; batch classifier loss: 0.416190; batch adversarial loss: 0.482313\n",
            "epoch 30; iter: 200; batch classifier loss: 0.381343; batch adversarial loss: 0.512170\n",
            "epoch 31; iter: 0; batch classifier loss: 0.354306; batch adversarial loss: 0.473259\n",
            "epoch 31; iter: 200; batch classifier loss: 0.406618; batch adversarial loss: 0.542698\n",
            "epoch 32; iter: 0; batch classifier loss: 0.353784; batch adversarial loss: 0.493622\n",
            "epoch 32; iter: 200; batch classifier loss: 0.464162; batch adversarial loss: 0.479266\n",
            "epoch 33; iter: 0; batch classifier loss: 0.397652; batch adversarial loss: 0.461438\n",
            "epoch 33; iter: 200; batch classifier loss: 0.462661; batch adversarial loss: 0.416247\n",
            "epoch 34; iter: 0; batch classifier loss: 0.435625; batch adversarial loss: 0.431441\n",
            "epoch 34; iter: 200; batch classifier loss: 0.390090; batch adversarial loss: 0.474086\n",
            "epoch 35; iter: 0; batch classifier loss: 0.478887; batch adversarial loss: 0.416719\n",
            "epoch 35; iter: 200; batch classifier loss: 0.415963; batch adversarial loss: 0.459339\n",
            "epoch 36; iter: 0; batch classifier loss: 0.428322; batch adversarial loss: 0.373449\n",
            "epoch 36; iter: 200; batch classifier loss: 0.423589; batch adversarial loss: 0.402875\n",
            "epoch 37; iter: 0; batch classifier loss: 0.406113; batch adversarial loss: 0.459679\n",
            "epoch 37; iter: 200; batch classifier loss: 0.423130; batch adversarial loss: 0.442234\n",
            "epoch 38; iter: 0; batch classifier loss: 0.399690; batch adversarial loss: 0.424055\n",
            "epoch 38; iter: 200; batch classifier loss: 0.424736; batch adversarial loss: 0.405511\n",
            "epoch 39; iter: 0; batch classifier loss: 0.403322; batch adversarial loss: 0.429842\n",
            "epoch 39; iter: 200; batch classifier loss: 0.467205; batch adversarial loss: 0.390120\n",
            "epoch 40; iter: 0; batch classifier loss: 0.361501; batch adversarial loss: 0.435784\n",
            "epoch 40; iter: 200; batch classifier loss: 0.422229; batch adversarial loss: 0.438061\n",
            "epoch 41; iter: 0; batch classifier loss: 0.453041; batch adversarial loss: 0.444571\n",
            "epoch 41; iter: 200; batch classifier loss: 0.405144; batch adversarial loss: 0.468009\n",
            "epoch 42; iter: 0; batch classifier loss: 0.390241; batch adversarial loss: 0.425425\n",
            "epoch 42; iter: 200; batch classifier loss: 0.371131; batch adversarial loss: 0.425398\n",
            "epoch 43; iter: 0; batch classifier loss: 0.355072; batch adversarial loss: 0.447251\n",
            "epoch 43; iter: 200; batch classifier loss: 0.448682; batch adversarial loss: 0.433697\n",
            "epoch 44; iter: 0; batch classifier loss: 0.475050; batch adversarial loss: 0.426512\n",
            "epoch 44; iter: 200; batch classifier loss: 0.313329; batch adversarial loss: 0.460185\n",
            "epoch 45; iter: 0; batch classifier loss: 0.404383; batch adversarial loss: 0.380773\n",
            "epoch 45; iter: 200; batch classifier loss: 0.349155; batch adversarial loss: 0.444921\n",
            "epoch 46; iter: 0; batch classifier loss: 0.339440; batch adversarial loss: 0.525699\n",
            "epoch 46; iter: 200; batch classifier loss: 0.454697; batch adversarial loss: 0.511762\n",
            "epoch 47; iter: 0; batch classifier loss: 0.428270; batch adversarial loss: 0.444819\n",
            "epoch 47; iter: 200; batch classifier loss: 0.534439; batch adversarial loss: 0.533004\n",
            "epoch 48; iter: 0; batch classifier loss: 0.372953; batch adversarial loss: 0.579180\n",
            "epoch 48; iter: 200; batch classifier loss: 0.339550; batch adversarial loss: 0.552723\n",
            "epoch 49; iter: 0; batch classifier loss: 0.405475; batch adversarial loss: 0.527196\n",
            "epoch 49; iter: 200; batch classifier loss: 0.460781; batch adversarial loss: 0.499801\n",
            "epoch 0; iter: 0; batch classifier loss: 0.668524; batch adversarial loss: 0.746265\n",
            "epoch 0; iter: 200; batch classifier loss: 0.471727; batch adversarial loss: 0.640846\n",
            "epoch 1; iter: 0; batch classifier loss: 0.444816; batch adversarial loss: 0.594548\n",
            "epoch 1; iter: 200; batch classifier loss: 0.359711; batch adversarial loss: 0.575251\n",
            "epoch 2; iter: 0; batch classifier loss: 0.379378; batch adversarial loss: 0.587739\n",
            "epoch 2; iter: 200; batch classifier loss: 0.423124; batch adversarial loss: 0.608924\n",
            "epoch 3; iter: 0; batch classifier loss: 0.421052; batch adversarial loss: 0.567430\n",
            "epoch 3; iter: 200; batch classifier loss: 0.395067; batch adversarial loss: 0.583162\n",
            "epoch 4; iter: 0; batch classifier loss: 0.397989; batch adversarial loss: 0.586251\n",
            "epoch 4; iter: 200; batch classifier loss: 0.463093; batch adversarial loss: 0.592617\n",
            "epoch 5; iter: 0; batch classifier loss: 0.327020; batch adversarial loss: 0.569980\n",
            "epoch 5; iter: 200; batch classifier loss: 0.384058; batch adversarial loss: 0.630021\n",
            "epoch 6; iter: 0; batch classifier loss: 0.396098; batch adversarial loss: 0.661806\n",
            "epoch 6; iter: 200; batch classifier loss: 0.440631; batch adversarial loss: 0.588176\n",
            "epoch 7; iter: 0; batch classifier loss: 0.418303; batch adversarial loss: 0.631300\n",
            "epoch 7; iter: 200; batch classifier loss: 0.458764; batch adversarial loss: 0.590066\n",
            "epoch 8; iter: 0; batch classifier loss: 0.410645; batch adversarial loss: 0.669075\n",
            "epoch 8; iter: 200; batch classifier loss: 0.469103; batch adversarial loss: 0.610692\n",
            "epoch 9; iter: 0; batch classifier loss: 0.477796; batch adversarial loss: 0.597275\n",
            "epoch 9; iter: 200; batch classifier loss: 0.438567; batch adversarial loss: 0.607639\n",
            "epoch 10; iter: 0; batch classifier loss: 0.420379; batch adversarial loss: 0.651383\n",
            "epoch 10; iter: 200; batch classifier loss: 0.479647; batch adversarial loss: 0.673760\n",
            "epoch 11; iter: 0; batch classifier loss: 0.392458; batch adversarial loss: 0.598490\n",
            "epoch 11; iter: 200; batch classifier loss: 0.530280; batch adversarial loss: 0.670020\n",
            "epoch 12; iter: 0; batch classifier loss: 0.418467; batch adversarial loss: 0.650745\n",
            "epoch 12; iter: 200; batch classifier loss: 0.341537; batch adversarial loss: 0.586399\n",
            "epoch 13; iter: 0; batch classifier loss: 0.454720; batch adversarial loss: 0.606188\n",
            "epoch 13; iter: 200; batch classifier loss: 0.420303; batch adversarial loss: 0.589261\n",
            "epoch 14; iter: 0; batch classifier loss: 0.416535; batch adversarial loss: 0.588097\n",
            "epoch 14; iter: 200; batch classifier loss: 0.406380; batch adversarial loss: 0.625681\n",
            "epoch 15; iter: 0; batch classifier loss: 0.363693; batch adversarial loss: 0.588627\n",
            "epoch 15; iter: 200; batch classifier loss: 0.399378; batch adversarial loss: 0.641495\n",
            "epoch 16; iter: 0; batch classifier loss: 0.474308; batch adversarial loss: 0.613837\n",
            "epoch 16; iter: 200; batch classifier loss: 0.515965; batch adversarial loss: 0.518860\n",
            "epoch 17; iter: 0; batch classifier loss: 0.355910; batch adversarial loss: 0.534413\n",
            "epoch 17; iter: 200; batch classifier loss: 0.412856; batch adversarial loss: 0.544525\n",
            "epoch 18; iter: 0; batch classifier loss: 0.408403; batch adversarial loss: 0.553558\n",
            "epoch 18; iter: 200; batch classifier loss: 0.408736; batch adversarial loss: 0.527819\n",
            "epoch 19; iter: 0; batch classifier loss: 0.549731; batch adversarial loss: 0.567300\n",
            "epoch 19; iter: 200; batch classifier loss: 0.370125; batch adversarial loss: 0.554707\n",
            "epoch 20; iter: 0; batch classifier loss: 0.466145; batch adversarial loss: 0.612073\n",
            "epoch 20; iter: 200; batch classifier loss: 0.450676; batch adversarial loss: 0.581615\n",
            "epoch 21; iter: 0; batch classifier loss: 0.395382; batch adversarial loss: 0.550427\n",
            "epoch 21; iter: 200; batch classifier loss: 0.455742; batch adversarial loss: 0.532361\n",
            "epoch 22; iter: 0; batch classifier loss: 0.385315; batch adversarial loss: 0.522368\n",
            "epoch 22; iter: 200; batch classifier loss: 0.384923; batch adversarial loss: 0.477759\n",
            "epoch 23; iter: 0; batch classifier loss: 0.408777; batch adversarial loss: 0.539604\n",
            "epoch 23; iter: 200; batch classifier loss: 0.422379; batch adversarial loss: 0.479774\n",
            "epoch 24; iter: 0; batch classifier loss: 0.443347; batch adversarial loss: 0.491670\n",
            "epoch 24; iter: 200; batch classifier loss: 0.435190; batch adversarial loss: 0.503539\n",
            "epoch 25; iter: 0; batch classifier loss: 0.373858; batch adversarial loss: 0.556809\n",
            "epoch 25; iter: 200; batch classifier loss: 0.436391; batch adversarial loss: 0.478831\n",
            "epoch 26; iter: 0; batch classifier loss: 0.357816; batch adversarial loss: 0.532377\n",
            "epoch 26; iter: 200; batch classifier loss: 0.474322; batch adversarial loss: 0.525461\n",
            "epoch 27; iter: 0; batch classifier loss: 0.499620; batch adversarial loss: 0.489028\n",
            "epoch 27; iter: 200; batch classifier loss: 0.362015; batch adversarial loss: 0.504165\n",
            "epoch 28; iter: 0; batch classifier loss: 0.425338; batch adversarial loss: 0.559640\n",
            "epoch 28; iter: 200; batch classifier loss: 0.362548; batch adversarial loss: 0.544277\n",
            "epoch 29; iter: 0; batch classifier loss: 0.489989; batch adversarial loss: 0.498955\n",
            "epoch 29; iter: 200; batch classifier loss: 0.343913; batch adversarial loss: 0.573619\n",
            "epoch 30; iter: 0; batch classifier loss: 0.311074; batch adversarial loss: 0.559254\n",
            "epoch 30; iter: 200; batch classifier loss: 0.470729; batch adversarial loss: 0.513670\n",
            "epoch 31; iter: 0; batch classifier loss: 0.410401; batch adversarial loss: 0.542792\n",
            "epoch 31; iter: 200; batch classifier loss: 0.483781; batch adversarial loss: 0.469339\n",
            "epoch 32; iter: 0; batch classifier loss: 0.388905; batch adversarial loss: 0.538871\n",
            "epoch 32; iter: 200; batch classifier loss: 0.374322; batch adversarial loss: 0.573604\n",
            "epoch 33; iter: 0; batch classifier loss: 0.466560; batch adversarial loss: 0.477266\n",
            "epoch 33; iter: 200; batch classifier loss: 0.385705; batch adversarial loss: 0.515615\n",
            "epoch 34; iter: 0; batch classifier loss: 0.405286; batch adversarial loss: 0.495620\n",
            "epoch 34; iter: 200; batch classifier loss: 0.477771; batch adversarial loss: 0.529186\n",
            "epoch 35; iter: 0; batch classifier loss: 0.433213; batch adversarial loss: 0.561016\n",
            "epoch 35; iter: 200; batch classifier loss: 0.415934; batch adversarial loss: 0.579718\n",
            "epoch 36; iter: 0; batch classifier loss: 0.464912; batch adversarial loss: 0.587026\n",
            "epoch 36; iter: 200; batch classifier loss: 0.460145; batch adversarial loss: 0.514882\n",
            "epoch 37; iter: 0; batch classifier loss: 0.500800; batch adversarial loss: 0.552622\n",
            "epoch 37; iter: 200; batch classifier loss: 0.513499; batch adversarial loss: 0.563161\n",
            "epoch 38; iter: 0; batch classifier loss: 0.463279; batch adversarial loss: 0.568443\n",
            "epoch 38; iter: 200; batch classifier loss: 0.418107; batch adversarial loss: 0.593147\n",
            "epoch 39; iter: 0; batch classifier loss: 0.377758; batch adversarial loss: 0.644872\n",
            "epoch 39; iter: 200; batch classifier loss: 0.343060; batch adversarial loss: 0.474015\n",
            "epoch 40; iter: 0; batch classifier loss: 0.428928; batch adversarial loss: 0.511800\n",
            "epoch 40; iter: 200; batch classifier loss: 0.447794; batch adversarial loss: 0.550237\n",
            "epoch 41; iter: 0; batch classifier loss: 0.458852; batch adversarial loss: 0.578915\n",
            "epoch 41; iter: 200; batch classifier loss: 0.398141; batch adversarial loss: 0.555610\n",
            "epoch 42; iter: 0; batch classifier loss: 0.397013; batch adversarial loss: 0.610973\n",
            "epoch 42; iter: 200; batch classifier loss: 0.485903; batch adversarial loss: 0.530824\n",
            "epoch 43; iter: 0; batch classifier loss: 0.297185; batch adversarial loss: 0.539133\n",
            "epoch 43; iter: 200; batch classifier loss: 0.405148; batch adversarial loss: 0.549214\n",
            "epoch 44; iter: 0; batch classifier loss: 0.396620; batch adversarial loss: 0.537894\n",
            "epoch 44; iter: 200; batch classifier loss: 0.361369; batch adversarial loss: 0.497895\n",
            "epoch 45; iter: 0; batch classifier loss: 0.482465; batch adversarial loss: 0.520675\n",
            "epoch 45; iter: 200; batch classifier loss: 0.476410; batch adversarial loss: 0.502915\n",
            "epoch 46; iter: 0; batch classifier loss: 0.457543; batch adversarial loss: 0.475640\n",
            "epoch 46; iter: 200; batch classifier loss: 0.432346; batch adversarial loss: 0.466554\n",
            "epoch 47; iter: 0; batch classifier loss: 0.378162; batch adversarial loss: 0.510470\n",
            "epoch 47; iter: 200; batch classifier loss: 0.423545; batch adversarial loss: 0.519224\n",
            "epoch 48; iter: 0; batch classifier loss: 0.413850; batch adversarial loss: 0.603272\n",
            "epoch 48; iter: 200; batch classifier loss: 0.363230; batch adversarial loss: 0.435554\n",
            "epoch 49; iter: 0; batch classifier loss: 0.405151; batch adversarial loss: 0.572948\n",
            "epoch 49; iter: 200; batch classifier loss: 0.431900; batch adversarial loss: 0.494983\n",
            "epoch 0; iter: 0; batch classifier loss: 0.721711; batch adversarial loss: 0.717746\n",
            "epoch 0; iter: 200; batch classifier loss: 0.431281; batch adversarial loss: 0.666675\n",
            "epoch 1; iter: 0; batch classifier loss: 0.461941; batch adversarial loss: 0.659905\n",
            "epoch 1; iter: 200; batch classifier loss: 0.432308; batch adversarial loss: 0.633533\n",
            "epoch 2; iter: 0; batch classifier loss: 0.465735; batch adversarial loss: 0.637107\n",
            "epoch 2; iter: 200; batch classifier loss: 0.579427; batch adversarial loss: 0.672505\n",
            "epoch 3; iter: 0; batch classifier loss: 0.530829; batch adversarial loss: 0.617820\n",
            "epoch 3; iter: 200; batch classifier loss: 0.479867; batch adversarial loss: 0.651040\n",
            "epoch 4; iter: 0; batch classifier loss: 0.455523; batch adversarial loss: 0.645477\n",
            "epoch 4; iter: 200; batch classifier loss: 0.485655; batch adversarial loss: 0.634596\n",
            "epoch 5; iter: 0; batch classifier loss: 0.556076; batch adversarial loss: 0.632448\n",
            "epoch 5; iter: 200; batch classifier loss: 0.457244; batch adversarial loss: 0.611621\n",
            "epoch 6; iter: 0; batch classifier loss: 0.536685; batch adversarial loss: 0.647067\n",
            "epoch 6; iter: 200; batch classifier loss: 0.485990; batch adversarial loss: 0.637460\n",
            "epoch 7; iter: 0; batch classifier loss: 0.556590; batch adversarial loss: 0.623394\n",
            "epoch 7; iter: 200; batch classifier loss: 0.488507; batch adversarial loss: 0.652081\n",
            "epoch 8; iter: 0; batch classifier loss: 0.592654; batch adversarial loss: 0.614682\n",
            "epoch 8; iter: 200; batch classifier loss: 0.553684; batch adversarial loss: 0.567204\n",
            "epoch 9; iter: 0; batch classifier loss: 0.488734; batch adversarial loss: 0.576763\n",
            "epoch 9; iter: 200; batch classifier loss: 0.450351; batch adversarial loss: 0.572307\n",
            "epoch 10; iter: 0; batch classifier loss: 0.434753; batch adversarial loss: 0.705466\n",
            "epoch 10; iter: 200; batch classifier loss: 0.454995; batch adversarial loss: 0.597164\n",
            "epoch 11; iter: 0; batch classifier loss: 0.367237; batch adversarial loss: 0.626191\n",
            "epoch 11; iter: 200; batch classifier loss: 0.476548; batch adversarial loss: 0.606955\n",
            "epoch 12; iter: 0; batch classifier loss: 0.461156; batch adversarial loss: 0.600344\n",
            "epoch 12; iter: 200; batch classifier loss: 0.325274; batch adversarial loss: 0.603755\n",
            "epoch 13; iter: 0; batch classifier loss: 0.362939; batch adversarial loss: 0.676755\n",
            "epoch 13; iter: 200; batch classifier loss: 0.401465; batch adversarial loss: 0.612065\n",
            "epoch 14; iter: 0; batch classifier loss: 0.450958; batch adversarial loss: 0.626019\n",
            "epoch 14; iter: 200; batch classifier loss: 0.452289; batch adversarial loss: 0.567266\n",
            "epoch 15; iter: 0; batch classifier loss: 0.370000; batch adversarial loss: 0.621221\n",
            "epoch 15; iter: 200; batch classifier loss: 0.476730; batch adversarial loss: 0.616844\n",
            "epoch 16; iter: 0; batch classifier loss: 0.419306; batch adversarial loss: 0.645918\n",
            "epoch 16; iter: 200; batch classifier loss: 0.492231; batch adversarial loss: 0.671225\n",
            "epoch 17; iter: 0; batch classifier loss: 0.506277; batch adversarial loss: 0.616725\n",
            "epoch 17; iter: 200; batch classifier loss: 0.366126; batch adversarial loss: 0.708067\n",
            "epoch 18; iter: 0; batch classifier loss: 0.518069; batch adversarial loss: 0.613296\n",
            "epoch 18; iter: 200; batch classifier loss: 0.454105; batch adversarial loss: 0.633213\n",
            "epoch 19; iter: 0; batch classifier loss: 0.566036; batch adversarial loss: 0.604390\n",
            "epoch 19; iter: 200; batch classifier loss: 0.451050; batch adversarial loss: 0.673325\n",
            "epoch 20; iter: 0; batch classifier loss: 0.390738; batch adversarial loss: 0.670920\n",
            "epoch 20; iter: 200; batch classifier loss: 0.447530; batch adversarial loss: 0.620638\n",
            "epoch 21; iter: 0; batch classifier loss: 0.466512; batch adversarial loss: 0.616570\n",
            "epoch 21; iter: 200; batch classifier loss: 0.463673; batch adversarial loss: 0.664956\n",
            "epoch 22; iter: 0; batch classifier loss: 0.385770; batch adversarial loss: 0.671682\n",
            "epoch 22; iter: 200; batch classifier loss: 0.423427; batch adversarial loss: 0.588396\n",
            "epoch 23; iter: 0; batch classifier loss: 0.535560; batch adversarial loss: 0.605637\n",
            "epoch 23; iter: 200; batch classifier loss: 0.464691; batch adversarial loss: 0.646038\n",
            "epoch 24; iter: 0; batch classifier loss: 0.431030; batch adversarial loss: 0.640246\n",
            "epoch 24; iter: 200; batch classifier loss: 0.427453; batch adversarial loss: 0.669544\n",
            "epoch 25; iter: 0; batch classifier loss: 0.454768; batch adversarial loss: 0.642833\n",
            "epoch 25; iter: 200; batch classifier loss: 0.493493; batch adversarial loss: 0.629604\n",
            "epoch 26; iter: 0; batch classifier loss: 0.414724; batch adversarial loss: 0.562340\n",
            "epoch 26; iter: 200; batch classifier loss: 0.454546; batch adversarial loss: 0.623019\n",
            "epoch 27; iter: 0; batch classifier loss: 0.500510; batch adversarial loss: 0.605749\n",
            "epoch 27; iter: 200; batch classifier loss: 0.467976; batch adversarial loss: 0.621237\n",
            "epoch 28; iter: 0; batch classifier loss: 0.436422; batch adversarial loss: 0.639054\n",
            "epoch 28; iter: 200; batch classifier loss: 0.539791; batch adversarial loss: 0.595966\n",
            "epoch 29; iter: 0; batch classifier loss: 0.453238; batch adversarial loss: 0.591617\n",
            "epoch 29; iter: 200; batch classifier loss: 0.397374; batch adversarial loss: 0.618876\n",
            "epoch 30; iter: 0; batch classifier loss: 0.439806; batch adversarial loss: 0.629478\n",
            "epoch 30; iter: 200; batch classifier loss: 0.427413; batch adversarial loss: 0.647351\n",
            "epoch 31; iter: 0; batch classifier loss: 0.486166; batch adversarial loss: 0.633135\n",
            "epoch 31; iter: 200; batch classifier loss: 0.506740; batch adversarial loss: 0.689616\n",
            "epoch 32; iter: 0; batch classifier loss: 0.485047; batch adversarial loss: 0.638256\n",
            "epoch 32; iter: 200; batch classifier loss: 0.463403; batch adversarial loss: 0.653033\n",
            "epoch 33; iter: 0; batch classifier loss: 0.451906; batch adversarial loss: 0.633244\n",
            "epoch 33; iter: 200; batch classifier loss: 0.416198; batch adversarial loss: 0.603096\n",
            "epoch 34; iter: 0; batch classifier loss: 0.469868; batch adversarial loss: 0.655695\n",
            "epoch 34; iter: 200; batch classifier loss: 0.470173; batch adversarial loss: 0.596388\n",
            "epoch 35; iter: 0; batch classifier loss: 0.526720; batch adversarial loss: 0.595155\n",
            "epoch 35; iter: 200; batch classifier loss: 0.518157; batch adversarial loss: 0.562520\n",
            "epoch 36; iter: 0; batch classifier loss: 0.428918; batch adversarial loss: 0.620554\n",
            "epoch 36; iter: 200; batch classifier loss: 0.545271; batch adversarial loss: 0.627425\n",
            "epoch 37; iter: 0; batch classifier loss: 0.445456; batch adversarial loss: 0.596230\n",
            "epoch 37; iter: 200; batch classifier loss: 0.525420; batch adversarial loss: 0.645624\n",
            "epoch 38; iter: 0; batch classifier loss: 0.442196; batch adversarial loss: 0.685933\n",
            "epoch 38; iter: 200; batch classifier loss: 0.443527; batch adversarial loss: 0.595606\n",
            "epoch 39; iter: 0; batch classifier loss: 0.547488; batch adversarial loss: 0.650020\n",
            "epoch 39; iter: 200; batch classifier loss: 0.519996; batch adversarial loss: 0.651624\n",
            "epoch 40; iter: 0; batch classifier loss: 0.466947; batch adversarial loss: 0.641582\n",
            "epoch 40; iter: 200; batch classifier loss: 0.489839; batch adversarial loss: 0.659869\n",
            "epoch 41; iter: 0; batch classifier loss: 0.369649; batch adversarial loss: 0.614452\n",
            "epoch 41; iter: 200; batch classifier loss: 0.420782; batch adversarial loss: 0.646047\n",
            "epoch 42; iter: 0; batch classifier loss: 0.469952; batch adversarial loss: 0.646455\n",
            "epoch 42; iter: 200; batch classifier loss: 0.542637; batch adversarial loss: 0.642541\n",
            "epoch 43; iter: 0; batch classifier loss: 0.407459; batch adversarial loss: 0.609697\n",
            "epoch 43; iter: 200; batch classifier loss: 0.477845; batch adversarial loss: 0.652101\n",
            "epoch 44; iter: 0; batch classifier loss: 0.457058; batch adversarial loss: 0.655883\n",
            "epoch 44; iter: 200; batch classifier loss: 0.502952; batch adversarial loss: 0.639940\n",
            "epoch 45; iter: 0; batch classifier loss: 0.459100; batch adversarial loss: 0.669393\n",
            "epoch 45; iter: 200; batch classifier loss: 0.511858; batch adversarial loss: 0.580862\n",
            "epoch 46; iter: 0; batch classifier loss: 0.435441; batch adversarial loss: 0.552433\n",
            "epoch 46; iter: 200; batch classifier loss: 0.601685; batch adversarial loss: 0.672476\n",
            "epoch 47; iter: 0; batch classifier loss: 0.432022; batch adversarial loss: 0.606308\n",
            "epoch 47; iter: 200; batch classifier loss: 0.432103; batch adversarial loss: 0.605294\n",
            "epoch 48; iter: 0; batch classifier loss: 0.480874; batch adversarial loss: 0.591427\n",
            "epoch 48; iter: 200; batch classifier loss: 0.689583; batch adversarial loss: 0.651750\n",
            "epoch 49; iter: 0; batch classifier loss: 0.425371; batch adversarial loss: 0.566944\n",
            "epoch 49; iter: 200; batch classifier loss: 0.392617; batch adversarial loss: 0.607673\n",
            "epoch 0; iter: 0; batch classifier loss: 0.782268; batch adversarial loss: 0.702311\n",
            "epoch 0; iter: 200; batch classifier loss: 1.497566; batch adversarial loss: 0.698409\n",
            "epoch 1; iter: 0; batch classifier loss: 1.234118; batch adversarial loss: 0.680187\n",
            "epoch 1; iter: 200; batch classifier loss: 0.485599; batch adversarial loss: 0.673794\n",
            "epoch 2; iter: 0; batch classifier loss: 0.648013; batch adversarial loss: 0.650258\n",
            "epoch 2; iter: 200; batch classifier loss: 0.457496; batch adversarial loss: 0.645638\n",
            "epoch 3; iter: 0; batch classifier loss: 0.749765; batch adversarial loss: 0.632894\n",
            "epoch 3; iter: 200; batch classifier loss: 0.673878; batch adversarial loss: 0.606640\n",
            "epoch 4; iter: 0; batch classifier loss: 0.616389; batch adversarial loss: 0.657251\n",
            "epoch 4; iter: 200; batch classifier loss: 0.588925; batch adversarial loss: 0.643237\n",
            "epoch 5; iter: 0; batch classifier loss: 0.712809; batch adversarial loss: 0.633172\n",
            "epoch 5; iter: 200; batch classifier loss: 0.535559; batch adversarial loss: 0.653486\n",
            "epoch 6; iter: 0; batch classifier loss: 0.537787; batch adversarial loss: 0.643141\n",
            "epoch 6; iter: 200; batch classifier loss: 0.470835; batch adversarial loss: 0.606355\n",
            "epoch 7; iter: 0; batch classifier loss: 0.466518; batch adversarial loss: 0.622499\n",
            "epoch 7; iter: 200; batch classifier loss: 0.624464; batch adversarial loss: 0.573747\n",
            "epoch 8; iter: 0; batch classifier loss: 0.772101; batch adversarial loss: 0.618691\n",
            "epoch 8; iter: 200; batch classifier loss: 0.625262; batch adversarial loss: 0.635104\n",
            "epoch 9; iter: 0; batch classifier loss: 0.593521; batch adversarial loss: 0.650079\n",
            "epoch 9; iter: 200; batch classifier loss: 0.725185; batch adversarial loss: 0.641499\n",
            "epoch 10; iter: 0; batch classifier loss: 0.770590; batch adversarial loss: 0.652066\n",
            "epoch 10; iter: 200; batch classifier loss: 0.590517; batch adversarial loss: 0.664697\n",
            "epoch 11; iter: 0; batch classifier loss: 0.654992; batch adversarial loss: 0.650870\n",
            "epoch 11; iter: 200; batch classifier loss: 0.758583; batch adversarial loss: 0.563945\n",
            "epoch 12; iter: 0; batch classifier loss: 0.779813; batch adversarial loss: 0.641567\n",
            "epoch 12; iter: 200; batch classifier loss: 0.639813; batch adversarial loss: 0.605500\n",
            "epoch 13; iter: 0; batch classifier loss: 0.482210; batch adversarial loss: 0.646353\n",
            "epoch 13; iter: 200; batch classifier loss: 0.539024; batch adversarial loss: 0.654806\n",
            "epoch 14; iter: 0; batch classifier loss: 0.645555; batch adversarial loss: 0.569233\n",
            "epoch 14; iter: 200; batch classifier loss: 0.697905; batch adversarial loss: 0.664799\n",
            "epoch 15; iter: 0; batch classifier loss: 0.580577; batch adversarial loss: 0.615604\n",
            "epoch 15; iter: 200; batch classifier loss: 0.553747; batch adversarial loss: 0.648519\n",
            "epoch 16; iter: 0; batch classifier loss: 0.873010; batch adversarial loss: 0.695129\n",
            "epoch 16; iter: 200; batch classifier loss: 0.607329; batch adversarial loss: 0.641890\n",
            "epoch 17; iter: 0; batch classifier loss: 0.689596; batch adversarial loss: 0.637838\n",
            "epoch 17; iter: 200; batch classifier loss: 0.661359; batch adversarial loss: 0.637773\n",
            "epoch 18; iter: 0; batch classifier loss: 0.547657; batch adversarial loss: 0.597320\n",
            "epoch 18; iter: 200; batch classifier loss: 0.885154; batch adversarial loss: 0.586177\n",
            "epoch 19; iter: 0; batch classifier loss: 0.650301; batch adversarial loss: 0.704429\n",
            "epoch 19; iter: 200; batch classifier loss: 0.580215; batch adversarial loss: 0.625055\n",
            "epoch 20; iter: 0; batch classifier loss: 0.744559; batch adversarial loss: 0.637933\n",
            "epoch 20; iter: 200; batch classifier loss: 0.437812; batch adversarial loss: 0.685807\n",
            "epoch 21; iter: 0; batch classifier loss: 0.588103; batch adversarial loss: 0.650852\n",
            "epoch 21; iter: 200; batch classifier loss: 0.828381; batch adversarial loss: 0.605314\n",
            "epoch 22; iter: 0; batch classifier loss: 0.797198; batch adversarial loss: 0.581324\n",
            "epoch 22; iter: 200; batch classifier loss: 0.679814; batch adversarial loss: 0.599204\n",
            "epoch 23; iter: 0; batch classifier loss: 0.748356; batch adversarial loss: 0.616917\n",
            "epoch 23; iter: 200; batch classifier loss: 0.594037; batch adversarial loss: 0.605944\n",
            "epoch 24; iter: 0; batch classifier loss: 0.588236; batch adversarial loss: 0.619100\n",
            "epoch 24; iter: 200; batch classifier loss: 0.588229; batch adversarial loss: 0.648387\n",
            "epoch 25; iter: 0; batch classifier loss: 0.647188; batch adversarial loss: 0.595287\n",
            "epoch 25; iter: 200; batch classifier loss: 0.522204; batch adversarial loss: 0.704922\n",
            "epoch 26; iter: 0; batch classifier loss: 0.637673; batch adversarial loss: 0.657218\n",
            "epoch 26; iter: 200; batch classifier loss: 0.661407; batch adversarial loss: 0.671257\n",
            "epoch 27; iter: 0; batch classifier loss: 0.895581; batch adversarial loss: 0.586654\n",
            "epoch 27; iter: 200; batch classifier loss: 0.630331; batch adversarial loss: 0.620144\n",
            "epoch 28; iter: 0; batch classifier loss: 0.556731; batch adversarial loss: 0.638084\n",
            "epoch 28; iter: 200; batch classifier loss: 0.799796; batch adversarial loss: 0.617482\n",
            "epoch 29; iter: 0; batch classifier loss: 0.690711; batch adversarial loss: 0.655043\n",
            "epoch 29; iter: 200; batch classifier loss: 0.626652; batch adversarial loss: 0.631979\n",
            "epoch 30; iter: 0; batch classifier loss: 0.692698; batch adversarial loss: 0.637102\n",
            "epoch 30; iter: 200; batch classifier loss: 0.726700; batch adversarial loss: 0.646257\n",
            "epoch 31; iter: 0; batch classifier loss: 0.848286; batch adversarial loss: 0.649434\n",
            "epoch 31; iter: 200; batch classifier loss: 0.647692; batch adversarial loss: 0.706097\n",
            "epoch 32; iter: 0; batch classifier loss: 0.599372; batch adversarial loss: 0.654655\n",
            "epoch 32; iter: 200; batch classifier loss: 0.614506; batch adversarial loss: 0.637156\n",
            "epoch 33; iter: 0; batch classifier loss: 0.424799; batch adversarial loss: 0.656572\n",
            "epoch 33; iter: 200; batch classifier loss: 0.696676; batch adversarial loss: 0.603765\n",
            "epoch 34; iter: 0; batch classifier loss: 0.787008; batch adversarial loss: 0.601051\n",
            "epoch 34; iter: 200; batch classifier loss: 0.515918; batch adversarial loss: 0.628240\n",
            "epoch 35; iter: 0; batch classifier loss: 0.619566; batch adversarial loss: 0.636038\n",
            "epoch 35; iter: 200; batch classifier loss: 0.690686; batch adversarial loss: 0.616106\n",
            "epoch 36; iter: 0; batch classifier loss: 0.745170; batch adversarial loss: 0.584118\n",
            "epoch 36; iter: 200; batch classifier loss: 0.653639; batch adversarial loss: 0.626060\n",
            "epoch 37; iter: 0; batch classifier loss: 0.594088; batch adversarial loss: 0.622681\n",
            "epoch 37; iter: 200; batch classifier loss: 0.854997; batch adversarial loss: 0.586951\n",
            "epoch 38; iter: 0; batch classifier loss: 0.766897; batch adversarial loss: 0.618940\n",
            "epoch 38; iter: 200; batch classifier loss: 0.582551; batch adversarial loss: 0.658420\n",
            "epoch 39; iter: 0; batch classifier loss: 0.642510; batch adversarial loss: 0.599080\n",
            "epoch 39; iter: 200; batch classifier loss: 0.896995; batch adversarial loss: 0.683649\n",
            "epoch 40; iter: 0; batch classifier loss: 0.597586; batch adversarial loss: 0.642948\n",
            "epoch 40; iter: 200; batch classifier loss: 0.686935; batch adversarial loss: 0.642518\n",
            "epoch 41; iter: 0; batch classifier loss: 0.532366; batch adversarial loss: 0.632571\n",
            "epoch 41; iter: 200; batch classifier loss: 0.795066; batch adversarial loss: 0.626961\n",
            "epoch 42; iter: 0; batch classifier loss: 0.600352; batch adversarial loss: 0.650905\n",
            "epoch 42; iter: 200; batch classifier loss: 0.619513; batch adversarial loss: 0.646987\n",
            "epoch 43; iter: 0; batch classifier loss: 0.441175; batch adversarial loss: 0.615611\n",
            "epoch 43; iter: 200; batch classifier loss: 0.725247; batch adversarial loss: 0.629668\n",
            "epoch 44; iter: 0; batch classifier loss: 0.546199; batch adversarial loss: 0.622686\n",
            "epoch 44; iter: 200; batch classifier loss: 0.477821; batch adversarial loss: 0.654412\n",
            "epoch 45; iter: 0; batch classifier loss: 0.518791; batch adversarial loss: 0.656379\n",
            "epoch 45; iter: 200; batch classifier loss: 0.673293; batch adversarial loss: 0.694900\n",
            "epoch 46; iter: 0; batch classifier loss: 0.527369; batch adversarial loss: 0.646716\n",
            "epoch 46; iter: 200; batch classifier loss: 0.675978; batch adversarial loss: 0.640922\n",
            "epoch 47; iter: 0; batch classifier loss: 0.746873; batch adversarial loss: 0.644167\n",
            "epoch 47; iter: 200; batch classifier loss: 0.533551; batch adversarial loss: 0.571743\n",
            "epoch 48; iter: 0; batch classifier loss: 0.619238; batch adversarial loss: 0.661929\n",
            "epoch 48; iter: 200; batch classifier loss: 0.780835; batch adversarial loss: 0.597108\n",
            "epoch 49; iter: 0; batch classifier loss: 0.848755; batch adversarial loss: 0.642672\n",
            "epoch 49; iter: 200; batch classifier loss: 0.648734; batch adversarial loss: 0.628296\n",
            "epoch 0; iter: 0; batch classifier loss: 0.701770; batch adversarial loss: 0.676248\n",
            "epoch 0; iter: 200; batch classifier loss: 1.083345; batch adversarial loss: 0.668833\n",
            "epoch 1; iter: 0; batch classifier loss: 2.963174; batch adversarial loss: 0.683727\n",
            "epoch 1; iter: 200; batch classifier loss: 0.950685; batch adversarial loss: 0.631799\n",
            "epoch 2; iter: 0; batch classifier loss: 1.087582; batch adversarial loss: 0.647767\n",
            "epoch 2; iter: 200; batch classifier loss: 1.508472; batch adversarial loss: 0.657537\n",
            "epoch 3; iter: 0; batch classifier loss: 1.781302; batch adversarial loss: 0.645027\n",
            "epoch 3; iter: 200; batch classifier loss: 1.593218; batch adversarial loss: 0.615704\n",
            "epoch 4; iter: 0; batch classifier loss: 1.716949; batch adversarial loss: 0.619780\n",
            "epoch 4; iter: 200; batch classifier loss: 1.447669; batch adversarial loss: 0.668783\n",
            "epoch 5; iter: 0; batch classifier loss: 1.708812; batch adversarial loss: 0.689537\n",
            "epoch 5; iter: 200; batch classifier loss: 2.344434; batch adversarial loss: 0.601212\n",
            "epoch 6; iter: 0; batch classifier loss: 1.749165; batch adversarial loss: 0.643621\n",
            "epoch 6; iter: 200; batch classifier loss: 1.521514; batch adversarial loss: 0.681404\n",
            "epoch 7; iter: 0; batch classifier loss: 1.848060; batch adversarial loss: 0.622086\n",
            "epoch 7; iter: 200; batch classifier loss: 1.894361; batch adversarial loss: 0.671252\n",
            "epoch 8; iter: 0; batch classifier loss: 1.296679; batch adversarial loss: 0.649199\n",
            "epoch 8; iter: 200; batch classifier loss: 1.697105; batch adversarial loss: 0.632873\n",
            "epoch 9; iter: 0; batch classifier loss: 2.085938; batch adversarial loss: 0.649225\n",
            "epoch 9; iter: 200; batch classifier loss: 1.767493; batch adversarial loss: 0.649239\n",
            "epoch 10; iter: 0; batch classifier loss: 1.406690; batch adversarial loss: 0.600098\n",
            "epoch 10; iter: 200; batch classifier loss: 1.974898; batch adversarial loss: 0.600068\n",
            "epoch 11; iter: 0; batch classifier loss: 2.034602; batch adversarial loss: 0.660139\n",
            "epoch 11; iter: 200; batch classifier loss: 1.425089; batch adversarial loss: 0.660099\n",
            "epoch 12; iter: 0; batch classifier loss: 1.418667; batch adversarial loss: 0.611089\n",
            "epoch 12; iter: 200; batch classifier loss: 1.911605; batch adversarial loss: 0.643794\n",
            "epoch 13; iter: 0; batch classifier loss: 1.835329; batch adversarial loss: 0.605606\n",
            "epoch 13; iter: 200; batch classifier loss: 1.558140; batch adversarial loss: 0.627437\n",
            "epoch 14; iter: 0; batch classifier loss: 1.411858; batch adversarial loss: 0.681803\n",
            "epoch 14; iter: 200; batch classifier loss: 1.292152; batch adversarial loss: 0.627416\n",
            "epoch 15; iter: 0; batch classifier loss: 1.678604; batch adversarial loss: 0.616472\n",
            "epoch 15; iter: 200; batch classifier loss: 1.167922; batch adversarial loss: 0.643850\n",
            "epoch 16; iter: 0; batch classifier loss: 1.138216; batch adversarial loss: 0.632875\n",
            "epoch 16; iter: 200; batch classifier loss: 1.054203; batch adversarial loss: 0.638321\n",
            "epoch 17; iter: 0; batch classifier loss: 0.970098; batch adversarial loss: 0.693053\n",
            "epoch 17; iter: 200; batch classifier loss: 0.732393; batch adversarial loss: 0.627750\n",
            "epoch 18; iter: 0; batch classifier loss: 0.592262; batch adversarial loss: 0.581123\n",
            "epoch 18; iter: 200; batch classifier loss: 1.560123; batch adversarial loss: 0.605791\n",
            "epoch 19; iter: 0; batch classifier loss: 1.548658; batch adversarial loss: 0.643794\n",
            "epoch 19; iter: 200; batch classifier loss: 1.742998; batch adversarial loss: 0.605760\n",
            "epoch 20; iter: 0; batch classifier loss: 1.300808; batch adversarial loss: 0.671232\n",
            "epoch 20; iter: 200; batch classifier loss: 1.270905; batch adversarial loss: 0.654924\n",
            "epoch 21; iter: 0; batch classifier loss: 1.314010; batch adversarial loss: 0.583553\n",
            "epoch 21; iter: 200; batch classifier loss: 0.887614; batch adversarial loss: 0.654781\n",
            "epoch 22; iter: 0; batch classifier loss: 1.133610; batch adversarial loss: 0.654671\n",
            "epoch 22; iter: 200; batch classifier loss: 1.759818; batch adversarial loss: 0.638318\n",
            "epoch 23; iter: 0; batch classifier loss: 2.287212; batch adversarial loss: 0.594433\n",
            "epoch 23; iter: 200; batch classifier loss: 2.865589; batch adversarial loss: 0.627350\n",
            "epoch 24; iter: 0; batch classifier loss: 2.163935; batch adversarial loss: 0.682437\n",
            "epoch 24; iter: 200; batch classifier loss: 2.127856; batch adversarial loss: 0.654741\n",
            "epoch 25; iter: 0; batch classifier loss: 2.487943; batch adversarial loss: 0.589007\n",
            "epoch 25; iter: 200; batch classifier loss: 2.382350; batch adversarial loss: 0.611055\n",
            "epoch 26; iter: 0; batch classifier loss: 1.610099; batch adversarial loss: 0.660195\n",
            "epoch 26; iter: 200; batch classifier loss: 1.963712; batch adversarial loss: 0.605575\n",
            "epoch 27; iter: 0; batch classifier loss: 1.645414; batch adversarial loss: 0.654765\n",
            "epoch 27; iter: 200; batch classifier loss: 1.958546; batch adversarial loss: 0.638333\n",
            "epoch 28; iter: 0; batch classifier loss: 2.357671; batch adversarial loss: 0.621865\n",
            "epoch 28; iter: 200; batch classifier loss: 1.479861; batch adversarial loss: 0.643833\n",
            "epoch 29; iter: 0; batch classifier loss: 1.262421; batch adversarial loss: 0.643818\n",
            "epoch 29; iter: 200; batch classifier loss: 1.406758; batch adversarial loss: 0.638326\n",
            "epoch 30; iter: 0; batch classifier loss: 1.303196; batch adversarial loss: 0.660204\n",
            "epoch 30; iter: 200; batch classifier loss: 1.032258; batch adversarial loss: 0.654835\n",
            "epoch 31; iter: 0; batch classifier loss: 0.786592; batch adversarial loss: 0.616406\n",
            "epoch 31; iter: 200; batch classifier loss: 0.732376; batch adversarial loss: 0.676412\n",
            "epoch 32; iter: 0; batch classifier loss: 1.669462; batch adversarial loss: 0.676423\n",
            "epoch 32; iter: 200; batch classifier loss: 1.107207; batch adversarial loss: 0.589414\n",
            "epoch 33; iter: 0; batch classifier loss: 1.496665; batch adversarial loss: 0.649275\n",
            "epoch 33; iter: 200; batch classifier loss: 1.452308; batch adversarial loss: 0.627396\n",
            "epoch 34; iter: 0; batch classifier loss: 1.850853; batch adversarial loss: 0.605677\n",
            "epoch 34; iter: 200; batch classifier loss: 1.348696; batch adversarial loss: 0.638317\n",
            "epoch 35; iter: 0; batch classifier loss: 1.384910; batch adversarial loss: 0.676369\n",
            "epoch 35; iter: 200; batch classifier loss: 1.535995; batch adversarial loss: 0.654629\n",
            "epoch 36; iter: 0; batch classifier loss: 1.398708; batch adversarial loss: 0.589235\n",
            "epoch 36; iter: 200; batch classifier loss: 1.747455; batch adversarial loss: 0.632855\n",
            "epoch 37; iter: 0; batch classifier loss: 1.737089; batch adversarial loss: 0.671055\n",
            "epoch 37; iter: 200; batch classifier loss: 1.303575; batch adversarial loss: 0.731102\n",
            "epoch 38; iter: 0; batch classifier loss: 1.680007; batch adversarial loss: 0.649264\n",
            "epoch 38; iter: 200; batch classifier loss: 1.359521; batch adversarial loss: 0.698462\n",
            "epoch 39; iter: 0; batch classifier loss: 1.216951; batch adversarial loss: 0.709544\n",
            "epoch 39; iter: 200; batch classifier loss: 1.987130; batch adversarial loss: 0.632860\n",
            "epoch 40; iter: 0; batch classifier loss: 2.474267; batch adversarial loss: 0.654696\n",
            "epoch 40; iter: 200; batch classifier loss: 2.500666; batch adversarial loss: 0.583995\n",
            "epoch 41; iter: 0; batch classifier loss: 1.970751; batch adversarial loss: 0.665827\n",
            "epoch 41; iter: 200; batch classifier loss: 2.204568; batch adversarial loss: 0.638343\n",
            "epoch 42; iter: 0; batch classifier loss: 1.093325; batch adversarial loss: 0.671251\n",
            "epoch 42; iter: 200; batch classifier loss: 1.905176; batch adversarial loss: 0.605570\n",
            "epoch 43; iter: 0; batch classifier loss: 1.509194; batch adversarial loss: 0.649251\n",
            "epoch 43; iter: 200; batch classifier loss: 1.581529; batch adversarial loss: 0.638338\n",
            "epoch 44; iter: 0; batch classifier loss: 1.801276; batch adversarial loss: 0.583614\n",
            "epoch 44; iter: 200; batch classifier loss: 1.176991; batch adversarial loss: 0.643856\n",
            "epoch 45; iter: 0; batch classifier loss: 1.782536; batch adversarial loss: 0.692824\n",
            "epoch 45; iter: 200; batch classifier loss: 1.606301; batch adversarial loss: 0.632864\n",
            "epoch 46; iter: 0; batch classifier loss: 1.596714; batch adversarial loss: 0.654623\n",
            "epoch 46; iter: 200; batch classifier loss: 1.456750; batch adversarial loss: 0.638349\n",
            "epoch 47; iter: 0; batch classifier loss: 1.103300; batch adversarial loss: 0.670963\n",
            "epoch 47; iter: 200; batch classifier loss: 1.397909; batch adversarial loss: 0.632891\n",
            "epoch 48; iter: 0; batch classifier loss: 1.667253; batch adversarial loss: 0.632872\n",
            "epoch 48; iter: 200; batch classifier loss: 1.260354; batch adversarial loss: 0.649242\n",
            "epoch 49; iter: 0; batch classifier loss: 1.407553; batch adversarial loss: 0.643800\n",
            "epoch 49; iter: 200; batch classifier loss: 1.226622; batch adversarial loss: 0.660159\n",
            "\n",
            " Welcome to Fold number 4 \n",
            "\n",
            "epoch 0; iter: 0; batch classifier loss: 0.750916; batch adversarial loss: 0.776690\n",
            "epoch 0; iter: 200; batch classifier loss: 0.478753; batch adversarial loss: 0.693874\n",
            "epoch 1; iter: 0; batch classifier loss: 0.518449; batch adversarial loss: 0.694616\n",
            "epoch 1; iter: 200; batch classifier loss: 0.534995; batch adversarial loss: 0.623474\n",
            "epoch 2; iter: 0; batch classifier loss: 0.471493; batch adversarial loss: 0.658416\n",
            "epoch 2; iter: 200; batch classifier loss: 0.469366; batch adversarial loss: 0.656257\n",
            "epoch 3; iter: 0; batch classifier loss: 0.460405; batch adversarial loss: 0.635600\n",
            "epoch 3; iter: 200; batch classifier loss: 0.474756; batch adversarial loss: 0.577490\n",
            "epoch 4; iter: 0; batch classifier loss: 0.390403; batch adversarial loss: 0.633494\n",
            "epoch 4; iter: 200; batch classifier loss: 0.475775; batch adversarial loss: 0.652604\n",
            "epoch 5; iter: 0; batch classifier loss: 0.397460; batch adversarial loss: 0.614539\n",
            "epoch 5; iter: 200; batch classifier loss: 0.388164; batch adversarial loss: 0.596734\n",
            "epoch 6; iter: 0; batch classifier loss: 0.404870; batch adversarial loss: 0.631688\n",
            "epoch 6; iter: 200; batch classifier loss: 0.467577; batch adversarial loss: 0.628806\n",
            "epoch 7; iter: 0; batch classifier loss: 0.478949; batch adversarial loss: 0.597505\n",
            "epoch 7; iter: 200; batch classifier loss: 0.436282; batch adversarial loss: 0.591742\n",
            "epoch 8; iter: 0; batch classifier loss: 0.450146; batch adversarial loss: 0.606163\n",
            "epoch 8; iter: 200; batch classifier loss: 0.399900; batch adversarial loss: 0.603677\n",
            "epoch 9; iter: 0; batch classifier loss: 0.446822; batch adversarial loss: 0.587866\n",
            "epoch 9; iter: 200; batch classifier loss: 0.411808; batch adversarial loss: 0.595614\n",
            "epoch 10; iter: 0; batch classifier loss: 0.520420; batch adversarial loss: 0.600465\n",
            "epoch 10; iter: 200; batch classifier loss: 0.418076; batch adversarial loss: 0.610031\n",
            "epoch 11; iter: 0; batch classifier loss: 0.389331; batch adversarial loss: 0.640818\n",
            "epoch 11; iter: 200; batch classifier loss: 0.435995; batch adversarial loss: 0.599456\n",
            "epoch 12; iter: 0; batch classifier loss: 0.388662; batch adversarial loss: 0.560982\n",
            "epoch 12; iter: 200; batch classifier loss: 0.379729; batch adversarial loss: 0.620817\n",
            "epoch 13; iter: 0; batch classifier loss: 0.414088; batch adversarial loss: 0.600593\n",
            "epoch 13; iter: 200; batch classifier loss: 0.451503; batch adversarial loss: 0.643621\n",
            "epoch 14; iter: 0; batch classifier loss: 0.355862; batch adversarial loss: 0.591915\n",
            "epoch 14; iter: 200; batch classifier loss: 0.464647; batch adversarial loss: 0.616264\n",
            "epoch 15; iter: 0; batch classifier loss: 0.373223; batch adversarial loss: 0.575934\n",
            "epoch 15; iter: 200; batch classifier loss: 0.364119; batch adversarial loss: 0.612226\n",
            "epoch 16; iter: 0; batch classifier loss: 0.421298; batch adversarial loss: 0.578984\n",
            "epoch 16; iter: 200; batch classifier loss: 0.439730; batch adversarial loss: 0.479465\n",
            "epoch 17; iter: 0; batch classifier loss: 0.436214; batch adversarial loss: 0.476008\n",
            "epoch 17; iter: 200; batch classifier loss: 0.451884; batch adversarial loss: 0.574821\n",
            "epoch 18; iter: 0; batch classifier loss: 0.396826; batch adversarial loss: 0.565307\n",
            "epoch 18; iter: 200; batch classifier loss: 0.477560; batch adversarial loss: 0.484003\n",
            "epoch 19; iter: 0; batch classifier loss: 0.414571; batch adversarial loss: 0.498613\n",
            "epoch 19; iter: 200; batch classifier loss: 0.426895; batch adversarial loss: 0.496249\n",
            "epoch 20; iter: 0; batch classifier loss: 0.408351; batch adversarial loss: 0.491462\n",
            "epoch 20; iter: 200; batch classifier loss: 0.527999; batch adversarial loss: 0.416524\n",
            "epoch 21; iter: 0; batch classifier loss: 0.447926; batch adversarial loss: 0.490549\n",
            "epoch 21; iter: 200; batch classifier loss: 0.445274; batch adversarial loss: 0.424187\n",
            "epoch 22; iter: 0; batch classifier loss: 0.549754; batch adversarial loss: 0.479779\n",
            "epoch 22; iter: 200; batch classifier loss: 0.395038; batch adversarial loss: 0.506649\n",
            "epoch 23; iter: 0; batch classifier loss: 0.398616; batch adversarial loss: 0.440726\n",
            "epoch 23; iter: 200; batch classifier loss: 0.382315; batch adversarial loss: 0.492268\n",
            "epoch 24; iter: 0; batch classifier loss: 0.410161; batch adversarial loss: 0.482948\n",
            "epoch 24; iter: 200; batch classifier loss: 0.403812; batch adversarial loss: 0.484083\n",
            "epoch 25; iter: 0; batch classifier loss: 0.364729; batch adversarial loss: 0.490707\n",
            "epoch 25; iter: 200; batch classifier loss: 0.390738; batch adversarial loss: 0.533089\n",
            "epoch 26; iter: 0; batch classifier loss: 0.469804; batch adversarial loss: 0.473795\n",
            "epoch 26; iter: 200; batch classifier loss: 0.507332; batch adversarial loss: 0.478475\n",
            "epoch 27; iter: 0; batch classifier loss: 0.481905; batch adversarial loss: 0.488798\n",
            "epoch 27; iter: 200; batch classifier loss: 0.413164; batch adversarial loss: 0.501073\n",
            "epoch 28; iter: 0; batch classifier loss: 0.364273; batch adversarial loss: 0.474281\n",
            "epoch 28; iter: 200; batch classifier loss: 0.396654; batch adversarial loss: 0.486994\n",
            "epoch 29; iter: 0; batch classifier loss: 0.406279; batch adversarial loss: 0.512648\n",
            "epoch 29; iter: 200; batch classifier loss: 0.417172; batch adversarial loss: 0.488171\n",
            "epoch 30; iter: 0; batch classifier loss: 0.372464; batch adversarial loss: 0.502011\n",
            "epoch 30; iter: 200; batch classifier loss: 0.457731; batch adversarial loss: 0.523209\n",
            "epoch 31; iter: 0; batch classifier loss: 0.355767; batch adversarial loss: 0.499428\n",
            "epoch 31; iter: 200; batch classifier loss: 0.456186; batch adversarial loss: 0.506744\n",
            "epoch 32; iter: 0; batch classifier loss: 0.435706; batch adversarial loss: 0.538545\n",
            "epoch 32; iter: 200; batch classifier loss: 0.424871; batch adversarial loss: 0.460600\n",
            "epoch 33; iter: 0; batch classifier loss: 0.468063; batch adversarial loss: 0.558077\n",
            "epoch 33; iter: 200; batch classifier loss: 0.538553; batch adversarial loss: 0.553639\n",
            "epoch 34; iter: 0; batch classifier loss: 0.401681; batch adversarial loss: 0.535677\n",
            "epoch 34; iter: 200; batch classifier loss: 0.365356; batch adversarial loss: 0.589704\n",
            "epoch 35; iter: 0; batch classifier loss: 0.369377; batch adversarial loss: 0.497083\n",
            "epoch 35; iter: 200; batch classifier loss: 0.458083; batch adversarial loss: 0.594340\n",
            "epoch 36; iter: 0; batch classifier loss: 0.434857; batch adversarial loss: 0.551935\n",
            "epoch 36; iter: 200; batch classifier loss: 0.453092; batch adversarial loss: 0.566996\n",
            "epoch 37; iter: 0; batch classifier loss: 0.486827; batch adversarial loss: 0.609217\n",
            "epoch 37; iter: 200; batch classifier loss: 0.418506; batch adversarial loss: 0.511098\n",
            "epoch 38; iter: 0; batch classifier loss: 0.374107; batch adversarial loss: 0.582225\n",
            "epoch 38; iter: 200; batch classifier loss: 0.367404; batch adversarial loss: 0.593555\n",
            "epoch 39; iter: 0; batch classifier loss: 0.493687; batch adversarial loss: 0.579759\n",
            "epoch 39; iter: 200; batch classifier loss: 0.363276; batch adversarial loss: 0.584402\n",
            "epoch 40; iter: 0; batch classifier loss: 0.430660; batch adversarial loss: 0.593593\n",
            "epoch 40; iter: 200; batch classifier loss: 0.384029; batch adversarial loss: 0.546948\n",
            "epoch 41; iter: 0; batch classifier loss: 0.517214; batch adversarial loss: 0.534625\n",
            "epoch 41; iter: 200; batch classifier loss: 0.536751; batch adversarial loss: 0.547279\n",
            "epoch 42; iter: 0; batch classifier loss: 0.443685; batch adversarial loss: 0.571052\n",
            "epoch 42; iter: 200; batch classifier loss: 0.397570; batch adversarial loss: 0.573682\n",
            "epoch 43; iter: 0; batch classifier loss: 0.339167; batch adversarial loss: 0.496740\n",
            "epoch 43; iter: 200; batch classifier loss: 0.375650; batch adversarial loss: 0.524684\n",
            "epoch 44; iter: 0; batch classifier loss: 0.419097; batch adversarial loss: 0.475116\n",
            "epoch 44; iter: 200; batch classifier loss: 0.388222; batch adversarial loss: 0.627595\n",
            "epoch 45; iter: 0; batch classifier loss: 0.514400; batch adversarial loss: 0.531365\n",
            "epoch 45; iter: 200; batch classifier loss: 0.441852; batch adversarial loss: 0.619507\n",
            "epoch 46; iter: 0; batch classifier loss: 0.418689; batch adversarial loss: 0.638865\n",
            "epoch 46; iter: 200; batch classifier loss: 0.407963; batch adversarial loss: 0.555281\n",
            "epoch 47; iter: 0; batch classifier loss: 0.336876; batch adversarial loss: 0.563988\n",
            "epoch 47; iter: 200; batch classifier loss: 0.424297; batch adversarial loss: 0.584036\n",
            "epoch 48; iter: 0; batch classifier loss: 0.452587; batch adversarial loss: 0.500872\n",
            "epoch 48; iter: 200; batch classifier loss: 0.470038; batch adversarial loss: 0.478760\n",
            "epoch 49; iter: 0; batch classifier loss: 0.397136; batch adversarial loss: 0.600318\n",
            "epoch 49; iter: 200; batch classifier loss: 0.426681; batch adversarial loss: 0.504349\n",
            "epoch 0; iter: 0; batch classifier loss: 0.720219; batch adversarial loss: 0.789953\n",
            "epoch 0; iter: 200; batch classifier loss: 0.440872; batch adversarial loss: 0.729518\n",
            "epoch 1; iter: 0; batch classifier loss: 0.475411; batch adversarial loss: 0.719972\n",
            "epoch 1; iter: 200; batch classifier loss: 0.449075; batch adversarial loss: 0.633477\n",
            "epoch 2; iter: 0; batch classifier loss: 0.462001; batch adversarial loss: 0.670546\n",
            "epoch 2; iter: 200; batch classifier loss: 0.522202; batch adversarial loss: 0.647135\n",
            "epoch 3; iter: 0; batch classifier loss: 0.354975; batch adversarial loss: 0.633334\n",
            "epoch 3; iter: 200; batch classifier loss: 0.442301; batch adversarial loss: 0.577491\n",
            "epoch 4; iter: 0; batch classifier loss: 0.427047; batch adversarial loss: 0.655244\n",
            "epoch 4; iter: 200; batch classifier loss: 0.495084; batch adversarial loss: 0.612824\n",
            "epoch 5; iter: 0; batch classifier loss: 0.369820; batch adversarial loss: 0.632483\n",
            "epoch 5; iter: 200; batch classifier loss: 0.439842; batch adversarial loss: 0.679491\n",
            "epoch 6; iter: 0; batch classifier loss: 0.426687; batch adversarial loss: 0.654411\n",
            "epoch 6; iter: 200; batch classifier loss: 0.406733; batch adversarial loss: 0.627710\n",
            "epoch 7; iter: 0; batch classifier loss: 0.388423; batch adversarial loss: 0.603944\n",
            "epoch 7; iter: 200; batch classifier loss: 0.404452; batch adversarial loss: 0.657909\n",
            "epoch 8; iter: 0; batch classifier loss: 0.528592; batch adversarial loss: 0.603117\n",
            "epoch 8; iter: 200; batch classifier loss: 0.376935; batch adversarial loss: 0.683776\n",
            "epoch 9; iter: 0; batch classifier loss: 0.401065; batch adversarial loss: 0.605946\n",
            "epoch 9; iter: 200; batch classifier loss: 0.369416; batch adversarial loss: 0.592142\n",
            "epoch 10; iter: 0; batch classifier loss: 0.428469; batch adversarial loss: 0.628123\n",
            "epoch 10; iter: 200; batch classifier loss: 0.466544; batch adversarial loss: 0.587413\n",
            "epoch 11; iter: 0; batch classifier loss: 0.418293; batch adversarial loss: 0.576873\n",
            "epoch 11; iter: 200; batch classifier loss: 0.327386; batch adversarial loss: 0.578618\n",
            "epoch 12; iter: 0; batch classifier loss: 0.446817; batch adversarial loss: 0.590294\n",
            "epoch 12; iter: 200; batch classifier loss: 0.412301; batch adversarial loss: 0.622702\n",
            "epoch 13; iter: 0; batch classifier loss: 0.310319; batch adversarial loss: 0.573336\n",
            "epoch 13; iter: 200; batch classifier loss: 0.441792; batch adversarial loss: 0.610363\n",
            "epoch 14; iter: 0; batch classifier loss: 0.399429; batch adversarial loss: 0.601072\n",
            "epoch 14; iter: 200; batch classifier loss: 0.379187; batch adversarial loss: 0.539607\n",
            "epoch 15; iter: 0; batch classifier loss: 0.384511; batch adversarial loss: 0.548342\n",
            "epoch 15; iter: 200; batch classifier loss: 0.475714; batch adversarial loss: 0.496134\n",
            "epoch 16; iter: 0; batch classifier loss: 0.458852; batch adversarial loss: 0.579072\n",
            "epoch 16; iter: 200; batch classifier loss: 0.383205; batch adversarial loss: 0.625114\n",
            "epoch 17; iter: 0; batch classifier loss: 0.385981; batch adversarial loss: 0.567459\n",
            "epoch 17; iter: 200; batch classifier loss: 0.366940; batch adversarial loss: 0.525927\n",
            "epoch 18; iter: 0; batch classifier loss: 0.478037; batch adversarial loss: 0.537485\n",
            "epoch 18; iter: 200; batch classifier loss: 0.386996; batch adversarial loss: 0.572716\n",
            "epoch 19; iter: 0; batch classifier loss: 0.394347; batch adversarial loss: 0.577885\n",
            "epoch 19; iter: 200; batch classifier loss: 0.386197; batch adversarial loss: 0.595780\n",
            "epoch 20; iter: 0; batch classifier loss: 0.417493; batch adversarial loss: 0.522631\n",
            "epoch 20; iter: 200; batch classifier loss: 0.460939; batch adversarial loss: 0.525298\n",
            "epoch 21; iter: 0; batch classifier loss: 0.385389; batch adversarial loss: 0.562718\n",
            "epoch 21; iter: 200; batch classifier loss: 0.384520; batch adversarial loss: 0.506662\n",
            "epoch 22; iter: 0; batch classifier loss: 0.416724; batch adversarial loss: 0.528833\n",
            "epoch 22; iter: 200; batch classifier loss: 0.370110; batch adversarial loss: 0.498062\n",
            "epoch 23; iter: 0; batch classifier loss: 0.365942; batch adversarial loss: 0.547789\n",
            "epoch 23; iter: 200; batch classifier loss: 0.447272; batch adversarial loss: 0.532697\n",
            "epoch 24; iter: 0; batch classifier loss: 0.450984; batch adversarial loss: 0.522616\n",
            "epoch 24; iter: 200; batch classifier loss: 0.470176; batch adversarial loss: 0.574422\n",
            "epoch 25; iter: 0; batch classifier loss: 0.422884; batch adversarial loss: 0.536160\n",
            "epoch 25; iter: 200; batch classifier loss: 0.455108; batch adversarial loss: 0.593322\n",
            "epoch 26; iter: 0; batch classifier loss: 0.349034; batch adversarial loss: 0.517581\n",
            "epoch 26; iter: 200; batch classifier loss: 0.364285; batch adversarial loss: 0.591632\n",
            "epoch 27; iter: 0; batch classifier loss: 0.405129; batch adversarial loss: 0.482692\n",
            "epoch 27; iter: 200; batch classifier loss: 0.407922; batch adversarial loss: 0.531900\n",
            "epoch 28; iter: 0; batch classifier loss: 0.365722; batch adversarial loss: 0.562002\n",
            "epoch 28; iter: 200; batch classifier loss: 0.356632; batch adversarial loss: 0.527645\n",
            "epoch 29; iter: 0; batch classifier loss: 0.445115; batch adversarial loss: 0.551192\n",
            "epoch 29; iter: 200; batch classifier loss: 0.415650; batch adversarial loss: 0.539647\n",
            "epoch 30; iter: 0; batch classifier loss: 0.388488; batch adversarial loss: 0.580535\n",
            "epoch 30; iter: 200; batch classifier loss: 0.354059; batch adversarial loss: 0.574805\n",
            "epoch 31; iter: 0; batch classifier loss: 0.422122; batch adversarial loss: 0.528177\n",
            "epoch 31; iter: 200; batch classifier loss: 0.413442; batch adversarial loss: 0.500883\n",
            "epoch 32; iter: 0; batch classifier loss: 0.397203; batch adversarial loss: 0.564422\n",
            "epoch 32; iter: 200; batch classifier loss: 0.440206; batch adversarial loss: 0.479744\n",
            "epoch 33; iter: 0; batch classifier loss: 0.398877; batch adversarial loss: 0.545223\n",
            "epoch 33; iter: 200; batch classifier loss: 0.396632; batch adversarial loss: 0.515745\n",
            "epoch 34; iter: 0; batch classifier loss: 0.439996; batch adversarial loss: 0.525106\n",
            "epoch 34; iter: 200; batch classifier loss: 0.472173; batch adversarial loss: 0.506793\n",
            "epoch 35; iter: 0; batch classifier loss: 0.379998; batch adversarial loss: 0.528490\n",
            "epoch 35; iter: 200; batch classifier loss: 0.331150; batch adversarial loss: 0.533018\n",
            "epoch 36; iter: 0; batch classifier loss: 0.418810; batch adversarial loss: 0.498476\n",
            "epoch 36; iter: 200; batch classifier loss: 0.363540; batch adversarial loss: 0.449313\n",
            "epoch 37; iter: 0; batch classifier loss: 0.406190; batch adversarial loss: 0.494222\n",
            "epoch 37; iter: 200; batch classifier loss: 0.462874; batch adversarial loss: 0.507906\n",
            "epoch 38; iter: 0; batch classifier loss: 0.401329; batch adversarial loss: 0.520852\n",
            "epoch 38; iter: 200; batch classifier loss: 0.371166; batch adversarial loss: 0.537145\n",
            "epoch 39; iter: 0; batch classifier loss: 0.392729; batch adversarial loss: 0.559164\n",
            "epoch 39; iter: 200; batch classifier loss: 0.398485; batch adversarial loss: 0.540446\n",
            "epoch 40; iter: 0; batch classifier loss: 0.415372; batch adversarial loss: 0.510448\n",
            "epoch 40; iter: 200; batch classifier loss: 0.419263; batch adversarial loss: 0.560234\n",
            "epoch 41; iter: 0; batch classifier loss: 0.422032; batch adversarial loss: 0.585457\n",
            "epoch 41; iter: 200; batch classifier loss: 0.404276; batch adversarial loss: 0.567192\n",
            "epoch 42; iter: 0; batch classifier loss: 0.465792; batch adversarial loss: 0.529106\n",
            "epoch 42; iter: 200; batch classifier loss: 0.507987; batch adversarial loss: 0.510251\n",
            "epoch 43; iter: 0; batch classifier loss: 0.447581; batch adversarial loss: 0.500056\n",
            "epoch 43; iter: 200; batch classifier loss: 0.430530; batch adversarial loss: 0.499461\n",
            "epoch 44; iter: 0; batch classifier loss: 0.368630; batch adversarial loss: 0.529806\n",
            "epoch 44; iter: 200; batch classifier loss: 0.381777; batch adversarial loss: 0.504694\n",
            "epoch 45; iter: 0; batch classifier loss: 0.497803; batch adversarial loss: 0.527809\n",
            "epoch 45; iter: 200; batch classifier loss: 0.366067; batch adversarial loss: 0.567459\n",
            "epoch 46; iter: 0; batch classifier loss: 0.440790; batch adversarial loss: 0.516071\n",
            "epoch 46; iter: 200; batch classifier loss: 0.333438; batch adversarial loss: 0.510004\n",
            "epoch 47; iter: 0; batch classifier loss: 0.447692; batch adversarial loss: 0.573541\n",
            "epoch 47; iter: 200; batch classifier loss: 0.391563; batch adversarial loss: 0.583546\n",
            "epoch 48; iter: 0; batch classifier loss: 0.343984; batch adversarial loss: 0.516351\n",
            "epoch 48; iter: 200; batch classifier loss: 0.337400; batch adversarial loss: 0.534426\n",
            "epoch 49; iter: 0; batch classifier loss: 0.372452; batch adversarial loss: 0.526237\n",
            "epoch 49; iter: 200; batch classifier loss: 0.477476; batch adversarial loss: 0.524951\n",
            "epoch 0; iter: 0; batch classifier loss: 0.669786; batch adversarial loss: 0.617955\n",
            "epoch 0; iter: 200; batch classifier loss: 1.041968; batch adversarial loss: 0.836975\n",
            "epoch 1; iter: 0; batch classifier loss: 0.983714; batch adversarial loss: 0.875129\n",
            "epoch 1; iter: 200; batch classifier loss: 0.844648; batch adversarial loss: 0.719309\n",
            "epoch 2; iter: 0; batch classifier loss: 0.848435; batch adversarial loss: 0.738678\n",
            "epoch 2; iter: 200; batch classifier loss: 0.546863; batch adversarial loss: 0.634207\n",
            "epoch 3; iter: 0; batch classifier loss: 0.543413; batch adversarial loss: 0.656512\n",
            "epoch 3; iter: 200; batch classifier loss: 0.424251; batch adversarial loss: 0.647011\n",
            "epoch 4; iter: 0; batch classifier loss: 0.446103; batch adversarial loss: 0.648066\n",
            "epoch 4; iter: 200; batch classifier loss: 0.512904; batch adversarial loss: 0.630624\n",
            "epoch 5; iter: 0; batch classifier loss: 0.476246; batch adversarial loss: 0.657730\n",
            "epoch 5; iter: 200; batch classifier loss: 0.435235; batch adversarial loss: 0.684226\n",
            "epoch 6; iter: 0; batch classifier loss: 0.421862; batch adversarial loss: 0.597254\n",
            "epoch 6; iter: 200; batch classifier loss: 0.550717; batch adversarial loss: 0.622087\n",
            "epoch 7; iter: 0; batch classifier loss: 0.506935; batch adversarial loss: 0.630123\n",
            "epoch 7; iter: 200; batch classifier loss: 0.450261; batch adversarial loss: 0.634177\n",
            "epoch 8; iter: 0; batch classifier loss: 0.470495; batch adversarial loss: 0.712636\n",
            "epoch 8; iter: 200; batch classifier loss: 0.539456; batch adversarial loss: 0.624786\n",
            "epoch 9; iter: 0; batch classifier loss: 0.522742; batch adversarial loss: 0.670106\n",
            "epoch 9; iter: 200; batch classifier loss: 0.447435; batch adversarial loss: 0.678099\n",
            "epoch 10; iter: 0; batch classifier loss: 0.357826; batch adversarial loss: 0.656139\n",
            "epoch 10; iter: 200; batch classifier loss: 0.397671; batch adversarial loss: 0.680185\n",
            "epoch 11; iter: 0; batch classifier loss: 0.437863; batch adversarial loss: 0.591915\n",
            "epoch 11; iter: 200; batch classifier loss: 0.406036; batch adversarial loss: 0.613333\n",
            "epoch 12; iter: 0; batch classifier loss: 0.473070; batch adversarial loss: 0.637637\n",
            "epoch 12; iter: 200; batch classifier loss: 0.389423; batch adversarial loss: 0.625932\n",
            "epoch 13; iter: 0; batch classifier loss: 0.479932; batch adversarial loss: 0.590449\n",
            "epoch 13; iter: 200; batch classifier loss: 0.458000; batch adversarial loss: 0.576514\n",
            "epoch 14; iter: 0; batch classifier loss: 0.542445; batch adversarial loss: 0.659532\n",
            "epoch 14; iter: 200; batch classifier loss: 0.516214; batch adversarial loss: 0.659064\n",
            "epoch 15; iter: 0; batch classifier loss: 0.470564; batch adversarial loss: 0.627266\n",
            "epoch 15; iter: 200; batch classifier loss: 0.561779; batch adversarial loss: 0.590849\n",
            "epoch 16; iter: 0; batch classifier loss: 0.438548; batch adversarial loss: 0.669407\n",
            "epoch 16; iter: 200; batch classifier loss: 0.454706; batch adversarial loss: 0.570617\n",
            "epoch 17; iter: 0; batch classifier loss: 0.411444; batch adversarial loss: 0.594984\n",
            "epoch 17; iter: 200; batch classifier loss: 0.418689; batch adversarial loss: 0.594091\n",
            "epoch 18; iter: 0; batch classifier loss: 0.423071; batch adversarial loss: 0.632681\n",
            "epoch 18; iter: 200; batch classifier loss: 0.457237; batch adversarial loss: 0.642528\n",
            "epoch 19; iter: 0; batch classifier loss: 0.443197; batch adversarial loss: 0.630438\n",
            "epoch 19; iter: 200; batch classifier loss: 0.504281; batch adversarial loss: 0.590492\n",
            "epoch 20; iter: 0; batch classifier loss: 0.450870; batch adversarial loss: 0.658356\n",
            "epoch 20; iter: 200; batch classifier loss: 0.476160; batch adversarial loss: 0.632287\n",
            "epoch 21; iter: 0; batch classifier loss: 0.524797; batch adversarial loss: 0.545780\n",
            "epoch 21; iter: 200; batch classifier loss: 0.481119; batch adversarial loss: 0.552573\n",
            "epoch 22; iter: 0; batch classifier loss: 0.401817; batch adversarial loss: 0.591077\n",
            "epoch 22; iter: 200; batch classifier loss: 0.433226; batch adversarial loss: 0.573526\n",
            "epoch 23; iter: 0; batch classifier loss: 0.448845; batch adversarial loss: 0.599426\n",
            "epoch 23; iter: 200; batch classifier loss: 0.406334; batch adversarial loss: 0.625898\n",
            "epoch 24; iter: 0; batch classifier loss: 0.373595; batch adversarial loss: 0.630639\n",
            "epoch 24; iter: 200; batch classifier loss: 0.483596; batch adversarial loss: 0.623167\n",
            "epoch 25; iter: 0; batch classifier loss: 0.381607; batch adversarial loss: 0.669662\n",
            "epoch 25; iter: 200; batch classifier loss: 0.463659; batch adversarial loss: 0.582922\n",
            "epoch 26; iter: 0; batch classifier loss: 0.349126; batch adversarial loss: 0.595490\n",
            "epoch 26; iter: 200; batch classifier loss: 0.379170; batch adversarial loss: 0.620184\n",
            "epoch 27; iter: 0; batch classifier loss: 0.433045; batch adversarial loss: 0.596259\n",
            "epoch 27; iter: 200; batch classifier loss: 0.447537; batch adversarial loss: 0.603257\n",
            "epoch 28; iter: 0; batch classifier loss: 0.464256; batch adversarial loss: 0.633731\n",
            "epoch 28; iter: 200; batch classifier loss: 0.541920; batch adversarial loss: 0.600588\n",
            "epoch 29; iter: 0; batch classifier loss: 0.533848; batch adversarial loss: 0.603470\n",
            "epoch 29; iter: 200; batch classifier loss: 0.431846; batch adversarial loss: 0.584681\n",
            "epoch 30; iter: 0; batch classifier loss: 0.485732; batch adversarial loss: 0.620771\n",
            "epoch 30; iter: 200; batch classifier loss: 0.567787; batch adversarial loss: 0.566594\n",
            "epoch 31; iter: 0; batch classifier loss: 0.450923; batch adversarial loss: 0.603950\n",
            "epoch 31; iter: 200; batch classifier loss: 0.554948; batch adversarial loss: 0.573169\n",
            "epoch 32; iter: 0; batch classifier loss: 0.445288; batch adversarial loss: 0.607558\n",
            "epoch 32; iter: 200; batch classifier loss: 0.474524; batch adversarial loss: 0.612003\n",
            "epoch 33; iter: 0; batch classifier loss: 0.457520; batch adversarial loss: 0.637920\n",
            "epoch 33; iter: 200; batch classifier loss: 0.406313; batch adversarial loss: 0.631896\n",
            "epoch 34; iter: 0; batch classifier loss: 0.432838; batch adversarial loss: 0.620135\n",
            "epoch 34; iter: 200; batch classifier loss: 0.431671; batch adversarial loss: 0.614349\n",
            "epoch 35; iter: 0; batch classifier loss: 0.433493; batch adversarial loss: 0.597779\n",
            "epoch 35; iter: 200; batch classifier loss: 0.417112; batch adversarial loss: 0.718695\n",
            "epoch 36; iter: 0; batch classifier loss: 0.492619; batch adversarial loss: 0.582942\n",
            "epoch 36; iter: 200; batch classifier loss: 0.462508; batch adversarial loss: 0.590744\n",
            "epoch 37; iter: 0; batch classifier loss: 0.556782; batch adversarial loss: 0.578659\n",
            "epoch 37; iter: 200; batch classifier loss: 0.367397; batch adversarial loss: 0.631135\n",
            "epoch 38; iter: 0; batch classifier loss: 0.392482; batch adversarial loss: 0.794898\n",
            "epoch 38; iter: 200; batch classifier loss: 0.415826; batch adversarial loss: 0.614872\n",
            "epoch 39; iter: 0; batch classifier loss: 0.335624; batch adversarial loss: 0.670779\n",
            "epoch 39; iter: 200; batch classifier loss: 0.455453; batch adversarial loss: 0.618312\n",
            "epoch 40; iter: 0; batch classifier loss: 0.453310; batch adversarial loss: 0.659008\n",
            "epoch 40; iter: 200; batch classifier loss: 0.446598; batch adversarial loss: 0.709873\n",
            "epoch 41; iter: 0; batch classifier loss: 0.520176; batch adversarial loss: 0.579467\n",
            "epoch 41; iter: 200; batch classifier loss: 0.460999; batch adversarial loss: 0.622524\n",
            "epoch 42; iter: 0; batch classifier loss: 0.509556; batch adversarial loss: 0.613373\n",
            "epoch 42; iter: 200; batch classifier loss: 0.435571; batch adversarial loss: 0.677788\n",
            "epoch 43; iter: 0; batch classifier loss: 0.525710; batch adversarial loss: 0.640435\n",
            "epoch 43; iter: 200; batch classifier loss: 0.476987; batch adversarial loss: 0.641946\n",
            "epoch 44; iter: 0; batch classifier loss: 0.484012; batch adversarial loss: 0.624626\n",
            "epoch 44; iter: 200; batch classifier loss: 0.565742; batch adversarial loss: 0.632434\n",
            "epoch 45; iter: 0; batch classifier loss: 0.557292; batch adversarial loss: 0.587988\n",
            "epoch 45; iter: 200; batch classifier loss: 0.450030; batch adversarial loss: 0.645873\n",
            "epoch 46; iter: 0; batch classifier loss: 0.423499; batch adversarial loss: 0.611557\n",
            "epoch 46; iter: 200; batch classifier loss: 0.471338; batch adversarial loss: 0.641451\n",
            "epoch 47; iter: 0; batch classifier loss: 0.608348; batch adversarial loss: 0.561648\n",
            "epoch 47; iter: 200; batch classifier loss: 0.408552; batch adversarial loss: 0.669859\n",
            "epoch 48; iter: 0; batch classifier loss: 0.401595; batch adversarial loss: 0.635532\n",
            "epoch 48; iter: 200; batch classifier loss: 0.513676; batch adversarial loss: 0.594314\n",
            "epoch 49; iter: 0; batch classifier loss: 0.429905; batch adversarial loss: 0.607156\n",
            "epoch 49; iter: 200; batch classifier loss: 0.506003; batch adversarial loss: 0.626990\n",
            "epoch 0; iter: 0; batch classifier loss: 0.699388; batch adversarial loss: 0.626354\n",
            "epoch 0; iter: 200; batch classifier loss: 1.359081; batch adversarial loss: 0.774779\n",
            "epoch 1; iter: 0; batch classifier loss: 1.238451; batch adversarial loss: 0.747367\n",
            "epoch 1; iter: 200; batch classifier loss: 1.243239; batch adversarial loss: 0.677323\n",
            "epoch 2; iter: 0; batch classifier loss: 0.952554; batch adversarial loss: 0.658572\n",
            "epoch 2; iter: 200; batch classifier loss: 0.654735; batch adversarial loss: 0.650477\n",
            "epoch 3; iter: 0; batch classifier loss: 0.626971; batch adversarial loss: 0.621332\n",
            "epoch 3; iter: 200; batch classifier loss: 0.662158; batch adversarial loss: 0.666849\n",
            "epoch 4; iter: 0; batch classifier loss: 0.573086; batch adversarial loss: 0.632947\n",
            "epoch 4; iter: 200; batch classifier loss: 0.610317; batch adversarial loss: 0.560553\n",
            "epoch 5; iter: 0; batch classifier loss: 0.551604; batch adversarial loss: 0.643672\n",
            "epoch 5; iter: 200; batch classifier loss: 0.884449; batch adversarial loss: 0.621913\n",
            "epoch 6; iter: 0; batch classifier loss: 0.661101; batch adversarial loss: 0.643635\n",
            "epoch 6; iter: 200; batch classifier loss: 0.718338; batch adversarial loss: 0.636997\n",
            "epoch 7; iter: 0; batch classifier loss: 0.496514; batch adversarial loss: 0.620549\n",
            "epoch 7; iter: 200; batch classifier loss: 0.576744; batch adversarial loss: 0.606679\n",
            "epoch 8; iter: 0; batch classifier loss: 0.566947; batch adversarial loss: 0.653058\n",
            "epoch 8; iter: 200; batch classifier loss: 0.609788; batch adversarial loss: 0.620933\n",
            "epoch 9; iter: 0; batch classifier loss: 0.719347; batch adversarial loss: 0.650374\n",
            "epoch 9; iter: 200; batch classifier loss: 0.611033; batch adversarial loss: 0.641485\n",
            "epoch 10; iter: 0; batch classifier loss: 0.651729; batch adversarial loss: 0.660819\n",
            "epoch 10; iter: 200; batch classifier loss: 0.905396; batch adversarial loss: 0.571951\n",
            "epoch 11; iter: 0; batch classifier loss: 0.698944; batch adversarial loss: 0.611387\n",
            "epoch 11; iter: 200; batch classifier loss: 0.664817; batch adversarial loss: 0.683447\n",
            "epoch 12; iter: 0; batch classifier loss: 0.784070; batch adversarial loss: 0.643929\n",
            "epoch 12; iter: 200; batch classifier loss: 0.534940; batch adversarial loss: 0.639360\n",
            "epoch 13; iter: 0; batch classifier loss: 0.730158; batch adversarial loss: 0.621391\n",
            "epoch 13; iter: 200; batch classifier loss: 0.594310; batch adversarial loss: 0.611649\n",
            "epoch 14; iter: 0; batch classifier loss: 0.697981; batch adversarial loss: 0.633466\n",
            "epoch 14; iter: 200; batch classifier loss: 0.849973; batch adversarial loss: 0.569836\n",
            "epoch 15; iter: 0; batch classifier loss: 0.769342; batch adversarial loss: 0.650871\n",
            "epoch 15; iter: 200; batch classifier loss: 0.549751; batch adversarial loss: 0.674728\n",
            "epoch 16; iter: 0; batch classifier loss: 0.726915; batch adversarial loss: 0.633678\n",
            "epoch 16; iter: 200; batch classifier loss: 0.597868; batch adversarial loss: 0.634482\n",
            "epoch 17; iter: 0; batch classifier loss: 0.509068; batch adversarial loss: 0.612774\n",
            "epoch 17; iter: 200; batch classifier loss: 0.604842; batch adversarial loss: 0.684241\n",
            "epoch 18; iter: 0; batch classifier loss: 0.591882; batch adversarial loss: 0.605111\n",
            "epoch 18; iter: 200; batch classifier loss: 0.663656; batch adversarial loss: 0.663691\n",
            "epoch 19; iter: 0; batch classifier loss: 0.677854; batch adversarial loss: 0.684572\n",
            "epoch 19; iter: 200; batch classifier loss: 0.629849; batch adversarial loss: 0.630121\n",
            "epoch 20; iter: 0; batch classifier loss: 0.650366; batch adversarial loss: 0.643120\n",
            "epoch 20; iter: 200; batch classifier loss: 0.609002; batch adversarial loss: 0.647089\n",
            "epoch 21; iter: 0; batch classifier loss: 0.587709; batch adversarial loss: 0.629526\n",
            "epoch 21; iter: 200; batch classifier loss: 0.958485; batch adversarial loss: 0.663108\n",
            "epoch 22; iter: 0; batch classifier loss: 0.673183; batch adversarial loss: 0.679923\n",
            "epoch 22; iter: 200; batch classifier loss: 0.666245; batch adversarial loss: 0.640063\n",
            "epoch 23; iter: 0; batch classifier loss: 0.761949; batch adversarial loss: 0.642035\n",
            "epoch 23; iter: 200; batch classifier loss: 0.630756; batch adversarial loss: 0.653752\n",
            "epoch 24; iter: 0; batch classifier loss: 0.585311; batch adversarial loss: 0.627834\n",
            "epoch 24; iter: 200; batch classifier loss: 0.538856; batch adversarial loss: 0.596102\n",
            "epoch 25; iter: 0; batch classifier loss: 0.528630; batch adversarial loss: 0.635576\n",
            "epoch 25; iter: 200; batch classifier loss: 0.873701; batch adversarial loss: 0.587095\n",
            "epoch 26; iter: 0; batch classifier loss: 0.821730; batch adversarial loss: 0.637060\n",
            "epoch 26; iter: 200; batch classifier loss: 0.622236; batch adversarial loss: 0.608255\n",
            "epoch 27; iter: 0; batch classifier loss: 0.520544; batch adversarial loss: 0.654570\n",
            "epoch 27; iter: 200; batch classifier loss: 0.509562; batch adversarial loss: 0.646447\n",
            "epoch 28; iter: 0; batch classifier loss: 0.547987; batch adversarial loss: 0.644363\n",
            "epoch 28; iter: 200; batch classifier loss: 0.889603; batch adversarial loss: 0.633455\n",
            "epoch 29; iter: 0; batch classifier loss: 0.604315; batch adversarial loss: 0.647248\n",
            "epoch 29; iter: 200; batch classifier loss: 0.684921; batch adversarial loss: 0.602475\n",
            "epoch 30; iter: 0; batch classifier loss: 0.637252; batch adversarial loss: 0.646829\n",
            "epoch 30; iter: 200; batch classifier loss: 0.522920; batch adversarial loss: 0.609222\n",
            "epoch 31; iter: 0; batch classifier loss: 0.639666; batch adversarial loss: 0.623728\n",
            "epoch 31; iter: 200; batch classifier loss: 0.631830; batch adversarial loss: 0.675271\n",
            "epoch 32; iter: 0; batch classifier loss: 0.794227; batch adversarial loss: 0.638464\n",
            "epoch 32; iter: 200; batch classifier loss: 0.567515; batch adversarial loss: 0.651145\n",
            "epoch 33; iter: 0; batch classifier loss: 0.622702; batch adversarial loss: 0.639177\n",
            "epoch 33; iter: 200; batch classifier loss: 0.698468; batch adversarial loss: 0.637133\n",
            "epoch 34; iter: 0; batch classifier loss: 0.626177; batch adversarial loss: 0.631734\n",
            "epoch 34; iter: 200; batch classifier loss: 0.617812; batch adversarial loss: 0.602650\n",
            "epoch 35; iter: 0; batch classifier loss: 0.628243; batch adversarial loss: 0.664990\n",
            "epoch 35; iter: 200; batch classifier loss: 0.630618; batch adversarial loss: 0.635016\n",
            "epoch 36; iter: 0; batch classifier loss: 0.610780; batch adversarial loss: 0.673090\n",
            "epoch 36; iter: 200; batch classifier loss: 0.670407; batch adversarial loss: 0.649936\n",
            "epoch 37; iter: 0; batch classifier loss: 0.527570; batch adversarial loss: 0.620975\n",
            "epoch 37; iter: 200; batch classifier loss: 0.830772; batch adversarial loss: 0.653670\n",
            "epoch 38; iter: 0; batch classifier loss: 0.517798; batch adversarial loss: 0.633457\n",
            "epoch 38; iter: 200; batch classifier loss: 0.695426; batch adversarial loss: 0.585280\n",
            "epoch 39; iter: 0; batch classifier loss: 1.103787; batch adversarial loss: 0.583130\n",
            "epoch 39; iter: 200; batch classifier loss: 0.505796; batch adversarial loss: 0.620603\n",
            "epoch 40; iter: 0; batch classifier loss: 0.598686; batch adversarial loss: 0.687711\n",
            "epoch 40; iter: 200; batch classifier loss: 0.589800; batch adversarial loss: 0.616157\n",
            "epoch 41; iter: 0; batch classifier loss: 0.448954; batch adversarial loss: 0.633353\n",
            "epoch 41; iter: 200; batch classifier loss: 0.767078; batch adversarial loss: 0.614320\n",
            "epoch 42; iter: 0; batch classifier loss: 0.678139; batch adversarial loss: 0.609655\n",
            "epoch 42; iter: 200; batch classifier loss: 0.388649; batch adversarial loss: 0.684408\n",
            "epoch 43; iter: 0; batch classifier loss: 0.723625; batch adversarial loss: 0.611283\n",
            "epoch 43; iter: 200; batch classifier loss: 0.587388; batch adversarial loss: 0.668208\n",
            "epoch 44; iter: 0; batch classifier loss: 0.640076; batch adversarial loss: 0.569020\n",
            "epoch 44; iter: 200; batch classifier loss: 0.693232; batch adversarial loss: 0.599630\n",
            "epoch 45; iter: 0; batch classifier loss: 0.496708; batch adversarial loss: 0.627339\n",
            "epoch 45; iter: 200; batch classifier loss: 0.667583; batch adversarial loss: 0.589896\n",
            "epoch 46; iter: 0; batch classifier loss: 0.685459; batch adversarial loss: 0.688258\n",
            "epoch 46; iter: 200; batch classifier loss: 0.563435; batch adversarial loss: 0.636336\n",
            "epoch 47; iter: 0; batch classifier loss: 0.541150; batch adversarial loss: 0.645230\n",
            "epoch 47; iter: 200; batch classifier loss: 0.712546; batch adversarial loss: 0.604996\n",
            "epoch 48; iter: 0; batch classifier loss: 0.625273; batch adversarial loss: 0.678661\n",
            "epoch 48; iter: 200; batch classifier loss: 0.724962; batch adversarial loss: 0.579790\n",
            "epoch 49; iter: 0; batch classifier loss: 0.708625; batch adversarial loss: 0.660529\n",
            "epoch 49; iter: 200; batch classifier loss: 0.543032; batch adversarial loss: 0.644562\n",
            "epoch 0; iter: 0; batch classifier loss: 0.658426; batch adversarial loss: 0.741499\n",
            "epoch 0; iter: 200; batch classifier loss: 1.353543; batch adversarial loss: 0.762533\n",
            "epoch 1; iter: 0; batch classifier loss: 1.503312; batch adversarial loss: 0.758082\n",
            "epoch 1; iter: 200; batch classifier loss: 1.321686; batch adversarial loss: 0.671250\n",
            "epoch 2; iter: 0; batch classifier loss: 1.262273; batch adversarial loss: 0.640766\n",
            "epoch 2; iter: 200; batch classifier loss: 1.680152; batch adversarial loss: 0.621556\n",
            "epoch 3; iter: 0; batch classifier loss: 1.924861; batch adversarial loss: 0.644384\n",
            "epoch 3; iter: 200; batch classifier loss: 1.802681; batch adversarial loss: 0.612774\n",
            "epoch 4; iter: 0; batch classifier loss: 2.648061; batch adversarial loss: 0.635387\n",
            "epoch 4; iter: 200; batch classifier loss: 2.313046; batch adversarial loss: 0.639201\n",
            "epoch 5; iter: 0; batch classifier loss: 1.916533; batch adversarial loss: 0.667193\n",
            "epoch 5; iter: 200; batch classifier loss: 1.890390; batch adversarial loss: 0.658407\n",
            "epoch 6; iter: 0; batch classifier loss: 2.944973; batch adversarial loss: 0.628371\n",
            "epoch 6; iter: 200; batch classifier loss: 2.031185; batch adversarial loss: 0.643542\n",
            "epoch 7; iter: 0; batch classifier loss: 2.395427; batch adversarial loss: 0.622612\n",
            "epoch 7; iter: 200; batch classifier loss: 1.811427; batch adversarial loss: 0.622275\n",
            "epoch 8; iter: 0; batch classifier loss: 2.090925; batch adversarial loss: 0.670452\n",
            "epoch 8; iter: 200; batch classifier loss: 2.077953; batch adversarial loss: 0.687019\n",
            "epoch 9; iter: 0; batch classifier loss: 2.526625; batch adversarial loss: 0.627465\n",
            "epoch 9; iter: 200; batch classifier loss: 1.923630; batch adversarial loss: 0.643823\n",
            "epoch 10; iter: 0; batch classifier loss: 1.930892; batch adversarial loss: 0.638348\n",
            "epoch 10; iter: 200; batch classifier loss: 1.939364; batch adversarial loss: 0.589150\n",
            "epoch 11; iter: 0; batch classifier loss: 1.757774; batch adversarial loss: 0.638341\n",
            "epoch 11; iter: 200; batch classifier loss: 1.359629; batch adversarial loss: 0.692822\n",
            "epoch 12; iter: 0; batch classifier loss: 1.740641; batch adversarial loss: 0.665863\n",
            "epoch 12; iter: 200; batch classifier loss: 2.038271; batch adversarial loss: 0.577885\n",
            "epoch 13; iter: 0; batch classifier loss: 1.901006; batch adversarial loss: 0.627355\n",
            "epoch 13; iter: 200; batch classifier loss: 1.858275; batch adversarial loss: 0.638355\n",
            "epoch 14; iter: 0; batch classifier loss: 1.603851; batch adversarial loss: 0.621872\n",
            "epoch 14; iter: 200; batch classifier loss: 2.015823; batch adversarial loss: 0.687675\n",
            "epoch 15; iter: 0; batch classifier loss: 1.707579; batch adversarial loss: 0.621873\n",
            "epoch 15; iter: 200; batch classifier loss: 2.014432; batch adversarial loss: 0.610729\n",
            "epoch 16; iter: 0; batch classifier loss: 1.802800; batch adversarial loss: 0.643865\n",
            "epoch 16; iter: 200; batch classifier loss: 1.859071; batch adversarial loss: 0.654806\n",
            "epoch 17; iter: 0; batch classifier loss: 1.773792; batch adversarial loss: 0.594448\n",
            "epoch 17; iter: 200; batch classifier loss: 1.862178; batch adversarial loss: 0.665651\n",
            "epoch 18; iter: 0; batch classifier loss: 2.035986; batch adversarial loss: 0.638347\n",
            "epoch 18; iter: 200; batch classifier loss: 1.828600; batch adversarial loss: 0.649408\n",
            "epoch 19; iter: 0; batch classifier loss: 1.577090; batch adversarial loss: 0.638379\n",
            "epoch 19; iter: 200; batch classifier loss: 2.034552; batch adversarial loss: 0.638364\n",
            "epoch 20; iter: 0; batch classifier loss: 1.594037; batch adversarial loss: 0.687831\n",
            "epoch 20; iter: 200; batch classifier loss: 2.030123; batch adversarial loss: 0.660580\n",
            "epoch 21; iter: 0; batch classifier loss: 1.590666; batch adversarial loss: 0.627418\n",
            "epoch 21; iter: 200; batch classifier loss: 1.143899; batch adversarial loss: 0.654760\n",
            "epoch 22; iter: 0; batch classifier loss: 1.819731; batch adversarial loss: 0.605440\n",
            "epoch 22; iter: 200; batch classifier loss: 1.834991; batch adversarial loss: 0.605616\n",
            "epoch 23; iter: 0; batch classifier loss: 1.326623; batch adversarial loss: 0.649363\n",
            "epoch 23; iter: 200; batch classifier loss: 1.516971; batch adversarial loss: 0.666137\n",
            "epoch 24; iter: 0; batch classifier loss: 1.738250; batch adversarial loss: 0.561550\n",
            "epoch 24; iter: 200; batch classifier loss: 1.549177; batch adversarial loss: 0.621938\n",
            "epoch 25; iter: 0; batch classifier loss: 1.043821; batch adversarial loss: 0.654806\n",
            "epoch 25; iter: 200; batch classifier loss: 1.501660; batch adversarial loss: 0.632876\n",
            "epoch 26; iter: 0; batch classifier loss: 1.405295; batch adversarial loss: 0.594376\n",
            "epoch 26; iter: 200; batch classifier loss: 1.708225; batch adversarial loss: 0.654933\n",
            "epoch 27; iter: 0; batch classifier loss: 1.583813; batch adversarial loss: 0.583363\n",
            "epoch 27; iter: 200; batch classifier loss: 1.624374; batch adversarial loss: 0.660362\n",
            "epoch 28; iter: 0; batch classifier loss: 1.362253; batch adversarial loss: 0.594260\n",
            "epoch 28; iter: 200; batch classifier loss: 1.753555; batch adversarial loss: 0.599958\n",
            "epoch 29; iter: 0; batch classifier loss: 1.796349; batch adversarial loss: 0.610832\n",
            "epoch 29; iter: 200; batch classifier loss: 2.069171; batch adversarial loss: 0.660415\n",
            "epoch 30; iter: 0; batch classifier loss: 1.545217; batch adversarial loss: 0.594344\n",
            "epoch 30; iter: 200; batch classifier loss: 1.654276; batch adversarial loss: 0.638352\n",
            "epoch 31; iter: 0; batch classifier loss: 1.633676; batch adversarial loss: 0.610933\n",
            "epoch 31; iter: 200; batch classifier loss: 1.933568; batch adversarial loss: 0.599573\n",
            "epoch 32; iter: 0; batch classifier loss: 2.068836; batch adversarial loss: 0.665803\n",
            "epoch 32; iter: 200; batch classifier loss: 1.581791; batch adversarial loss: 0.682168\n",
            "epoch 33; iter: 0; batch classifier loss: 1.335933; batch adversarial loss: 0.621858\n",
            "epoch 33; iter: 200; batch classifier loss: 1.985203; batch adversarial loss: 0.682391\n",
            "epoch 34; iter: 0; batch classifier loss: 2.108905; batch adversarial loss: 0.589014\n",
            "epoch 34; iter: 200; batch classifier loss: 2.165313; batch adversarial loss: 0.643850\n",
            "epoch 35; iter: 0; batch classifier loss: 1.928176; batch adversarial loss: 0.616409\n",
            "epoch 35; iter: 200; batch classifier loss: 2.126884; batch adversarial loss: 0.632848\n",
            "epoch 36; iter: 0; batch classifier loss: 2.078402; batch adversarial loss: 0.577713\n",
            "epoch 36; iter: 200; batch classifier loss: 1.713520; batch adversarial loss: 0.704443\n",
            "epoch 37; iter: 0; batch classifier loss: 1.719072; batch adversarial loss: 0.616413\n",
            "epoch 37; iter: 200; batch classifier loss: 1.672136; batch adversarial loss: 0.638382\n",
            "epoch 38; iter: 0; batch classifier loss: 1.853240; batch adversarial loss: 0.605488\n",
            "epoch 38; iter: 200; batch classifier loss: 1.329906; batch adversarial loss: 0.643779\n",
            "epoch 39; iter: 0; batch classifier loss: 1.082966; batch adversarial loss: 0.665687\n",
            "epoch 39; iter: 200; batch classifier loss: 1.543688; batch adversarial loss: 0.671377\n",
            "epoch 40; iter: 0; batch classifier loss: 1.476009; batch adversarial loss: 0.610847\n",
            "epoch 40; iter: 200; batch classifier loss: 3.009676; batch adversarial loss: 0.632809\n",
            "epoch 41; iter: 0; batch classifier loss: 2.347733; batch adversarial loss: 0.638159\n",
            "epoch 41; iter: 200; batch classifier loss: 2.438025; batch adversarial loss: 0.649422\n",
            "epoch 42; iter: 0; batch classifier loss: 1.885232; batch adversarial loss: 0.621881\n",
            "epoch 42; iter: 200; batch classifier loss: 1.887939; batch adversarial loss: 0.638335\n",
            "epoch 43; iter: 0; batch classifier loss: 2.551773; batch adversarial loss: 0.621909\n",
            "epoch 43; iter: 200; batch classifier loss: 2.451242; batch adversarial loss: 0.638370\n",
            "epoch 44; iter: 0; batch classifier loss: 2.394958; batch adversarial loss: 0.627384\n",
            "epoch 44; iter: 200; batch classifier loss: 1.822344; batch adversarial loss: 0.649293\n",
            "epoch 45; iter: 0; batch classifier loss: 2.047259; batch adversarial loss: 0.632861\n",
            "epoch 45; iter: 200; batch classifier loss: 1.944686; batch adversarial loss: 0.638354\n",
            "epoch 46; iter: 0; batch classifier loss: 2.658214; batch adversarial loss: 0.665818\n",
            "epoch 46; iter: 200; batch classifier loss: 2.421614; batch adversarial loss: 0.654742\n",
            "epoch 47; iter: 0; batch classifier loss: 1.950728; batch adversarial loss: 0.616359\n",
            "epoch 47; iter: 200; batch classifier loss: 3.041522; batch adversarial loss: 0.583360\n",
            "epoch 48; iter: 0; batch classifier loss: 2.783020; batch adversarial loss: 0.649388\n",
            "epoch 48; iter: 200; batch classifier loss: 1.616195; batch adversarial loss: 0.643869\n",
            "epoch 49; iter: 0; batch classifier loss: 2.320298; batch adversarial loss: 0.610938\n",
            "epoch 49; iter: 200; batch classifier loss: 1.916829; batch adversarial loss: 0.665864\n",
            "\n",
            " Welcome to split number [0.9] \n",
            "\n",
            "training data size (43957, 18)\n",
            "dataset feature names ['race', 'sex', 'Age (decade)=10', 'Age (decade)=20', 'Age (decade)=30', 'Age (decade)=40', 'Age (decade)=50', 'Age (decade)=60', 'Age (decade)=>=70', 'Education Years=6', 'Education Years=7', 'Education Years=8', 'Education Years=9', 'Education Years=10', 'Education Years=11', 'Education Years=12', 'Education Years=<6', 'Education Years=>12']\n",
            "\n",
            " Welcome to Fold number 0 \n",
            "\n",
            "epoch 0; iter: 0; batch classifier loss: 0.626351; batch adversarial loss: 0.773077\n",
            "epoch 0; iter: 200; batch classifier loss: 0.491872; batch adversarial loss: 0.684903\n",
            "epoch 1; iter: 0; batch classifier loss: 0.414449; batch adversarial loss: 0.650323\n",
            "epoch 1; iter: 200; batch classifier loss: 0.432356; batch adversarial loss: 0.649526\n",
            "epoch 2; iter: 0; batch classifier loss: 0.458453; batch adversarial loss: 0.669516\n",
            "epoch 2; iter: 200; batch classifier loss: 0.473510; batch adversarial loss: 0.626000\n",
            "epoch 3; iter: 0; batch classifier loss: 0.501454; batch adversarial loss: 0.638531\n",
            "epoch 3; iter: 200; batch classifier loss: 0.376163; batch adversarial loss: 0.654966\n",
            "epoch 4; iter: 0; batch classifier loss: 0.390581; batch adversarial loss: 0.648427\n",
            "epoch 4; iter: 200; batch classifier loss: 0.542737; batch adversarial loss: 0.583901\n",
            "epoch 5; iter: 0; batch classifier loss: 0.426054; batch adversarial loss: 0.589702\n",
            "epoch 5; iter: 200; batch classifier loss: 0.426134; batch adversarial loss: 0.528089\n",
            "epoch 6; iter: 0; batch classifier loss: 0.421030; batch adversarial loss: 0.579312\n",
            "epoch 6; iter: 200; batch classifier loss: 0.453143; batch adversarial loss: 0.534221\n",
            "epoch 7; iter: 0; batch classifier loss: 0.450441; batch adversarial loss: 0.599232\n",
            "epoch 7; iter: 200; batch classifier loss: 0.359020; batch adversarial loss: 0.604011\n",
            "epoch 8; iter: 0; batch classifier loss: 0.337556; batch adversarial loss: 0.591050\n",
            "epoch 8; iter: 200; batch classifier loss: 0.433194; batch adversarial loss: 0.565155\n",
            "epoch 9; iter: 0; batch classifier loss: 0.420244; batch adversarial loss: 0.628339\n",
            "epoch 9; iter: 200; batch classifier loss: 0.465618; batch adversarial loss: 0.534624\n",
            "epoch 10; iter: 0; batch classifier loss: 0.417660; batch adversarial loss: 0.592529\n",
            "epoch 10; iter: 200; batch classifier loss: 0.390473; batch adversarial loss: 0.604590\n",
            "epoch 11; iter: 0; batch classifier loss: 0.439355; batch adversarial loss: 0.550480\n",
            "epoch 11; iter: 200; batch classifier loss: 0.426960; batch adversarial loss: 0.534654\n",
            "epoch 12; iter: 0; batch classifier loss: 0.379555; batch adversarial loss: 0.609958\n",
            "epoch 12; iter: 200; batch classifier loss: 0.382182; batch adversarial loss: 0.545374\n",
            "epoch 13; iter: 0; batch classifier loss: 0.408955; batch adversarial loss: 0.586331\n",
            "epoch 13; iter: 200; batch classifier loss: 0.350283; batch adversarial loss: 0.552452\n",
            "epoch 14; iter: 0; batch classifier loss: 0.414111; batch adversarial loss: 0.575611\n",
            "epoch 14; iter: 200; batch classifier loss: 0.410415; batch adversarial loss: 0.597344\n",
            "epoch 15; iter: 0; batch classifier loss: 0.381309; batch adversarial loss: 0.574532\n",
            "epoch 15; iter: 200; batch classifier loss: 0.365720; batch adversarial loss: 0.648467\n",
            "epoch 16; iter: 0; batch classifier loss: 0.461218; batch adversarial loss: 0.629051\n",
            "epoch 16; iter: 200; batch classifier loss: 0.458128; batch adversarial loss: 0.589716\n",
            "epoch 17; iter: 0; batch classifier loss: 0.343989; batch adversarial loss: 0.541398\n",
            "epoch 17; iter: 200; batch classifier loss: 0.440776; batch adversarial loss: 0.557035\n",
            "epoch 18; iter: 0; batch classifier loss: 0.382084; batch adversarial loss: 0.500894\n",
            "epoch 18; iter: 200; batch classifier loss: 0.431959; batch adversarial loss: 0.505070\n",
            "epoch 19; iter: 0; batch classifier loss: 0.351445; batch adversarial loss: 0.600483\n",
            "epoch 19; iter: 200; batch classifier loss: 0.382569; batch adversarial loss: 0.564292\n",
            "epoch 20; iter: 0; batch classifier loss: 0.476379; batch adversarial loss: 0.518419\n",
            "epoch 20; iter: 200; batch classifier loss: 0.422842; batch adversarial loss: 0.523956\n",
            "epoch 21; iter: 0; batch classifier loss: 0.461199; batch adversarial loss: 0.539973\n",
            "epoch 21; iter: 200; batch classifier loss: 0.412421; batch adversarial loss: 0.515689\n",
            "epoch 22; iter: 0; batch classifier loss: 0.468945; batch adversarial loss: 0.577356\n",
            "epoch 22; iter: 200; batch classifier loss: 0.367735; batch adversarial loss: 0.536044\n",
            "epoch 23; iter: 0; batch classifier loss: 0.484556; batch adversarial loss: 0.505097\n",
            "epoch 23; iter: 200; batch classifier loss: 0.396966; batch adversarial loss: 0.511147\n",
            "epoch 24; iter: 0; batch classifier loss: 0.429224; batch adversarial loss: 0.556393\n",
            "epoch 24; iter: 200; batch classifier loss: 0.408350; batch adversarial loss: 0.558926\n",
            "epoch 25; iter: 0; batch classifier loss: 0.562526; batch adversarial loss: 0.524874\n",
            "epoch 25; iter: 200; batch classifier loss: 0.400614; batch adversarial loss: 0.545952\n",
            "epoch 26; iter: 0; batch classifier loss: 0.382189; batch adversarial loss: 0.602697\n",
            "epoch 26; iter: 200; batch classifier loss: 0.512605; batch adversarial loss: 0.540728\n",
            "epoch 27; iter: 0; batch classifier loss: 0.400704; batch adversarial loss: 0.538970\n",
            "epoch 27; iter: 200; batch classifier loss: 0.373834; batch adversarial loss: 0.500580\n",
            "epoch 28; iter: 0; batch classifier loss: 0.447500; batch adversarial loss: 0.550250\n",
            "epoch 28; iter: 200; batch classifier loss: 0.454164; batch adversarial loss: 0.566253\n",
            "epoch 29; iter: 0; batch classifier loss: 0.446772; batch adversarial loss: 0.521968\n",
            "epoch 29; iter: 200; batch classifier loss: 0.395028; batch adversarial loss: 0.564876\n",
            "epoch 30; iter: 0; batch classifier loss: 0.342382; batch adversarial loss: 0.568347\n",
            "epoch 30; iter: 200; batch classifier loss: 0.434631; batch adversarial loss: 0.523192\n",
            "epoch 31; iter: 0; batch classifier loss: 0.435425; batch adversarial loss: 0.629744\n",
            "epoch 31; iter: 200; batch classifier loss: 0.403739; batch adversarial loss: 0.510256\n",
            "epoch 32; iter: 0; batch classifier loss: 0.362718; batch adversarial loss: 0.527473\n",
            "epoch 32; iter: 200; batch classifier loss: 0.449507; batch adversarial loss: 0.544098\n",
            "epoch 33; iter: 0; batch classifier loss: 0.455817; batch adversarial loss: 0.660281\n",
            "epoch 33; iter: 200; batch classifier loss: 0.374383; batch adversarial loss: 0.548065\n",
            "epoch 34; iter: 0; batch classifier loss: 0.371320; batch adversarial loss: 0.582026\n",
            "epoch 34; iter: 200; batch classifier loss: 0.315388; batch adversarial loss: 0.556126\n",
            "epoch 35; iter: 0; batch classifier loss: 0.333507; batch adversarial loss: 0.551478\n",
            "epoch 35; iter: 200; batch classifier loss: 0.389647; batch adversarial loss: 0.536445\n",
            "epoch 36; iter: 0; batch classifier loss: 0.350480; batch adversarial loss: 0.523479\n",
            "epoch 36; iter: 200; batch classifier loss: 0.386820; batch adversarial loss: 0.508550\n",
            "epoch 37; iter: 0; batch classifier loss: 0.445216; batch adversarial loss: 0.477036\n",
            "epoch 37; iter: 200; batch classifier loss: 0.361384; batch adversarial loss: 0.490759\n",
            "epoch 38; iter: 0; batch classifier loss: 0.410728; batch adversarial loss: 0.496312\n",
            "epoch 38; iter: 200; batch classifier loss: 0.380584; batch adversarial loss: 0.471270\n",
            "epoch 39; iter: 0; batch classifier loss: 0.363475; batch adversarial loss: 0.511660\n",
            "epoch 39; iter: 200; batch classifier loss: 0.363579; batch adversarial loss: 0.536884\n",
            "epoch 40; iter: 0; batch classifier loss: 0.446525; batch adversarial loss: 0.490692\n",
            "epoch 40; iter: 200; batch classifier loss: 0.483552; batch adversarial loss: 0.489878\n",
            "epoch 41; iter: 0; batch classifier loss: 0.377362; batch adversarial loss: 0.517309\n",
            "epoch 41; iter: 200; batch classifier loss: 0.455022; batch adversarial loss: 0.508515\n",
            "epoch 42; iter: 0; batch classifier loss: 0.354461; batch adversarial loss: 0.502103\n",
            "epoch 42; iter: 200; batch classifier loss: 0.422897; batch adversarial loss: 0.538328\n",
            "epoch 43; iter: 0; batch classifier loss: 0.445652; batch adversarial loss: 0.535032\n",
            "epoch 43; iter: 200; batch classifier loss: 0.399041; batch adversarial loss: 0.486612\n",
            "epoch 44; iter: 0; batch classifier loss: 0.414968; batch adversarial loss: 0.495796\n",
            "epoch 44; iter: 200; batch classifier loss: 0.418799; batch adversarial loss: 0.506100\n",
            "epoch 45; iter: 0; batch classifier loss: 0.502784; batch adversarial loss: 0.571357\n",
            "epoch 45; iter: 200; batch classifier loss: 0.394655; batch adversarial loss: 0.560894\n",
            "epoch 46; iter: 0; batch classifier loss: 0.400459; batch adversarial loss: 0.511900\n",
            "epoch 46; iter: 200; batch classifier loss: 0.494731; batch adversarial loss: 0.571734\n",
            "epoch 47; iter: 0; batch classifier loss: 0.419049; batch adversarial loss: 0.470063\n",
            "epoch 47; iter: 200; batch classifier loss: 0.399616; batch adversarial loss: 0.487505\n",
            "epoch 48; iter: 0; batch classifier loss: 0.398030; batch adversarial loss: 0.564141\n",
            "epoch 48; iter: 200; batch classifier loss: 0.375252; batch adversarial loss: 0.493318\n",
            "epoch 49; iter: 0; batch classifier loss: 0.433996; batch adversarial loss: 0.421269\n",
            "epoch 49; iter: 200; batch classifier loss: 0.366361; batch adversarial loss: 0.573732\n",
            "epoch 0; iter: 0; batch classifier loss: 0.675351; batch adversarial loss: 0.621974\n",
            "epoch 0; iter: 200; batch classifier loss: 0.422304; batch adversarial loss: 0.657941\n",
            "epoch 1; iter: 0; batch classifier loss: 0.399003; batch adversarial loss: 0.618271\n",
            "epoch 1; iter: 200; batch classifier loss: 0.444077; batch adversarial loss: 0.624927\n",
            "epoch 2; iter: 0; batch classifier loss: 0.376577; batch adversarial loss: 0.631523\n",
            "epoch 2; iter: 200; batch classifier loss: 0.430155; batch adversarial loss: 0.614473\n",
            "epoch 3; iter: 0; batch classifier loss: 0.377756; batch adversarial loss: 0.647163\n",
            "epoch 3; iter: 200; batch classifier loss: 0.407202; batch adversarial loss: 0.635814\n",
            "epoch 4; iter: 0; batch classifier loss: 0.468568; batch adversarial loss: 0.658866\n",
            "epoch 4; iter: 200; batch classifier loss: 0.371875; batch adversarial loss: 0.582535\n",
            "epoch 5; iter: 0; batch classifier loss: 0.423615; batch adversarial loss: 0.627118\n",
            "epoch 5; iter: 200; batch classifier loss: 0.440415; batch adversarial loss: 0.544662\n",
            "epoch 6; iter: 0; batch classifier loss: 0.440537; batch adversarial loss: 0.579413\n",
            "epoch 6; iter: 200; batch classifier loss: 0.414352; batch adversarial loss: 0.579375\n",
            "epoch 7; iter: 0; batch classifier loss: 0.435974; batch adversarial loss: 0.554902\n",
            "epoch 7; iter: 200; batch classifier loss: 0.406794; batch adversarial loss: 0.530966\n",
            "epoch 8; iter: 0; batch classifier loss: 0.375522; batch adversarial loss: 0.526293\n",
            "epoch 8; iter: 200; batch classifier loss: 0.374895; batch adversarial loss: 0.496550\n",
            "epoch 9; iter: 0; batch classifier loss: 0.381243; batch adversarial loss: 0.495429\n",
            "epoch 9; iter: 200; batch classifier loss: 0.407226; batch adversarial loss: 0.558534\n",
            "epoch 10; iter: 0; batch classifier loss: 0.396911; batch adversarial loss: 0.523703\n",
            "epoch 10; iter: 200; batch classifier loss: 0.330907; batch adversarial loss: 0.557825\n",
            "epoch 11; iter: 0; batch classifier loss: 0.380412; batch adversarial loss: 0.534550\n",
            "epoch 11; iter: 200; batch classifier loss: 0.414199; batch adversarial loss: 0.523683\n",
            "epoch 12; iter: 0; batch classifier loss: 0.439491; batch adversarial loss: 0.480017\n",
            "epoch 12; iter: 200; batch classifier loss: 0.438062; batch adversarial loss: 0.425443\n",
            "epoch 13; iter: 0; batch classifier loss: 0.427556; batch adversarial loss: 0.507957\n",
            "epoch 13; iter: 200; batch classifier loss: 0.428078; batch adversarial loss: 0.566281\n",
            "epoch 14; iter: 0; batch classifier loss: 0.390998; batch adversarial loss: 0.561427\n",
            "epoch 14; iter: 200; batch classifier loss: 0.442088; batch adversarial loss: 0.463722\n",
            "epoch 15; iter: 0; batch classifier loss: 0.421674; batch adversarial loss: 0.565107\n",
            "epoch 15; iter: 200; batch classifier loss: 0.376217; batch adversarial loss: 0.555895\n",
            "epoch 16; iter: 0; batch classifier loss: 0.381697; batch adversarial loss: 0.543455\n",
            "epoch 16; iter: 200; batch classifier loss: 0.356488; batch adversarial loss: 0.514081\n",
            "epoch 17; iter: 0; batch classifier loss: 0.361629; batch adversarial loss: 0.516956\n",
            "epoch 17; iter: 200; batch classifier loss: 0.387509; batch adversarial loss: 0.512145\n",
            "epoch 18; iter: 0; batch classifier loss: 0.499265; batch adversarial loss: 0.521650\n",
            "epoch 18; iter: 200; batch classifier loss: 0.410804; batch adversarial loss: 0.592569\n",
            "epoch 19; iter: 0; batch classifier loss: 0.454082; batch adversarial loss: 0.522112\n",
            "epoch 19; iter: 200; batch classifier loss: 0.441467; batch adversarial loss: 0.517796\n",
            "epoch 20; iter: 0; batch classifier loss: 0.499217; batch adversarial loss: 0.530439\n",
            "epoch 20; iter: 200; batch classifier loss: 0.387748; batch adversarial loss: 0.512364\n",
            "epoch 21; iter: 0; batch classifier loss: 0.429293; batch adversarial loss: 0.557470\n",
            "epoch 21; iter: 200; batch classifier loss: 0.460009; batch adversarial loss: 0.529465\n",
            "epoch 22; iter: 0; batch classifier loss: 0.477412; batch adversarial loss: 0.479973\n",
            "epoch 22; iter: 200; batch classifier loss: 0.453367; batch adversarial loss: 0.566164\n",
            "epoch 23; iter: 0; batch classifier loss: 0.367508; batch adversarial loss: 0.528840\n",
            "epoch 23; iter: 200; batch classifier loss: 0.417485; batch adversarial loss: 0.617905\n",
            "epoch 24; iter: 0; batch classifier loss: 0.415075; batch adversarial loss: 0.610293\n",
            "epoch 24; iter: 200; batch classifier loss: 0.409985; batch adversarial loss: 0.542251\n",
            "epoch 25; iter: 0; batch classifier loss: 0.406309; batch adversarial loss: 0.560320\n",
            "epoch 25; iter: 200; batch classifier loss: 0.375319; batch adversarial loss: 0.625569\n",
            "epoch 26; iter: 0; batch classifier loss: 0.440888; batch adversarial loss: 0.668294\n",
            "epoch 26; iter: 200; batch classifier loss: 0.405864; batch adversarial loss: 0.519307\n",
            "epoch 27; iter: 0; batch classifier loss: 0.451521; batch adversarial loss: 0.580016\n",
            "epoch 27; iter: 200; batch classifier loss: 0.355318; batch adversarial loss: 0.586525\n",
            "epoch 28; iter: 0; batch classifier loss: 0.364545; batch adversarial loss: 0.525753\n",
            "epoch 28; iter: 200; batch classifier loss: 0.428091; batch adversarial loss: 0.587941\n",
            "epoch 29; iter: 0; batch classifier loss: 0.435800; batch adversarial loss: 0.510408\n",
            "epoch 29; iter: 200; batch classifier loss: 0.386558; batch adversarial loss: 0.547966\n",
            "epoch 30; iter: 0; batch classifier loss: 0.391052; batch adversarial loss: 0.464011\n",
            "epoch 30; iter: 200; batch classifier loss: 0.439583; batch adversarial loss: 0.501497\n",
            "epoch 31; iter: 0; batch classifier loss: 0.434281; batch adversarial loss: 0.521810\n",
            "epoch 31; iter: 200; batch classifier loss: 0.347433; batch adversarial loss: 0.537896\n",
            "epoch 32; iter: 0; batch classifier loss: 0.408548; batch adversarial loss: 0.572041\n",
            "epoch 32; iter: 200; batch classifier loss: 0.487336; batch adversarial loss: 0.470815\n",
            "epoch 33; iter: 0; batch classifier loss: 0.380485; batch adversarial loss: 0.534382\n",
            "epoch 33; iter: 200; batch classifier loss: 0.454271; batch adversarial loss: 0.530945\n",
            "epoch 34; iter: 0; batch classifier loss: 0.418884; batch adversarial loss: 0.477364\n",
            "epoch 34; iter: 200; batch classifier loss: 0.332101; batch adversarial loss: 0.518689\n",
            "epoch 35; iter: 0; batch classifier loss: 0.372518; batch adversarial loss: 0.521437\n",
            "epoch 35; iter: 200; batch classifier loss: 0.431248; batch adversarial loss: 0.510674\n",
            "epoch 36; iter: 0; batch classifier loss: 0.477031; batch adversarial loss: 0.516096\n",
            "epoch 36; iter: 200; batch classifier loss: 0.415617; batch adversarial loss: 0.503613\n",
            "epoch 37; iter: 0; batch classifier loss: 0.556810; batch adversarial loss: 0.385059\n",
            "epoch 37; iter: 200; batch classifier loss: 0.384918; batch adversarial loss: 0.500984\n",
            "epoch 38; iter: 0; batch classifier loss: 0.426821; batch adversarial loss: 0.473277\n",
            "epoch 38; iter: 200; batch classifier loss: 0.400994; batch adversarial loss: 0.456696\n",
            "epoch 39; iter: 0; batch classifier loss: 0.449572; batch adversarial loss: 0.476886\n",
            "epoch 39; iter: 200; batch classifier loss: 0.530584; batch adversarial loss: 0.507491\n",
            "epoch 40; iter: 0; batch classifier loss: 0.406231; batch adversarial loss: 0.483313\n",
            "epoch 40; iter: 200; batch classifier loss: 0.452448; batch adversarial loss: 0.483669\n",
            "epoch 41; iter: 0; batch classifier loss: 0.482332; batch adversarial loss: 0.457397\n",
            "epoch 41; iter: 200; batch classifier loss: 0.322743; batch adversarial loss: 0.556641\n",
            "epoch 42; iter: 0; batch classifier loss: 0.370952; batch adversarial loss: 0.534995\n",
            "epoch 42; iter: 200; batch classifier loss: 0.418840; batch adversarial loss: 0.553209\n",
            "epoch 43; iter: 0; batch classifier loss: 0.476664; batch adversarial loss: 0.590065\n",
            "epoch 43; iter: 200; batch classifier loss: 0.410251; batch adversarial loss: 0.595723\n",
            "epoch 44; iter: 0; batch classifier loss: 0.366832; batch adversarial loss: 0.556410\n",
            "epoch 44; iter: 200; batch classifier loss: 0.437611; batch adversarial loss: 0.508016\n",
            "epoch 45; iter: 0; batch classifier loss: 0.408225; batch adversarial loss: 0.632895\n",
            "epoch 45; iter: 200; batch classifier loss: 0.495586; batch adversarial loss: 0.551461\n",
            "epoch 46; iter: 0; batch classifier loss: 0.366520; batch adversarial loss: 0.607099\n",
            "epoch 46; iter: 200; batch classifier loss: 0.338666; batch adversarial loss: 0.604987\n",
            "epoch 47; iter: 0; batch classifier loss: 0.493070; batch adversarial loss: 0.634618\n",
            "epoch 47; iter: 200; batch classifier loss: 0.386734; batch adversarial loss: 0.562962\n",
            "epoch 48; iter: 0; batch classifier loss: 0.473182; batch adversarial loss: 0.601669\n",
            "epoch 48; iter: 200; batch classifier loss: 0.359577; batch adversarial loss: 0.504656\n",
            "epoch 49; iter: 0; batch classifier loss: 0.496030; batch adversarial loss: 0.626914\n",
            "epoch 49; iter: 200; batch classifier loss: 0.453461; batch adversarial loss: 0.538340\n",
            "epoch 0; iter: 0; batch classifier loss: 0.695003; batch adversarial loss: 0.635276\n",
            "epoch 0; iter: 200; batch classifier loss: 0.849059; batch adversarial loss: 0.716450\n",
            "epoch 1; iter: 0; batch classifier loss: 0.878241; batch adversarial loss: 0.697899\n",
            "epoch 1; iter: 200; batch classifier loss: 0.309142; batch adversarial loss: 0.650422\n",
            "epoch 2; iter: 0; batch classifier loss: 0.413641; batch adversarial loss: 0.654869\n",
            "epoch 2; iter: 200; batch classifier loss: 0.440731; batch adversarial loss: 0.611889\n",
            "epoch 3; iter: 0; batch classifier loss: 0.449863; batch adversarial loss: 0.627977\n",
            "epoch 3; iter: 200; batch classifier loss: 0.423270; batch adversarial loss: 0.613796\n",
            "epoch 4; iter: 0; batch classifier loss: 0.523337; batch adversarial loss: 0.632404\n",
            "epoch 4; iter: 200; batch classifier loss: 0.568403; batch adversarial loss: 0.588824\n",
            "epoch 5; iter: 0; batch classifier loss: 0.505411; batch adversarial loss: 0.577440\n",
            "epoch 5; iter: 200; batch classifier loss: 0.508868; batch adversarial loss: 0.578567\n",
            "epoch 6; iter: 0; batch classifier loss: 0.434717; batch adversarial loss: 0.610325\n",
            "epoch 6; iter: 200; batch classifier loss: 0.423500; batch adversarial loss: 0.607726\n",
            "epoch 7; iter: 0; batch classifier loss: 0.539158; batch adversarial loss: 0.593557\n",
            "epoch 7; iter: 200; batch classifier loss: 0.446515; batch adversarial loss: 0.673147\n",
            "epoch 8; iter: 0; batch classifier loss: 0.443115; batch adversarial loss: 0.633186\n",
            "epoch 8; iter: 200; batch classifier loss: 0.505772; batch adversarial loss: 0.594845\n",
            "epoch 9; iter: 0; batch classifier loss: 0.503745; batch adversarial loss: 0.689366\n",
            "epoch 9; iter: 200; batch classifier loss: 0.451046; batch adversarial loss: 0.662785\n",
            "epoch 10; iter: 0; batch classifier loss: 0.483105; batch adversarial loss: 0.623238\n",
            "epoch 10; iter: 200; batch classifier loss: 0.529743; batch adversarial loss: 0.624323\n",
            "epoch 11; iter: 0; batch classifier loss: 0.514419; batch adversarial loss: 0.577220\n",
            "epoch 11; iter: 200; batch classifier loss: 0.440676; batch adversarial loss: 0.672302\n",
            "epoch 12; iter: 0; batch classifier loss: 0.481878; batch adversarial loss: 0.589233\n",
            "epoch 12; iter: 200; batch classifier loss: 0.446421; batch adversarial loss: 0.633763\n",
            "epoch 13; iter: 0; batch classifier loss: 0.461374; batch adversarial loss: 0.579037\n",
            "epoch 13; iter: 200; batch classifier loss: 0.438665; batch adversarial loss: 0.614884\n",
            "epoch 14; iter: 0; batch classifier loss: 0.473743; batch adversarial loss: 0.622233\n",
            "epoch 14; iter: 200; batch classifier loss: 0.378548; batch adversarial loss: 0.625941\n",
            "epoch 15; iter: 0; batch classifier loss: 0.504824; batch adversarial loss: 0.553302\n",
            "epoch 15; iter: 200; batch classifier loss: 0.475771; batch adversarial loss: 0.660550\n",
            "epoch 16; iter: 0; batch classifier loss: 0.416997; batch adversarial loss: 0.618653\n",
            "epoch 16; iter: 200; batch classifier loss: 0.508511; batch adversarial loss: 0.579612\n",
            "epoch 17; iter: 0; batch classifier loss: 0.381381; batch adversarial loss: 0.648761\n",
            "epoch 17; iter: 200; batch classifier loss: 0.439125; batch adversarial loss: 0.596598\n",
            "epoch 18; iter: 0; batch classifier loss: 0.478335; batch adversarial loss: 0.614797\n",
            "epoch 18; iter: 200; batch classifier loss: 0.563764; batch adversarial loss: 0.638924\n",
            "epoch 19; iter: 0; batch classifier loss: 0.435618; batch adversarial loss: 0.603772\n",
            "epoch 19; iter: 200; batch classifier loss: 0.433856; batch adversarial loss: 0.571978\n",
            "epoch 20; iter: 0; batch classifier loss: 0.457106; batch adversarial loss: 0.656031\n",
            "epoch 20; iter: 200; batch classifier loss: 0.481624; batch adversarial loss: 0.643870\n",
            "epoch 21; iter: 0; batch classifier loss: 0.483892; batch adversarial loss: 0.664919\n",
            "epoch 21; iter: 200; batch classifier loss: 0.411227; batch adversarial loss: 0.616897\n",
            "epoch 22; iter: 0; batch classifier loss: 0.475308; batch adversarial loss: 0.640015\n",
            "epoch 22; iter: 200; batch classifier loss: 0.584720; batch adversarial loss: 0.674449\n",
            "epoch 23; iter: 0; batch classifier loss: 0.458939; batch adversarial loss: 0.646707\n",
            "epoch 23; iter: 200; batch classifier loss: 0.550203; batch adversarial loss: 0.568989\n",
            "epoch 24; iter: 0; batch classifier loss: 0.482711; batch adversarial loss: 0.574931\n",
            "epoch 24; iter: 200; batch classifier loss: 0.531648; batch adversarial loss: 0.649377\n",
            "epoch 25; iter: 0; batch classifier loss: 0.432135; batch adversarial loss: 0.623552\n",
            "epoch 25; iter: 200; batch classifier loss: 0.490153; batch adversarial loss: 0.598742\n",
            "epoch 26; iter: 0; batch classifier loss: 0.468407; batch adversarial loss: 0.593738\n",
            "epoch 26; iter: 200; batch classifier loss: 0.506034; batch adversarial loss: 0.657531\n",
            "epoch 27; iter: 0; batch classifier loss: 0.359309; batch adversarial loss: 0.619805\n",
            "epoch 27; iter: 200; batch classifier loss: 0.467265; batch adversarial loss: 0.664466\n",
            "epoch 28; iter: 0; batch classifier loss: 0.387599; batch adversarial loss: 0.641888\n",
            "epoch 28; iter: 200; batch classifier loss: 0.418094; batch adversarial loss: 0.605198\n",
            "epoch 29; iter: 0; batch classifier loss: 0.454069; batch adversarial loss: 0.648694\n",
            "epoch 29; iter: 200; batch classifier loss: 0.473740; batch adversarial loss: 0.557586\n",
            "epoch 30; iter: 0; batch classifier loss: 0.524411; batch adversarial loss: 0.603490\n",
            "epoch 30; iter: 200; batch classifier loss: 0.577461; batch adversarial loss: 0.604449\n",
            "epoch 31; iter: 0; batch classifier loss: 0.473309; batch adversarial loss: 0.640423\n",
            "epoch 31; iter: 200; batch classifier loss: 0.497080; batch adversarial loss: 0.628709\n",
            "epoch 32; iter: 0; batch classifier loss: 0.610641; batch adversarial loss: 0.640842\n",
            "epoch 32; iter: 200; batch classifier loss: 0.489034; batch adversarial loss: 0.616647\n",
            "epoch 33; iter: 0; batch classifier loss: 0.500600; batch adversarial loss: 0.603478\n",
            "epoch 33; iter: 200; batch classifier loss: 0.417489; batch adversarial loss: 0.629397\n",
            "epoch 34; iter: 0; batch classifier loss: 0.573730; batch adversarial loss: 0.616436\n",
            "epoch 34; iter: 200; batch classifier loss: 0.518652; batch adversarial loss: 0.568463\n",
            "epoch 35; iter: 0; batch classifier loss: 0.555324; batch adversarial loss: 0.623299\n",
            "epoch 35; iter: 200; batch classifier loss: 0.609023; batch adversarial loss: 0.621182\n",
            "epoch 36; iter: 0; batch classifier loss: 0.497226; batch adversarial loss: 0.599192\n",
            "epoch 36; iter: 200; batch classifier loss: 0.543817; batch adversarial loss: 0.652771\n",
            "epoch 37; iter: 0; batch classifier loss: 0.468109; batch adversarial loss: 0.618448\n",
            "epoch 37; iter: 200; batch classifier loss: 0.482076; batch adversarial loss: 0.599462\n",
            "epoch 38; iter: 0; batch classifier loss: 0.545843; batch adversarial loss: 0.623273\n",
            "epoch 38; iter: 200; batch classifier loss: 0.528166; batch adversarial loss: 0.561085\n",
            "epoch 39; iter: 0; batch classifier loss: 0.381992; batch adversarial loss: 0.642591\n",
            "epoch 39; iter: 200; batch classifier loss: 0.367075; batch adversarial loss: 0.567813\n",
            "epoch 40; iter: 0; batch classifier loss: 0.648646; batch adversarial loss: 0.541309\n",
            "epoch 40; iter: 200; batch classifier loss: 0.594659; batch adversarial loss: 0.619268\n",
            "epoch 41; iter: 0; batch classifier loss: 0.531785; batch adversarial loss: 0.645734\n",
            "epoch 41; iter: 200; batch classifier loss: 0.551973; batch adversarial loss: 0.644289\n",
            "epoch 42; iter: 0; batch classifier loss: 0.364529; batch adversarial loss: 0.634030\n",
            "epoch 42; iter: 200; batch classifier loss: 0.437889; batch adversarial loss: 0.624971\n",
            "epoch 43; iter: 0; batch classifier loss: 0.484311; batch adversarial loss: 0.622192\n",
            "epoch 43; iter: 200; batch classifier loss: 0.517022; batch adversarial loss: 0.673441\n",
            "epoch 44; iter: 0; batch classifier loss: 0.632854; batch adversarial loss: 0.604037\n",
            "epoch 44; iter: 200; batch classifier loss: 0.512766; batch adversarial loss: 0.599927\n",
            "epoch 45; iter: 0; batch classifier loss: 0.503697; batch adversarial loss: 0.623557\n",
            "epoch 45; iter: 200; batch classifier loss: 0.467573; batch adversarial loss: 0.611144\n",
            "epoch 46; iter: 0; batch classifier loss: 0.461275; batch adversarial loss: 0.636876\n",
            "epoch 46; iter: 200; batch classifier loss: 0.419975; batch adversarial loss: 0.599285\n",
            "epoch 47; iter: 0; batch classifier loss: 0.537548; batch adversarial loss: 0.629763\n",
            "epoch 47; iter: 200; batch classifier loss: 0.496899; batch adversarial loss: 0.618578\n",
            "epoch 48; iter: 0; batch classifier loss: 0.581566; batch adversarial loss: 0.653775\n",
            "epoch 48; iter: 200; batch classifier loss: 0.429796; batch adversarial loss: 0.626418\n",
            "epoch 49; iter: 0; batch classifier loss: 0.426284; batch adversarial loss: 0.618358\n",
            "epoch 49; iter: 200; batch classifier loss: 0.528574; batch adversarial loss: 0.646618\n",
            "epoch 0; iter: 0; batch classifier loss: 0.713067; batch adversarial loss: 0.603022\n",
            "epoch 0; iter: 200; batch classifier loss: 1.225905; batch adversarial loss: 0.782801\n",
            "epoch 1; iter: 0; batch classifier loss: 1.222977; batch adversarial loss: 0.707987\n",
            "epoch 1; iter: 200; batch classifier loss: 0.986163; batch adversarial loss: 0.639219\n",
            "epoch 2; iter: 0; batch classifier loss: 1.092162; batch adversarial loss: 0.667330\n",
            "epoch 2; iter: 200; batch classifier loss: 0.789788; batch adversarial loss: 0.621534\n",
            "epoch 3; iter: 0; batch classifier loss: 0.693095; batch adversarial loss: 0.598413\n",
            "epoch 3; iter: 200; batch classifier loss: 0.432659; batch adversarial loss: 0.623291\n",
            "epoch 4; iter: 0; batch classifier loss: 0.425215; batch adversarial loss: 0.644714\n",
            "epoch 4; iter: 200; batch classifier loss: 0.855596; batch adversarial loss: 0.648212\n",
            "epoch 5; iter: 0; batch classifier loss: 0.731433; batch adversarial loss: 0.622275\n",
            "epoch 5; iter: 200; batch classifier loss: 0.800764; batch adversarial loss: 0.611375\n",
            "epoch 6; iter: 0; batch classifier loss: 0.559152; batch adversarial loss: 0.687278\n",
            "epoch 6; iter: 200; batch classifier loss: 0.741166; batch adversarial loss: 0.666643\n",
            "epoch 7; iter: 0; batch classifier loss: 0.578058; batch adversarial loss: 0.612002\n",
            "epoch 7; iter: 200; batch classifier loss: 0.815712; batch adversarial loss: 0.571693\n",
            "epoch 8; iter: 0; batch classifier loss: 0.616760; batch adversarial loss: 0.641283\n",
            "epoch 8; iter: 200; batch classifier loss: 0.695657; batch adversarial loss: 0.645360\n",
            "epoch 9; iter: 0; batch classifier loss: 0.592359; batch adversarial loss: 0.684332\n",
            "epoch 9; iter: 200; batch classifier loss: 0.665752; batch adversarial loss: 0.611336\n",
            "epoch 10; iter: 0; batch classifier loss: 0.850132; batch adversarial loss: 0.611495\n",
            "epoch 10; iter: 200; batch classifier loss: 0.562473; batch adversarial loss: 0.650330\n",
            "epoch 11; iter: 0; batch classifier loss: 0.439812; batch adversarial loss: 0.676718\n",
            "epoch 11; iter: 200; batch classifier loss: 0.549012; batch adversarial loss: 0.672953\n",
            "epoch 12; iter: 0; batch classifier loss: 0.549822; batch adversarial loss: 0.659741\n",
            "epoch 12; iter: 200; batch classifier loss: 0.508517; batch adversarial loss: 0.660469\n",
            "epoch 13; iter: 0; batch classifier loss: 0.518728; batch adversarial loss: 0.688475\n",
            "epoch 13; iter: 200; batch classifier loss: 0.548271; batch adversarial loss: 0.601259\n",
            "epoch 14; iter: 0; batch classifier loss: 0.763610; batch adversarial loss: 0.633147\n",
            "epoch 14; iter: 200; batch classifier loss: 0.637725; batch adversarial loss: 0.619270\n",
            "epoch 15; iter: 0; batch classifier loss: 0.762495; batch adversarial loss: 0.691097\n",
            "epoch 15; iter: 200; batch classifier loss: 0.708266; batch adversarial loss: 0.650554\n",
            "epoch 16; iter: 0; batch classifier loss: 0.882659; batch adversarial loss: 0.632021\n",
            "epoch 16; iter: 200; batch classifier loss: 0.882792; batch adversarial loss: 0.694529\n",
            "epoch 17; iter: 0; batch classifier loss: 0.562920; batch adversarial loss: 0.616786\n",
            "epoch 17; iter: 200; batch classifier loss: 0.902800; batch adversarial loss: 0.567977\n",
            "epoch 18; iter: 0; batch classifier loss: 0.656880; batch adversarial loss: 0.594998\n",
            "epoch 18; iter: 200; batch classifier loss: 0.902940; batch adversarial loss: 0.616893\n",
            "epoch 19; iter: 0; batch classifier loss: 0.444975; batch adversarial loss: 0.638746\n",
            "epoch 19; iter: 200; batch classifier loss: 0.827324; batch adversarial loss: 0.670236\n",
            "epoch 20; iter: 0; batch classifier loss: 0.561899; batch adversarial loss: 0.659718\n",
            "epoch 20; iter: 200; batch classifier loss: 0.685735; batch adversarial loss: 0.605179\n",
            "epoch 21; iter: 0; batch classifier loss: 0.561782; batch adversarial loss: 0.601024\n",
            "epoch 21; iter: 200; batch classifier loss: 0.676981; batch adversarial loss: 0.633289\n",
            "epoch 22; iter: 0; batch classifier loss: 0.546576; batch adversarial loss: 0.600097\n",
            "epoch 22; iter: 200; batch classifier loss: 0.899981; batch adversarial loss: 0.588569\n",
            "epoch 23; iter: 0; batch classifier loss: 0.629696; batch adversarial loss: 0.661883\n",
            "epoch 23; iter: 200; batch classifier loss: 0.787421; batch adversarial loss: 0.632239\n",
            "epoch 24; iter: 0; batch classifier loss: 0.615326; batch adversarial loss: 0.632300\n",
            "epoch 24; iter: 200; batch classifier loss: 0.759976; batch adversarial loss: 0.627651\n",
            "epoch 25; iter: 0; batch classifier loss: 0.562862; batch adversarial loss: 0.652210\n",
            "epoch 25; iter: 200; batch classifier loss: 0.640463; batch adversarial loss: 0.617030\n",
            "epoch 26; iter: 0; batch classifier loss: 0.645852; batch adversarial loss: 0.616763\n",
            "epoch 26; iter: 200; batch classifier loss: 0.584187; batch adversarial loss: 0.638251\n",
            "epoch 27; iter: 0; batch classifier loss: 0.724726; batch adversarial loss: 0.637575\n",
            "epoch 27; iter: 200; batch classifier loss: 0.649618; batch adversarial loss: 0.611848\n",
            "epoch 28; iter: 0; batch classifier loss: 0.607292; batch adversarial loss: 0.681544\n",
            "epoch 28; iter: 200; batch classifier loss: 0.502169; batch adversarial loss: 0.632431\n",
            "epoch 29; iter: 0; batch classifier loss: 0.535122; batch adversarial loss: 0.649128\n",
            "epoch 29; iter: 200; batch classifier loss: 0.682300; batch adversarial loss: 0.592745\n",
            "epoch 30; iter: 0; batch classifier loss: 0.582432; batch adversarial loss: 0.599957\n",
            "epoch 30; iter: 200; batch classifier loss: 0.634604; batch adversarial loss: 0.605426\n",
            "epoch 31; iter: 0; batch classifier loss: 0.713148; batch adversarial loss: 0.636640\n",
            "epoch 31; iter: 200; batch classifier loss: 0.498575; batch adversarial loss: 0.635967\n",
            "epoch 32; iter: 0; batch classifier loss: 0.704784; batch adversarial loss: 0.631132\n",
            "epoch 32; iter: 200; batch classifier loss: 0.577402; batch adversarial loss: 0.656553\n",
            "epoch 33; iter: 0; batch classifier loss: 0.538467; batch adversarial loss: 0.624458\n",
            "epoch 33; iter: 200; batch classifier loss: 0.816045; batch adversarial loss: 0.649572\n",
            "epoch 34; iter: 0; batch classifier loss: 0.617506; batch adversarial loss: 0.632407\n",
            "epoch 34; iter: 200; batch classifier loss: 0.667891; batch adversarial loss: 0.679796\n",
            "epoch 35; iter: 0; batch classifier loss: 0.782570; batch adversarial loss: 0.656032\n",
            "epoch 35; iter: 200; batch classifier loss: 0.669512; batch adversarial loss: 0.597533\n",
            "epoch 36; iter: 0; batch classifier loss: 0.620959; batch adversarial loss: 0.658412\n",
            "epoch 36; iter: 200; batch classifier loss: 0.724897; batch adversarial loss: 0.604624\n",
            "epoch 37; iter: 0; batch classifier loss: 0.548731; batch adversarial loss: 0.628220\n",
            "epoch 37; iter: 200; batch classifier loss: 0.826463; batch adversarial loss: 0.610345\n",
            "epoch 38; iter: 0; batch classifier loss: 1.012458; batch adversarial loss: 0.646742\n",
            "epoch 38; iter: 200; batch classifier loss: 0.496938; batch adversarial loss: 0.664637\n",
            "epoch 39; iter: 0; batch classifier loss: 0.568486; batch adversarial loss: 0.674844\n",
            "epoch 39; iter: 200; batch classifier loss: 0.607643; batch adversarial loss: 0.631797\n",
            "epoch 40; iter: 0; batch classifier loss: 0.531293; batch adversarial loss: 0.575174\n",
            "epoch 40; iter: 200; batch classifier loss: 0.674405; batch adversarial loss: 0.662889\n",
            "epoch 41; iter: 0; batch classifier loss: 0.662728; batch adversarial loss: 0.650033\n",
            "epoch 41; iter: 200; batch classifier loss: 0.612743; batch adversarial loss: 0.603837\n",
            "epoch 42; iter: 0; batch classifier loss: 0.588492; batch adversarial loss: 0.732097\n",
            "epoch 42; iter: 200; batch classifier loss: 0.588392; batch adversarial loss: 0.648113\n",
            "epoch 43; iter: 0; batch classifier loss: 0.594173; batch adversarial loss: 0.661264\n",
            "epoch 43; iter: 200; batch classifier loss: 0.815084; batch adversarial loss: 0.677437\n",
            "epoch 44; iter: 0; batch classifier loss: 0.579286; batch adversarial loss: 0.588281\n",
            "epoch 44; iter: 200; batch classifier loss: 0.674738; batch adversarial loss: 0.634292\n",
            "epoch 45; iter: 0; batch classifier loss: 0.906013; batch adversarial loss: 0.617652\n",
            "epoch 45; iter: 200; batch classifier loss: 0.609176; batch adversarial loss: 0.611968\n",
            "epoch 46; iter: 0; batch classifier loss: 0.603375; batch adversarial loss: 0.633089\n",
            "epoch 46; iter: 200; batch classifier loss: 0.600547; batch adversarial loss: 0.615377\n",
            "epoch 47; iter: 0; batch classifier loss: 0.569862; batch adversarial loss: 0.661411\n",
            "epoch 47; iter: 200; batch classifier loss: 0.517123; batch adversarial loss: 0.639205\n",
            "epoch 48; iter: 0; batch classifier loss: 0.652153; batch adversarial loss: 0.619787\n",
            "epoch 48; iter: 200; batch classifier loss: 0.841588; batch adversarial loss: 0.627398\n",
            "epoch 49; iter: 0; batch classifier loss: 0.725196; batch adversarial loss: 0.639043\n",
            "epoch 49; iter: 200; batch classifier loss: 0.586000; batch adversarial loss: 0.573306\n",
            "epoch 0; iter: 0; batch classifier loss: 0.668367; batch adversarial loss: 0.664035\n",
            "epoch 0; iter: 200; batch classifier loss: 1.485455; batch adversarial loss: 0.749838\n",
            "epoch 1; iter: 0; batch classifier loss: 1.522625; batch adversarial loss: 0.738144\n",
            "epoch 1; iter: 200; batch classifier loss: 1.480144; batch adversarial loss: 0.624283\n",
            "epoch 2; iter: 0; batch classifier loss: 1.638968; batch adversarial loss: 0.596365\n",
            "epoch 2; iter: 200; batch classifier loss: 1.818966; batch adversarial loss: 0.612582\n",
            "epoch 3; iter: 0; batch classifier loss: 2.121429; batch adversarial loss: 0.606263\n",
            "epoch 3; iter: 200; batch classifier loss: 1.709734; batch adversarial loss: 0.663686\n",
            "epoch 4; iter: 0; batch classifier loss: 1.798236; batch adversarial loss: 0.663951\n",
            "epoch 4; iter: 200; batch classifier loss: 2.498758; batch adversarial loss: 0.643583\n",
            "epoch 5; iter: 0; batch classifier loss: 2.257722; batch adversarial loss: 0.675535\n",
            "epoch 5; iter: 200; batch classifier loss: 2.287113; batch adversarial loss: 0.622077\n",
            "epoch 6; iter: 0; batch classifier loss: 2.708942; batch adversarial loss: 0.638312\n",
            "epoch 6; iter: 200; batch classifier loss: 2.336901; batch adversarial loss: 0.671022\n",
            "epoch 7; iter: 0; batch classifier loss: 2.467722; batch adversarial loss: 0.643792\n",
            "epoch 7; iter: 200; batch classifier loss: 2.765038; batch adversarial loss: 0.649360\n",
            "epoch 8; iter: 0; batch classifier loss: 2.869607; batch adversarial loss: 0.611062\n",
            "epoch 8; iter: 200; batch classifier loss: 2.240266; batch adversarial loss: 0.589419\n",
            "epoch 9; iter: 0; batch classifier loss: 2.664096; batch adversarial loss: 0.594771\n",
            "epoch 9; iter: 200; batch classifier loss: 1.804871; batch adversarial loss: 0.605497\n",
            "epoch 10; iter: 0; batch classifier loss: 2.756070; batch adversarial loss: 0.670974\n",
            "epoch 10; iter: 200; batch classifier loss: 1.909024; batch adversarial loss: 0.638340\n",
            "epoch 11; iter: 0; batch classifier loss: 2.385036; batch adversarial loss: 0.605485\n",
            "epoch 11; iter: 200; batch classifier loss: 1.693656; batch adversarial loss: 0.660238\n",
            "epoch 12; iter: 0; batch classifier loss: 2.041706; batch adversarial loss: 0.671441\n",
            "epoch 12; iter: 200; batch classifier loss: 1.932247; batch adversarial loss: 0.599768\n",
            "epoch 13; iter: 0; batch classifier loss: 1.977858; batch adversarial loss: 0.654844\n",
            "epoch 13; iter: 200; batch classifier loss: 1.998445; batch adversarial loss: 0.610867\n",
            "epoch 14; iter: 0; batch classifier loss: 2.395227; batch adversarial loss: 0.682171\n",
            "epoch 14; iter: 200; batch classifier loss: 1.822778; batch adversarial loss: 0.610706\n",
            "epoch 15; iter: 0; batch classifier loss: 1.696116; batch adversarial loss: 0.605576\n",
            "epoch 15; iter: 200; batch classifier loss: 1.362473; batch adversarial loss: 0.632873\n",
            "epoch 16; iter: 0; batch classifier loss: 0.996964; batch adversarial loss: 0.704043\n",
            "epoch 16; iter: 200; batch classifier loss: 1.815662; batch adversarial loss: 0.654713\n",
            "epoch 17; iter: 0; batch classifier loss: 1.597432; batch adversarial loss: 0.682710\n",
            "epoch 17; iter: 200; batch classifier loss: 1.833462; batch adversarial loss: 0.594487\n",
            "epoch 18; iter: 0; batch classifier loss: 1.717826; batch adversarial loss: 0.660277\n",
            "epoch 18; iter: 200; batch classifier loss: 1.281517; batch adversarial loss: 0.627379\n",
            "epoch 19; iter: 0; batch classifier loss: 1.085125; batch adversarial loss: 0.638339\n",
            "epoch 19; iter: 200; batch classifier loss: 0.832474; batch adversarial loss: 0.630494\n",
            "epoch 20; iter: 0; batch classifier loss: 1.200176; batch adversarial loss: 0.577888\n",
            "epoch 20; iter: 200; batch classifier loss: 1.464889; batch adversarial loss: 0.644240\n",
            "epoch 21; iter: 0; batch classifier loss: 1.426801; batch adversarial loss: 0.627352\n",
            "epoch 21; iter: 200; batch classifier loss: 1.938827; batch adversarial loss: 0.649271\n",
            "epoch 22; iter: 0; batch classifier loss: 1.382120; batch adversarial loss: 0.616405\n",
            "epoch 22; iter: 200; batch classifier loss: 1.681385; batch adversarial loss: 0.638357\n",
            "epoch 23; iter: 0; batch classifier loss: 1.253515; batch adversarial loss: 0.643877\n",
            "epoch 23; iter: 200; batch classifier loss: 2.076499; batch adversarial loss: 0.556465\n",
            "epoch 24; iter: 0; batch classifier loss: 1.911725; batch adversarial loss: 0.649312\n",
            "epoch 24; iter: 200; batch classifier loss: 2.017775; batch adversarial loss: 0.649345\n",
            "epoch 25; iter: 0; batch classifier loss: 1.644665; batch adversarial loss: 0.621856\n",
            "epoch 25; iter: 200; batch classifier loss: 2.462584; batch adversarial loss: 0.648451\n",
            "epoch 26; iter: 0; batch classifier loss: 1.918056; batch adversarial loss: 0.649135\n",
            "epoch 26; iter: 200; batch classifier loss: 2.592283; batch adversarial loss: 0.583588\n",
            "epoch 27; iter: 0; batch classifier loss: 2.813643; batch adversarial loss: 0.638382\n",
            "epoch 27; iter: 200; batch classifier loss: 2.643674; batch adversarial loss: 0.594549\n",
            "epoch 28; iter: 0; batch classifier loss: 2.416770; batch adversarial loss: 0.671230\n",
            "epoch 28; iter: 200; batch classifier loss: 2.886312; batch adversarial loss: 0.627464\n",
            "epoch 29; iter: 0; batch classifier loss: 3.316093; batch adversarial loss: 0.676740\n",
            "epoch 29; iter: 200; batch classifier loss: 2.843714; batch adversarial loss: 0.577622\n",
            "epoch 30; iter: 0; batch classifier loss: 1.614282; batch adversarial loss: 0.621910\n",
            "epoch 30; iter: 200; batch classifier loss: 1.729568; batch adversarial loss: 0.616513\n",
            "epoch 31; iter: 0; batch classifier loss: 1.659508; batch adversarial loss: 0.660416\n",
            "epoch 31; iter: 200; batch classifier loss: 2.439574; batch adversarial loss: 0.594255\n",
            "epoch 32; iter: 0; batch classifier loss: 3.093889; batch adversarial loss: 0.594442\n",
            "epoch 32; iter: 200; batch classifier loss: 2.473659; batch adversarial loss: 0.632877\n",
            "epoch 33; iter: 0; batch classifier loss: 2.922935; batch adversarial loss: 0.605522\n",
            "epoch 33; iter: 200; batch classifier loss: 2.268575; batch adversarial loss: 0.627430\n",
            "epoch 34; iter: 0; batch classifier loss: 1.876337; batch adversarial loss: 0.577874\n",
            "epoch 34; iter: 200; batch classifier loss: 2.093306; batch adversarial loss: 0.627400\n",
            "epoch 35; iter: 0; batch classifier loss: 2.148857; batch adversarial loss: 0.594412\n",
            "epoch 35; iter: 200; batch classifier loss: 1.840244; batch adversarial loss: 0.567298\n",
            "epoch 36; iter: 0; batch classifier loss: 2.058713; batch adversarial loss: 0.654747\n",
            "epoch 36; iter: 200; batch classifier loss: 1.717249; batch adversarial loss: 0.666060\n",
            "epoch 37; iter: 0; batch classifier loss: 1.952005; batch adversarial loss: 0.600026\n",
            "epoch 37; iter: 200; batch classifier loss: 1.373586; batch adversarial loss: 0.649395\n",
            "epoch 38; iter: 0; batch classifier loss: 1.381635; batch adversarial loss: 0.611174\n",
            "epoch 38; iter: 200; batch classifier loss: 1.503569; batch adversarial loss: 0.632877\n",
            "epoch 39; iter: 0; batch classifier loss: 1.784140; batch adversarial loss: 0.676679\n",
            "epoch 39; iter: 200; batch classifier loss: 1.196721; batch adversarial loss: 0.616634\n",
            "epoch 40; iter: 0; batch classifier loss: 0.647770; batch adversarial loss: 0.673496\n",
            "epoch 40; iter: 200; batch classifier loss: 5.596076; batch adversarial loss: 0.617401\n",
            "epoch 41; iter: 0; batch classifier loss: 5.978803; batch adversarial loss: 0.618099\n",
            "epoch 41; iter: 200; batch classifier loss: 4.633036; batch adversarial loss: 0.603103\n",
            "epoch 42; iter: 0; batch classifier loss: 4.284133; batch adversarial loss: 0.591141\n",
            "epoch 42; iter: 200; batch classifier loss: 4.708790; batch adversarial loss: 0.669344\n",
            "epoch 43; iter: 0; batch classifier loss: 4.593196; batch adversarial loss: 0.592188\n",
            "epoch 43; iter: 200; batch classifier loss: 4.373095; batch adversarial loss: 0.611948\n",
            "epoch 44; iter: 0; batch classifier loss: 4.101110; batch adversarial loss: 0.631206\n",
            "epoch 44; iter: 200; batch classifier loss: 3.641228; batch adversarial loss: 0.579749\n",
            "epoch 45; iter: 0; batch classifier loss: 3.650816; batch adversarial loss: 0.631539\n",
            "epoch 45; iter: 200; batch classifier loss: 3.091534; batch adversarial loss: 0.545173\n",
            "epoch 46; iter: 0; batch classifier loss: 1.693568; batch adversarial loss: 0.620105\n",
            "epoch 46; iter: 200; batch classifier loss: 1.795381; batch adversarial loss: 0.648944\n",
            "epoch 47; iter: 0; batch classifier loss: 1.732111; batch adversarial loss: 0.649747\n",
            "epoch 47; iter: 200; batch classifier loss: 1.835618; batch adversarial loss: 0.617778\n",
            "epoch 48; iter: 0; batch classifier loss: 2.077831; batch adversarial loss: 0.608566\n",
            "epoch 48; iter: 200; batch classifier loss: 2.839898; batch adversarial loss: 0.643699\n",
            "epoch 49; iter: 0; batch classifier loss: 2.330558; batch adversarial loss: 0.627613\n",
            "epoch 49; iter: 200; batch classifier loss: 2.380123; batch adversarial loss: 0.643700\n",
            "\n",
            " Welcome to Fold number 1 \n",
            "\n",
            "epoch 0; iter: 0; batch classifier loss: 0.690186; batch adversarial loss: 0.678846\n",
            "epoch 0; iter: 200; batch classifier loss: 0.419999; batch adversarial loss: 0.660496\n",
            "epoch 1; iter: 0; batch classifier loss: 0.409928; batch adversarial loss: 0.667524\n",
            "epoch 1; iter: 200; batch classifier loss: 0.542101; batch adversarial loss: 0.639860\n",
            "epoch 2; iter: 0; batch classifier loss: 0.494433; batch adversarial loss: 0.630888\n",
            "epoch 2; iter: 200; batch classifier loss: 0.419944; batch adversarial loss: 0.589903\n",
            "epoch 3; iter: 0; batch classifier loss: 0.406022; batch adversarial loss: 0.593911\n",
            "epoch 3; iter: 200; batch classifier loss: 0.371807; batch adversarial loss: 0.666230\n",
            "epoch 4; iter: 0; batch classifier loss: 0.463236; batch adversarial loss: 0.580463\n",
            "epoch 4; iter: 200; batch classifier loss: 0.439222; batch adversarial loss: 0.649986\n",
            "epoch 5; iter: 0; batch classifier loss: 0.481887; batch adversarial loss: 0.586484\n",
            "epoch 5; iter: 200; batch classifier loss: 0.463945; batch adversarial loss: 0.608794\n",
            "epoch 6; iter: 0; batch classifier loss: 0.343455; batch adversarial loss: 0.671416\n",
            "epoch 6; iter: 200; batch classifier loss: 0.343090; batch adversarial loss: 0.555614\n",
            "epoch 7; iter: 0; batch classifier loss: 0.413643; batch adversarial loss: 0.624601\n",
            "epoch 7; iter: 200; batch classifier loss: 0.485705; batch adversarial loss: 0.603760\n",
            "epoch 8; iter: 0; batch classifier loss: 0.417465; batch adversarial loss: 0.619398\n",
            "epoch 8; iter: 200; batch classifier loss: 0.397883; batch adversarial loss: 0.621966\n",
            "epoch 9; iter: 0; batch classifier loss: 0.342400; batch adversarial loss: 0.577180\n",
            "epoch 9; iter: 200; batch classifier loss: 0.375939; batch adversarial loss: 0.582606\n",
            "epoch 10; iter: 0; batch classifier loss: 0.448935; batch adversarial loss: 0.611362\n",
            "epoch 10; iter: 200; batch classifier loss: 0.349247; batch adversarial loss: 0.539880\n",
            "epoch 11; iter: 0; batch classifier loss: 0.442760; batch adversarial loss: 0.614377\n",
            "epoch 11; iter: 200; batch classifier loss: 0.394396; batch adversarial loss: 0.492904\n",
            "epoch 12; iter: 0; batch classifier loss: 0.458791; batch adversarial loss: 0.559725\n",
            "epoch 12; iter: 200; batch classifier loss: 0.407560; batch adversarial loss: 0.529244\n",
            "epoch 13; iter: 0; batch classifier loss: 0.404688; batch adversarial loss: 0.544007\n",
            "epoch 13; iter: 200; batch classifier loss: 0.400961; batch adversarial loss: 0.498439\n",
            "epoch 14; iter: 0; batch classifier loss: 0.465641; batch adversarial loss: 0.538501\n",
            "epoch 14; iter: 200; batch classifier loss: 0.391841; batch adversarial loss: 0.528267\n",
            "epoch 15; iter: 0; batch classifier loss: 0.352149; batch adversarial loss: 0.496289\n",
            "epoch 15; iter: 200; batch classifier loss: 0.378872; batch adversarial loss: 0.513627\n",
            "epoch 16; iter: 0; batch classifier loss: 0.461601; batch adversarial loss: 0.477699\n",
            "epoch 16; iter: 200; batch classifier loss: 0.413623; batch adversarial loss: 0.546301\n",
            "epoch 17; iter: 0; batch classifier loss: 0.411794; batch adversarial loss: 0.522409\n",
            "epoch 17; iter: 200; batch classifier loss: 0.455544; batch adversarial loss: 0.467495\n",
            "epoch 18; iter: 0; batch classifier loss: 0.440707; batch adversarial loss: 0.437547\n",
            "epoch 18; iter: 200; batch classifier loss: 0.353236; batch adversarial loss: 0.506075\n",
            "epoch 19; iter: 0; batch classifier loss: 0.433837; batch adversarial loss: 0.445821\n",
            "epoch 19; iter: 200; batch classifier loss: 0.522080; batch adversarial loss: 0.474795\n",
            "epoch 20; iter: 0; batch classifier loss: 0.447426; batch adversarial loss: 0.533961\n",
            "epoch 20; iter: 200; batch classifier loss: 0.455645; batch adversarial loss: 0.517553\n",
            "epoch 21; iter: 0; batch classifier loss: 0.433287; batch adversarial loss: 0.497614\n",
            "epoch 21; iter: 200; batch classifier loss: 0.368453; batch adversarial loss: 0.597686\n",
            "epoch 22; iter: 0; batch classifier loss: 0.427123; batch adversarial loss: 0.569338\n",
            "epoch 22; iter: 200; batch classifier loss: 0.451657; batch adversarial loss: 0.569016\n",
            "epoch 23; iter: 0; batch classifier loss: 0.459748; batch adversarial loss: 0.503801\n",
            "epoch 23; iter: 200; batch classifier loss: 0.395894; batch adversarial loss: 0.521126\n",
            "epoch 24; iter: 0; batch classifier loss: 0.457373; batch adversarial loss: 0.515215\n",
            "epoch 24; iter: 200; batch classifier loss: 0.296433; batch adversarial loss: 0.578859\n",
            "epoch 25; iter: 0; batch classifier loss: 0.515554; batch adversarial loss: 0.615509\n",
            "epoch 25; iter: 200; batch classifier loss: 0.444695; batch adversarial loss: 0.541376\n",
            "epoch 26; iter: 0; batch classifier loss: 0.384701; batch adversarial loss: 0.569466\n",
            "epoch 26; iter: 200; batch classifier loss: 0.487032; batch adversarial loss: 0.517174\n",
            "epoch 27; iter: 0; batch classifier loss: 0.437931; batch adversarial loss: 0.487036\n",
            "epoch 27; iter: 200; batch classifier loss: 0.437834; batch adversarial loss: 0.477982\n",
            "epoch 28; iter: 0; batch classifier loss: 0.386786; batch adversarial loss: 0.528579\n",
            "epoch 28; iter: 200; batch classifier loss: 0.413697; batch adversarial loss: 0.480197\n",
            "epoch 29; iter: 0; batch classifier loss: 0.455938; batch adversarial loss: 0.462730\n",
            "epoch 29; iter: 200; batch classifier loss: 0.357593; batch adversarial loss: 0.514517\n",
            "epoch 30; iter: 0; batch classifier loss: 0.402986; batch adversarial loss: 0.531537\n",
            "epoch 30; iter: 200; batch classifier loss: 0.526547; batch adversarial loss: 0.518705\n",
            "epoch 31; iter: 0; batch classifier loss: 0.371239; batch adversarial loss: 0.549716\n",
            "epoch 31; iter: 200; batch classifier loss: 0.586928; batch adversarial loss: 0.508594\n",
            "epoch 32; iter: 0; batch classifier loss: 0.424623; batch adversarial loss: 0.481214\n",
            "epoch 32; iter: 200; batch classifier loss: 0.434404; batch adversarial loss: 0.404710\n",
            "epoch 33; iter: 0; batch classifier loss: 0.397419; batch adversarial loss: 0.510945\n",
            "epoch 33; iter: 200; batch classifier loss: 0.516882; batch adversarial loss: 0.462002\n",
            "epoch 34; iter: 0; batch classifier loss: 0.469521; batch adversarial loss: 0.529850\n",
            "epoch 34; iter: 200; batch classifier loss: 0.392131; batch adversarial loss: 0.514778\n",
            "epoch 35; iter: 0; batch classifier loss: 0.411924; batch adversarial loss: 0.538634\n",
            "epoch 35; iter: 200; batch classifier loss: 0.431268; batch adversarial loss: 0.560305\n",
            "epoch 36; iter: 0; batch classifier loss: 0.498921; batch adversarial loss: 0.584658\n",
            "epoch 36; iter: 200; batch classifier loss: 0.456615; batch adversarial loss: 0.513770\n",
            "epoch 37; iter: 0; batch classifier loss: 0.410897; batch adversarial loss: 0.520758\n",
            "epoch 37; iter: 200; batch classifier loss: 0.399804; batch adversarial loss: 0.530033\n",
            "epoch 38; iter: 0; batch classifier loss: 0.437983; batch adversarial loss: 0.541338\n",
            "epoch 38; iter: 200; batch classifier loss: 0.471904; batch adversarial loss: 0.491401\n",
            "epoch 39; iter: 0; batch classifier loss: 0.352755; batch adversarial loss: 0.504132\n",
            "epoch 39; iter: 200; batch classifier loss: 0.476882; batch adversarial loss: 0.524710\n",
            "epoch 40; iter: 0; batch classifier loss: 0.464981; batch adversarial loss: 0.467543\n",
            "epoch 40; iter: 200; batch classifier loss: 0.449846; batch adversarial loss: 0.521333\n",
            "epoch 41; iter: 0; batch classifier loss: 0.403860; batch adversarial loss: 0.564799\n",
            "epoch 41; iter: 200; batch classifier loss: 0.445031; batch adversarial loss: 0.555807\n",
            "epoch 42; iter: 0; batch classifier loss: 0.391676; batch adversarial loss: 0.594353\n",
            "epoch 42; iter: 200; batch classifier loss: 0.415840; batch adversarial loss: 0.516576\n",
            "epoch 43; iter: 0; batch classifier loss: 0.403276; batch adversarial loss: 0.507630\n",
            "epoch 43; iter: 200; batch classifier loss: 0.484863; batch adversarial loss: 0.519829\n",
            "epoch 44; iter: 0; batch classifier loss: 0.458207; batch adversarial loss: 0.592318\n",
            "epoch 44; iter: 200; batch classifier loss: 0.424474; batch adversarial loss: 0.546760\n",
            "epoch 45; iter: 0; batch classifier loss: 0.461053; batch adversarial loss: 0.566023\n",
            "epoch 45; iter: 200; batch classifier loss: 0.400839; batch adversarial loss: 0.528969\n",
            "epoch 46; iter: 0; batch classifier loss: 0.382392; batch adversarial loss: 0.558813\n",
            "epoch 46; iter: 200; batch classifier loss: 0.352103; batch adversarial loss: 0.593218\n",
            "epoch 47; iter: 0; batch classifier loss: 0.371786; batch adversarial loss: 0.599010\n",
            "epoch 47; iter: 200; batch classifier loss: 0.404553; batch adversarial loss: 0.541400\n",
            "epoch 48; iter: 0; batch classifier loss: 0.334632; batch adversarial loss: 0.603458\n",
            "epoch 48; iter: 200; batch classifier loss: 0.427627; batch adversarial loss: 0.485960\n",
            "epoch 49; iter: 0; batch classifier loss: 0.492086; batch adversarial loss: 0.494862\n",
            "epoch 49; iter: 200; batch classifier loss: 0.426226; batch adversarial loss: 0.543591\n",
            "epoch 0; iter: 0; batch classifier loss: 0.642225; batch adversarial loss: 0.852468\n",
            "epoch 0; iter: 200; batch classifier loss: 0.440702; batch adversarial loss: 0.809024\n",
            "epoch 1; iter: 0; batch classifier loss: 0.420406; batch adversarial loss: 0.808518\n",
            "epoch 1; iter: 200; batch classifier loss: 0.467812; batch adversarial loss: 0.713191\n",
            "epoch 2; iter: 0; batch classifier loss: 0.662313; batch adversarial loss: 0.713130\n",
            "epoch 2; iter: 200; batch classifier loss: 0.591207; batch adversarial loss: 0.618758\n",
            "epoch 3; iter: 0; batch classifier loss: 0.416480; batch adversarial loss: 0.588459\n",
            "epoch 3; iter: 200; batch classifier loss: 0.461778; batch adversarial loss: 0.587549\n",
            "epoch 4; iter: 0; batch classifier loss: 0.371812; batch adversarial loss: 0.618622\n",
            "epoch 4; iter: 200; batch classifier loss: 0.377650; batch adversarial loss: 0.638160\n",
            "epoch 5; iter: 0; batch classifier loss: 0.377727; batch adversarial loss: 0.630738\n",
            "epoch 5; iter: 200; batch classifier loss: 0.396924; batch adversarial loss: 0.592313\n",
            "epoch 6; iter: 0; batch classifier loss: 0.426254; batch adversarial loss: 0.640889\n",
            "epoch 6; iter: 200; batch classifier loss: 0.432019; batch adversarial loss: 0.624419\n",
            "epoch 7; iter: 0; batch classifier loss: 0.428588; batch adversarial loss: 0.628943\n",
            "epoch 7; iter: 200; batch classifier loss: 0.413721; batch adversarial loss: 0.621420\n",
            "epoch 8; iter: 0; batch classifier loss: 0.472173; batch adversarial loss: 0.600715\n",
            "epoch 8; iter: 200; batch classifier loss: 0.447520; batch adversarial loss: 0.644588\n",
            "epoch 9; iter: 0; batch classifier loss: 0.440931; batch adversarial loss: 0.613686\n",
            "epoch 9; iter: 200; batch classifier loss: 0.486102; batch adversarial loss: 0.621948\n",
            "epoch 10; iter: 0; batch classifier loss: 0.396722; batch adversarial loss: 0.625963\n",
            "epoch 10; iter: 200; batch classifier loss: 0.407626; batch adversarial loss: 0.619311\n",
            "epoch 11; iter: 0; batch classifier loss: 0.422780; batch adversarial loss: 0.590474\n",
            "epoch 11; iter: 200; batch classifier loss: 0.424924; batch adversarial loss: 0.614774\n",
            "epoch 12; iter: 0; batch classifier loss: 0.494215; batch adversarial loss: 0.561414\n",
            "epoch 12; iter: 200; batch classifier loss: 0.476098; batch adversarial loss: 0.614664\n",
            "epoch 13; iter: 0; batch classifier loss: 0.471962; batch adversarial loss: 0.586181\n",
            "epoch 13; iter: 200; batch classifier loss: 0.313098; batch adversarial loss: 0.636880\n",
            "epoch 14; iter: 0; batch classifier loss: 0.435844; batch adversarial loss: 0.540995\n",
            "epoch 14; iter: 200; batch classifier loss: 0.368849; batch adversarial loss: 0.584305\n",
            "epoch 15; iter: 0; batch classifier loss: 0.482683; batch adversarial loss: 0.546204\n",
            "epoch 15; iter: 200; batch classifier loss: 0.404883; batch adversarial loss: 0.616221\n",
            "epoch 16; iter: 0; batch classifier loss: 0.411017; batch adversarial loss: 0.603123\n",
            "epoch 16; iter: 200; batch classifier loss: 0.438923; batch adversarial loss: 0.560581\n",
            "epoch 17; iter: 0; batch classifier loss: 0.509140; batch adversarial loss: 0.611236\n",
            "epoch 17; iter: 200; batch classifier loss: 0.357384; batch adversarial loss: 0.561172\n",
            "epoch 18; iter: 0; batch classifier loss: 0.416848; batch adversarial loss: 0.571068\n",
            "epoch 18; iter: 200; batch classifier loss: 0.427063; batch adversarial loss: 0.550747\n",
            "epoch 19; iter: 0; batch classifier loss: 0.444211; batch adversarial loss: 0.599453\n",
            "epoch 19; iter: 200; batch classifier loss: 0.415994; batch adversarial loss: 0.560696\n",
            "epoch 20; iter: 0; batch classifier loss: 0.455610; batch adversarial loss: 0.462582\n",
            "epoch 20; iter: 200; batch classifier loss: 0.454943; batch adversarial loss: 0.512935\n",
            "epoch 21; iter: 0; batch classifier loss: 0.439495; batch adversarial loss: 0.527224\n",
            "epoch 21; iter: 200; batch classifier loss: 0.425692; batch adversarial loss: 0.497930\n",
            "epoch 22; iter: 0; batch classifier loss: 0.469998; batch adversarial loss: 0.480328\n",
            "epoch 22; iter: 200; batch classifier loss: 0.441879; batch adversarial loss: 0.506529\n",
            "epoch 23; iter: 0; batch classifier loss: 0.332756; batch adversarial loss: 0.523434\n",
            "epoch 23; iter: 200; batch classifier loss: 0.432552; batch adversarial loss: 0.490579\n",
            "epoch 24; iter: 0; batch classifier loss: 0.504806; batch adversarial loss: 0.468229\n",
            "epoch 24; iter: 200; batch classifier loss: 0.353756; batch adversarial loss: 0.479687\n",
            "epoch 25; iter: 0; batch classifier loss: 0.406223; batch adversarial loss: 0.506905\n",
            "epoch 25; iter: 200; batch classifier loss: 0.395389; batch adversarial loss: 0.524946\n",
            "epoch 26; iter: 0; batch classifier loss: 0.342376; batch adversarial loss: 0.531537\n",
            "epoch 26; iter: 200; batch classifier loss: 0.309952; batch adversarial loss: 0.535626\n",
            "epoch 27; iter: 0; batch classifier loss: 0.461574; batch adversarial loss: 0.497721\n",
            "epoch 27; iter: 200; batch classifier loss: 0.350124; batch adversarial loss: 0.539072\n",
            "epoch 28; iter: 0; batch classifier loss: 0.428336; batch adversarial loss: 0.517060\n",
            "epoch 28; iter: 200; batch classifier loss: 0.361866; batch adversarial loss: 0.502007\n",
            "epoch 29; iter: 0; batch classifier loss: 0.413610; batch adversarial loss: 0.555000\n",
            "epoch 29; iter: 200; batch classifier loss: 0.371327; batch adversarial loss: 0.538532\n",
            "epoch 30; iter: 0; batch classifier loss: 0.412062; batch adversarial loss: 0.552004\n",
            "epoch 30; iter: 200; batch classifier loss: 0.341473; batch adversarial loss: 0.559893\n",
            "epoch 31; iter: 0; batch classifier loss: 0.450496; batch adversarial loss: 0.598170\n",
            "epoch 31; iter: 200; batch classifier loss: 0.421720; batch adversarial loss: 0.544298\n",
            "epoch 32; iter: 0; batch classifier loss: 0.449187; batch adversarial loss: 0.597132\n",
            "epoch 32; iter: 200; batch classifier loss: 0.512422; batch adversarial loss: 0.475657\n",
            "epoch 33; iter: 0; batch classifier loss: 0.456921; batch adversarial loss: 0.482704\n",
            "epoch 33; iter: 200; batch classifier loss: 0.374237; batch adversarial loss: 0.561850\n",
            "epoch 34; iter: 0; batch classifier loss: 0.420630; batch adversarial loss: 0.531717\n",
            "epoch 34; iter: 200; batch classifier loss: 0.527864; batch adversarial loss: 0.527148\n",
            "epoch 35; iter: 0; batch classifier loss: 0.442716; batch adversarial loss: 0.509274\n",
            "epoch 35; iter: 200; batch classifier loss: 0.404830; batch adversarial loss: 0.538443\n",
            "epoch 36; iter: 0; batch classifier loss: 0.500396; batch adversarial loss: 0.464195\n",
            "epoch 36; iter: 200; batch classifier loss: 0.487862; batch adversarial loss: 0.450962\n",
            "epoch 37; iter: 0; batch classifier loss: 0.352775; batch adversarial loss: 0.451732\n",
            "epoch 37; iter: 200; batch classifier loss: 0.499696; batch adversarial loss: 0.530485\n",
            "epoch 38; iter: 0; batch classifier loss: 0.418075; batch adversarial loss: 0.477852\n",
            "epoch 38; iter: 200; batch classifier loss: 0.433215; batch adversarial loss: 0.468319\n",
            "epoch 39; iter: 0; batch classifier loss: 0.357784; batch adversarial loss: 0.534356\n",
            "epoch 39; iter: 200; batch classifier loss: 0.472049; batch adversarial loss: 0.505103\n",
            "epoch 40; iter: 0; batch classifier loss: 0.392057; batch adversarial loss: 0.464073\n",
            "epoch 40; iter: 200; batch classifier loss: 0.363680; batch adversarial loss: 0.487222\n",
            "epoch 41; iter: 0; batch classifier loss: 0.452073; batch adversarial loss: 0.429413\n",
            "epoch 41; iter: 200; batch classifier loss: 0.374609; batch adversarial loss: 0.572499\n",
            "epoch 42; iter: 0; batch classifier loss: 0.411902; batch adversarial loss: 0.448884\n",
            "epoch 42; iter: 200; batch classifier loss: 0.316364; batch adversarial loss: 0.544347\n",
            "epoch 43; iter: 0; batch classifier loss: 0.380244; batch adversarial loss: 0.492691\n",
            "epoch 43; iter: 200; batch classifier loss: 0.374740; batch adversarial loss: 0.506078\n",
            "epoch 44; iter: 0; batch classifier loss: 0.483879; batch adversarial loss: 0.458400\n",
            "epoch 44; iter: 200; batch classifier loss: 0.397481; batch adversarial loss: 0.470208\n",
            "epoch 45; iter: 0; batch classifier loss: 0.452401; batch adversarial loss: 0.577762\n",
            "epoch 45; iter: 200; batch classifier loss: 0.358801; batch adversarial loss: 0.524115\n",
            "epoch 46; iter: 0; batch classifier loss: 0.389896; batch adversarial loss: 0.573282\n",
            "epoch 46; iter: 200; batch classifier loss: 0.408066; batch adversarial loss: 0.492947\n",
            "epoch 47; iter: 0; batch classifier loss: 0.402501; batch adversarial loss: 0.593647\n",
            "epoch 47; iter: 200; batch classifier loss: 0.475480; batch adversarial loss: 0.582558\n",
            "epoch 48; iter: 0; batch classifier loss: 0.414079; batch adversarial loss: 0.708118\n",
            "epoch 48; iter: 200; batch classifier loss: 0.457286; batch adversarial loss: 0.601016\n",
            "epoch 49; iter: 0; batch classifier loss: 0.399667; batch adversarial loss: 0.567079\n",
            "epoch 49; iter: 200; batch classifier loss: 0.429710; batch adversarial loss: 0.639784\n",
            "epoch 0; iter: 0; batch classifier loss: 0.668033; batch adversarial loss: 0.795366\n",
            "epoch 0; iter: 200; batch classifier loss: 1.285894; batch adversarial loss: 0.873189\n",
            "epoch 1; iter: 0; batch classifier loss: 1.211874; batch adversarial loss: 0.797159\n",
            "epoch 1; iter: 200; batch classifier loss: 0.591905; batch adversarial loss: 0.662915\n",
            "epoch 2; iter: 0; batch classifier loss: 0.519482; batch adversarial loss: 0.635624\n",
            "epoch 2; iter: 200; batch classifier loss: 0.427346; batch adversarial loss: 0.665976\n",
            "epoch 3; iter: 0; batch classifier loss: 0.444798; batch adversarial loss: 0.635257\n",
            "epoch 3; iter: 200; batch classifier loss: 0.404720; batch adversarial loss: 0.660337\n",
            "epoch 4; iter: 0; batch classifier loss: 0.383473; batch adversarial loss: 0.655222\n",
            "epoch 4; iter: 200; batch classifier loss: 0.373008; batch adversarial loss: 0.639129\n",
            "epoch 5; iter: 0; batch classifier loss: 0.460775; batch adversarial loss: 0.633248\n",
            "epoch 5; iter: 200; batch classifier loss: 0.467482; batch adversarial loss: 0.611370\n",
            "epoch 6; iter: 0; batch classifier loss: 0.512010; batch adversarial loss: 0.692359\n",
            "epoch 6; iter: 200; batch classifier loss: 0.702859; batch adversarial loss: 0.621107\n",
            "epoch 7; iter: 0; batch classifier loss: 0.525898; batch adversarial loss: 0.641336\n",
            "epoch 7; iter: 200; batch classifier loss: 0.538525; batch adversarial loss: 0.653800\n",
            "epoch 8; iter: 0; batch classifier loss: 0.385996; batch adversarial loss: 0.646888\n",
            "epoch 8; iter: 200; batch classifier loss: 0.418386; batch adversarial loss: 0.566619\n",
            "epoch 9; iter: 0; batch classifier loss: 0.462898; batch adversarial loss: 0.626853\n",
            "epoch 9; iter: 200; batch classifier loss: 0.398224; batch adversarial loss: 0.603859\n",
            "epoch 10; iter: 0; batch classifier loss: 0.497837; batch adversarial loss: 0.608902\n",
            "epoch 10; iter: 200; batch classifier loss: 0.392700; batch adversarial loss: 0.667332\n",
            "epoch 11; iter: 0; batch classifier loss: 0.381793; batch adversarial loss: 0.615982\n",
            "epoch 11; iter: 200; batch classifier loss: 0.472457; batch adversarial loss: 0.630766\n",
            "epoch 12; iter: 0; batch classifier loss: 0.467516; batch adversarial loss: 0.620444\n",
            "epoch 12; iter: 200; batch classifier loss: 0.373279; batch adversarial loss: 0.694492\n",
            "epoch 13; iter: 0; batch classifier loss: 0.487329; batch adversarial loss: 0.619762\n",
            "epoch 13; iter: 200; batch classifier loss: 0.400776; batch adversarial loss: 0.636619\n",
            "epoch 14; iter: 0; batch classifier loss: 0.444689; batch adversarial loss: 0.633488\n",
            "epoch 14; iter: 200; batch classifier loss: 0.448029; batch adversarial loss: 0.645552\n",
            "epoch 15; iter: 0; batch classifier loss: 0.402414; batch adversarial loss: 0.590096\n",
            "epoch 15; iter: 200; batch classifier loss: 0.504542; batch adversarial loss: 0.614911\n",
            "epoch 16; iter: 0; batch classifier loss: 0.409618; batch adversarial loss: 0.619789\n",
            "epoch 16; iter: 200; batch classifier loss: 0.422756; batch adversarial loss: 0.618150\n",
            "epoch 17; iter: 0; batch classifier loss: 0.478132; batch adversarial loss: 0.605724\n",
            "epoch 17; iter: 200; batch classifier loss: 0.521287; batch adversarial loss: 0.671173\n",
            "epoch 18; iter: 0; batch classifier loss: 0.418003; batch adversarial loss: 0.676328\n",
            "epoch 18; iter: 200; batch classifier loss: 0.519298; batch adversarial loss: 0.554413\n",
            "epoch 19; iter: 0; batch classifier loss: 0.483916; batch adversarial loss: 0.609884\n",
            "epoch 19; iter: 200; batch classifier loss: 0.413017; batch adversarial loss: 0.662791\n",
            "epoch 20; iter: 0; batch classifier loss: 0.501381; batch adversarial loss: 0.628480\n",
            "epoch 20; iter: 200; batch classifier loss: 0.345139; batch adversarial loss: 0.636251\n",
            "epoch 21; iter: 0; batch classifier loss: 0.495148; batch adversarial loss: 0.655459\n",
            "epoch 21; iter: 200; batch classifier loss: 0.414151; batch adversarial loss: 0.644464\n",
            "epoch 22; iter: 0; batch classifier loss: 0.397565; batch adversarial loss: 0.602421\n",
            "epoch 22; iter: 200; batch classifier loss: 0.468482; batch adversarial loss: 0.597900\n",
            "epoch 23; iter: 0; batch classifier loss: 0.496315; batch adversarial loss: 0.633982\n",
            "epoch 23; iter: 200; batch classifier loss: 0.496813; batch adversarial loss: 0.643770\n",
            "epoch 24; iter: 0; batch classifier loss: 0.434866; batch adversarial loss: 0.618257\n",
            "epoch 24; iter: 200; batch classifier loss: 0.478480; batch adversarial loss: 0.549973\n",
            "epoch 25; iter: 0; batch classifier loss: 0.412769; batch adversarial loss: 0.665618\n",
            "epoch 25; iter: 200; batch classifier loss: 0.376820; batch adversarial loss: 0.632490\n",
            "epoch 26; iter: 0; batch classifier loss: 0.540560; batch adversarial loss: 0.586024\n",
            "epoch 26; iter: 200; batch classifier loss: 0.483435; batch adversarial loss: 0.622291\n",
            "epoch 27; iter: 0; batch classifier loss: 0.568228; batch adversarial loss: 0.577081\n",
            "epoch 27; iter: 200; batch classifier loss: 0.634668; batch adversarial loss: 0.561468\n",
            "epoch 28; iter: 0; batch classifier loss: 0.500566; batch adversarial loss: 0.621614\n",
            "epoch 28; iter: 200; batch classifier loss: 0.470964; batch adversarial loss: 0.578120\n",
            "epoch 29; iter: 0; batch classifier loss: 0.362938; batch adversarial loss: 0.637083\n",
            "epoch 29; iter: 200; batch classifier loss: 0.520990; batch adversarial loss: 0.638956\n",
            "epoch 30; iter: 0; batch classifier loss: 0.514328; batch adversarial loss: 0.565228\n",
            "epoch 30; iter: 200; batch classifier loss: 0.451848; batch adversarial loss: 0.645016\n",
            "epoch 31; iter: 0; batch classifier loss: 0.452244; batch adversarial loss: 0.680600\n",
            "epoch 31; iter: 200; batch classifier loss: 0.487919; batch adversarial loss: 0.550846\n",
            "epoch 32; iter: 0; batch classifier loss: 0.489106; batch adversarial loss: 0.586755\n",
            "epoch 32; iter: 200; batch classifier loss: 0.442415; batch adversarial loss: 0.593273\n",
            "epoch 33; iter: 0; batch classifier loss: 0.436197; batch adversarial loss: 0.618302\n",
            "epoch 33; iter: 200; batch classifier loss: 0.407728; batch adversarial loss: 0.620427\n",
            "epoch 34; iter: 0; batch classifier loss: 0.392526; batch adversarial loss: 0.662615\n",
            "epoch 34; iter: 200; batch classifier loss: 0.547391; batch adversarial loss: 0.631136\n",
            "epoch 35; iter: 0; batch classifier loss: 0.457207; batch adversarial loss: 0.620559\n",
            "epoch 35; iter: 200; batch classifier loss: 0.457982; batch adversarial loss: 0.664108\n",
            "epoch 36; iter: 0; batch classifier loss: 0.342537; batch adversarial loss: 0.642767\n",
            "epoch 36; iter: 200; batch classifier loss: 0.489974; batch adversarial loss: 0.623069\n",
            "epoch 37; iter: 0; batch classifier loss: 0.506000; batch adversarial loss: 0.567593\n",
            "epoch 37; iter: 200; batch classifier loss: 0.470285; batch adversarial loss: 0.616624\n",
            "epoch 38; iter: 0; batch classifier loss: 0.525877; batch adversarial loss: 0.607376\n",
            "epoch 38; iter: 200; batch classifier loss: 0.373856; batch adversarial loss: 0.609453\n",
            "epoch 39; iter: 0; batch classifier loss: 0.400829; batch adversarial loss: 0.642142\n",
            "epoch 39; iter: 200; batch classifier loss: 0.581224; batch adversarial loss: 0.584477\n",
            "epoch 40; iter: 0; batch classifier loss: 0.428228; batch adversarial loss: 0.577461\n",
            "epoch 40; iter: 200; batch classifier loss: 0.517276; batch adversarial loss: 0.638448\n",
            "epoch 41; iter: 0; batch classifier loss: 0.577120; batch adversarial loss: 0.626181\n",
            "epoch 41; iter: 200; batch classifier loss: 0.479874; batch adversarial loss: 0.666777\n",
            "epoch 42; iter: 0; batch classifier loss: 0.546184; batch adversarial loss: 0.605311\n",
            "epoch 42; iter: 200; batch classifier loss: 0.512790; batch adversarial loss: 0.662705\n",
            "epoch 43; iter: 0; batch classifier loss: 0.441431; batch adversarial loss: 0.582723\n",
            "epoch 43; iter: 200; batch classifier loss: 0.453627; batch adversarial loss: 0.611863\n",
            "epoch 44; iter: 0; batch classifier loss: 0.506938; batch adversarial loss: 0.633578\n",
            "epoch 44; iter: 200; batch classifier loss: 0.503778; batch adversarial loss: 0.645898\n",
            "epoch 45; iter: 0; batch classifier loss: 0.496024; batch adversarial loss: 0.614100\n",
            "epoch 45; iter: 200; batch classifier loss: 0.520080; batch adversarial loss: 0.609021\n",
            "epoch 46; iter: 0; batch classifier loss: 0.411472; batch adversarial loss: 0.605554\n",
            "epoch 46; iter: 200; batch classifier loss: 0.493203; batch adversarial loss: 0.612183\n",
            "epoch 47; iter: 0; batch classifier loss: 0.532580; batch adversarial loss: 0.590335\n",
            "epoch 47; iter: 200; batch classifier loss: 0.502224; batch adversarial loss: 0.652375\n",
            "epoch 48; iter: 0; batch classifier loss: 0.500980; batch adversarial loss: 0.605762\n",
            "epoch 48; iter: 200; batch classifier loss: 0.467093; batch adversarial loss: 0.625548\n",
            "epoch 49; iter: 0; batch classifier loss: 0.529917; batch adversarial loss: 0.635347\n",
            "epoch 49; iter: 200; batch classifier loss: 0.470690; batch adversarial loss: 0.578406\n",
            "epoch 0; iter: 0; batch classifier loss: 0.724185; batch adversarial loss: 0.658932\n",
            "epoch 0; iter: 200; batch classifier loss: 1.321371; batch adversarial loss: 0.742902\n",
            "epoch 1; iter: 0; batch classifier loss: 1.254173; batch adversarial loss: 0.687883\n",
            "epoch 1; iter: 200; batch classifier loss: 0.930849; batch adversarial loss: 0.664252\n",
            "epoch 2; iter: 0; batch classifier loss: 1.121297; batch adversarial loss: 0.655644\n",
            "epoch 2; iter: 200; batch classifier loss: 0.943474; batch adversarial loss: 0.660858\n",
            "epoch 3; iter: 0; batch classifier loss: 1.169233; batch adversarial loss: 0.634778\n",
            "epoch 3; iter: 200; batch classifier loss: 0.940816; batch adversarial loss: 0.667642\n",
            "epoch 4; iter: 0; batch classifier loss: 0.992148; batch adversarial loss: 0.668635\n",
            "epoch 4; iter: 200; batch classifier loss: 0.426699; batch adversarial loss: 0.590456\n",
            "epoch 5; iter: 0; batch classifier loss: 0.506560; batch adversarial loss: 0.630544\n",
            "epoch 5; iter: 200; batch classifier loss: 0.824905; batch adversarial loss: 0.655313\n",
            "epoch 6; iter: 0; batch classifier loss: 1.082116; batch adversarial loss: 0.582967\n",
            "epoch 6; iter: 200; batch classifier loss: 1.126946; batch adversarial loss: 0.632223\n",
            "epoch 7; iter: 0; batch classifier loss: 0.918285; batch adversarial loss: 0.643408\n",
            "epoch 7; iter: 200; batch classifier loss: 0.900668; batch adversarial loss: 0.598962\n",
            "epoch 8; iter: 0; batch classifier loss: 0.796931; batch adversarial loss: 0.609259\n",
            "epoch 8; iter: 200; batch classifier loss: 0.854907; batch adversarial loss: 0.581546\n",
            "epoch 9; iter: 0; batch classifier loss: 0.991681; batch adversarial loss: 0.652268\n",
            "epoch 9; iter: 200; batch classifier loss: 0.903046; batch adversarial loss: 0.651299\n",
            "epoch 10; iter: 0; batch classifier loss: 0.986595; batch adversarial loss: 0.617657\n",
            "epoch 10; iter: 200; batch classifier loss: 1.076565; batch adversarial loss: 0.610699\n",
            "epoch 11; iter: 0; batch classifier loss: 0.998289; batch adversarial loss: 0.598767\n",
            "epoch 11; iter: 200; batch classifier loss: 0.803001; batch adversarial loss: 0.615771\n",
            "epoch 12; iter: 0; batch classifier loss: 0.798637; batch adversarial loss: 0.650285\n",
            "epoch 12; iter: 200; batch classifier loss: 0.526704; batch adversarial loss: 0.633611\n",
            "epoch 13; iter: 0; batch classifier loss: 0.898960; batch adversarial loss: 0.606807\n",
            "epoch 13; iter: 200; batch classifier loss: 0.713719; batch adversarial loss: 0.621671\n",
            "epoch 14; iter: 0; batch classifier loss: 0.689224; batch adversarial loss: 0.606071\n",
            "epoch 14; iter: 200; batch classifier loss: 0.639686; batch adversarial loss: 0.632078\n",
            "epoch 15; iter: 0; batch classifier loss: 0.547430; batch adversarial loss: 0.606275\n",
            "epoch 15; iter: 200; batch classifier loss: 0.610739; batch adversarial loss: 0.555156\n",
            "epoch 16; iter: 0; batch classifier loss: 0.610752; batch adversarial loss: 0.587739\n",
            "epoch 16; iter: 200; batch classifier loss: 0.611015; batch adversarial loss: 0.619957\n",
            "epoch 17; iter: 0; batch classifier loss: 0.531119; batch adversarial loss: 0.649260\n",
            "epoch 17; iter: 200; batch classifier loss: 0.619386; batch adversarial loss: 0.679871\n",
            "epoch 18; iter: 0; batch classifier loss: 0.499367; batch adversarial loss: 0.656251\n",
            "epoch 18; iter: 200; batch classifier loss: 0.586047; batch adversarial loss: 0.691327\n",
            "epoch 19; iter: 0; batch classifier loss: 0.612740; batch adversarial loss: 0.713316\n",
            "epoch 19; iter: 200; batch classifier loss: 0.958005; batch adversarial loss: 0.614253\n",
            "epoch 20; iter: 0; batch classifier loss: 0.575649; batch adversarial loss: 0.606737\n",
            "epoch 20; iter: 200; batch classifier loss: 0.705615; batch adversarial loss: 0.650831\n",
            "epoch 21; iter: 0; batch classifier loss: 0.611011; batch adversarial loss: 0.589130\n",
            "epoch 21; iter: 200; batch classifier loss: 0.880882; batch adversarial loss: 0.634014\n",
            "epoch 22; iter: 0; batch classifier loss: 0.650442; batch adversarial loss: 0.648380\n",
            "epoch 22; iter: 200; batch classifier loss: 0.713670; batch adversarial loss: 0.693685\n",
            "epoch 23; iter: 0; batch classifier loss: 0.749444; batch adversarial loss: 0.644325\n",
            "epoch 23; iter: 200; batch classifier loss: 0.615052; batch adversarial loss: 0.656143\n",
            "epoch 24; iter: 0; batch classifier loss: 0.723105; batch adversarial loss: 0.631530\n",
            "epoch 24; iter: 200; batch classifier loss: 0.547843; batch adversarial loss: 0.668660\n",
            "epoch 25; iter: 0; batch classifier loss: 0.705465; batch adversarial loss: 0.623452\n",
            "epoch 25; iter: 200; batch classifier loss: 0.495152; batch adversarial loss: 0.650346\n",
            "epoch 26; iter: 0; batch classifier loss: 0.571984; batch adversarial loss: 0.658173\n",
            "epoch 26; iter: 200; batch classifier loss: 0.563787; batch adversarial loss: 0.639259\n",
            "epoch 27; iter: 0; batch classifier loss: 0.546721; batch adversarial loss: 0.680753\n",
            "epoch 27; iter: 200; batch classifier loss: 0.686365; batch adversarial loss: 0.669914\n",
            "epoch 28; iter: 0; batch classifier loss: 0.460402; batch adversarial loss: 0.651785\n",
            "epoch 28; iter: 200; batch classifier loss: 0.575020; batch adversarial loss: 0.605509\n",
            "epoch 29; iter: 0; batch classifier loss: 0.470190; batch adversarial loss: 0.637133\n",
            "epoch 29; iter: 200; batch classifier loss: 0.637208; batch adversarial loss: 0.633915\n",
            "epoch 30; iter: 0; batch classifier loss: 0.690295; batch adversarial loss: 0.710626\n",
            "epoch 30; iter: 200; batch classifier loss: 0.555706; batch adversarial loss: 0.693170\n",
            "epoch 31; iter: 0; batch classifier loss: 0.609465; batch adversarial loss: 0.660513\n",
            "epoch 31; iter: 200; batch classifier loss: 0.521616; batch adversarial loss: 0.633550\n",
            "epoch 32; iter: 0; batch classifier loss: 0.519671; batch adversarial loss: 0.608555\n",
            "epoch 32; iter: 200; batch classifier loss: 0.673403; batch adversarial loss: 0.696514\n",
            "epoch 33; iter: 0; batch classifier loss: 0.477224; batch adversarial loss: 0.631787\n",
            "epoch 33; iter: 200; batch classifier loss: 0.586338; batch adversarial loss: 0.691165\n",
            "epoch 34; iter: 0; batch classifier loss: 0.629106; batch adversarial loss: 0.632204\n",
            "epoch 34; iter: 200; batch classifier loss: 0.660189; batch adversarial loss: 0.613524\n",
            "epoch 35; iter: 0; batch classifier loss: 0.660592; batch adversarial loss: 0.646795\n",
            "epoch 35; iter: 200; batch classifier loss: 0.427737; batch adversarial loss: 0.662482\n",
            "epoch 36; iter: 0; batch classifier loss: 0.645701; batch adversarial loss: 0.578267\n",
            "epoch 36; iter: 200; batch classifier loss: 0.516181; batch adversarial loss: 0.621153\n",
            "epoch 37; iter: 0; batch classifier loss: 0.599965; batch adversarial loss: 0.617503\n",
            "epoch 37; iter: 200; batch classifier loss: 0.876375; batch adversarial loss: 0.673268\n",
            "epoch 38; iter: 0; batch classifier loss: 0.751760; batch adversarial loss: 0.632416\n",
            "epoch 38; iter: 200; batch classifier loss: 0.617925; batch adversarial loss: 0.615394\n",
            "epoch 39; iter: 0; batch classifier loss: 0.612309; batch adversarial loss: 0.699498\n",
            "epoch 39; iter: 200; batch classifier loss: 0.641588; batch adversarial loss: 0.611540\n",
            "epoch 40; iter: 0; batch classifier loss: 0.604166; batch adversarial loss: 0.606979\n",
            "epoch 40; iter: 200; batch classifier loss: 0.852846; batch adversarial loss: 0.621446\n",
            "epoch 41; iter: 0; batch classifier loss: 0.839994; batch adversarial loss: 0.627298\n",
            "epoch 41; iter: 200; batch classifier loss: 0.548500; batch adversarial loss: 0.623007\n",
            "epoch 42; iter: 0; batch classifier loss: 0.610735; batch adversarial loss: 0.608367\n",
            "epoch 42; iter: 200; batch classifier loss: 0.575706; batch adversarial loss: 0.634149\n",
            "epoch 43; iter: 0; batch classifier loss: 0.524637; batch adversarial loss: 0.650553\n",
            "epoch 43; iter: 200; batch classifier loss: 0.526608; batch adversarial loss: 0.629521\n",
            "epoch 44; iter: 0; batch classifier loss: 0.474966; batch adversarial loss: 0.643596\n",
            "epoch 44; iter: 200; batch classifier loss: 0.629707; batch adversarial loss: 0.586380\n",
            "epoch 45; iter: 0; batch classifier loss: 0.633730; batch adversarial loss: 0.665386\n",
            "epoch 45; iter: 200; batch classifier loss: 0.711365; batch adversarial loss: 0.610465\n",
            "epoch 46; iter: 0; batch classifier loss: 0.609733; batch adversarial loss: 0.647856\n",
            "epoch 46; iter: 200; batch classifier loss: 0.528637; batch adversarial loss: 0.604387\n",
            "epoch 47; iter: 0; batch classifier loss: 0.647402; batch adversarial loss: 0.645396\n",
            "epoch 47; iter: 200; batch classifier loss: 0.609683; batch adversarial loss: 0.603838\n",
            "epoch 48; iter: 0; batch classifier loss: 0.573124; batch adversarial loss: 0.638123\n",
            "epoch 48; iter: 200; batch classifier loss: 0.653527; batch adversarial loss: 0.622318\n",
            "epoch 49; iter: 0; batch classifier loss: 0.681129; batch adversarial loss: 0.670729\n",
            "epoch 49; iter: 200; batch classifier loss: 0.592583; batch adversarial loss: 0.623650\n",
            "epoch 0; iter: 0; batch classifier loss: 0.731899; batch adversarial loss: 0.814230\n",
            "epoch 0; iter: 200; batch classifier loss: 1.321906; batch adversarial loss: 0.812779\n",
            "epoch 1; iter: 0; batch classifier loss: 1.527543; batch adversarial loss: 0.755302\n",
            "epoch 1; iter: 200; batch classifier loss: 1.607673; batch adversarial loss: 0.612965\n",
            "epoch 2; iter: 0; batch classifier loss: 1.233675; batch adversarial loss: 0.587825\n",
            "epoch 2; iter: 200; batch classifier loss: 1.893050; batch adversarial loss: 0.669012\n",
            "epoch 3; iter: 0; batch classifier loss: 1.590230; batch adversarial loss: 0.625240\n",
            "epoch 3; iter: 200; batch classifier loss: 1.608256; batch adversarial loss: 0.665803\n",
            "epoch 4; iter: 0; batch classifier loss: 1.544991; batch adversarial loss: 0.622759\n",
            "epoch 4; iter: 200; batch classifier loss: 1.522954; batch adversarial loss: 0.620225\n",
            "epoch 5; iter: 0; batch classifier loss: 1.108027; batch adversarial loss: 0.633945\n",
            "epoch 5; iter: 200; batch classifier loss: 1.702629; batch adversarial loss: 0.638426\n",
            "epoch 6; iter: 0; batch classifier loss: 1.372871; batch adversarial loss: 0.679381\n",
            "epoch 6; iter: 200; batch classifier loss: 1.410643; batch adversarial loss: 0.654141\n",
            "epoch 7; iter: 0; batch classifier loss: 1.927878; batch adversarial loss: 0.664935\n",
            "epoch 7; iter: 200; batch classifier loss: 1.613713; batch adversarial loss: 0.616811\n",
            "epoch 8; iter: 0; batch classifier loss: 1.445954; batch adversarial loss: 0.621999\n",
            "epoch 8; iter: 200; batch classifier loss: 1.861924; batch adversarial loss: 0.660212\n",
            "epoch 9; iter: 0; batch classifier loss: 1.771785; batch adversarial loss: 0.621973\n",
            "epoch 9; iter: 200; batch classifier loss: 1.716819; batch adversarial loss: 0.627343\n",
            "epoch 10; iter: 0; batch classifier loss: 1.737742; batch adversarial loss: 0.594404\n",
            "epoch 10; iter: 200; batch classifier loss: 1.349787; batch adversarial loss: 0.621828\n",
            "epoch 11; iter: 0; batch classifier loss: 1.662846; batch adversarial loss: 0.665757\n",
            "epoch 11; iter: 200; batch classifier loss: 1.790473; batch adversarial loss: 0.605397\n",
            "epoch 12; iter: 0; batch classifier loss: 1.346382; batch adversarial loss: 0.621845\n",
            "epoch 12; iter: 200; batch classifier loss: 2.276973; batch adversarial loss: 0.671316\n",
            "epoch 13; iter: 0; batch classifier loss: 1.509784; batch adversarial loss: 0.660478\n",
            "epoch 13; iter: 200; batch classifier loss: 1.541933; batch adversarial loss: 0.632862\n",
            "epoch 14; iter: 0; batch classifier loss: 1.619285; batch adversarial loss: 0.649443\n",
            "epoch 14; iter: 200; batch classifier loss: 1.631591; batch adversarial loss: 0.671378\n",
            "epoch 15; iter: 0; batch classifier loss: 1.790294; batch adversarial loss: 0.638386\n",
            "epoch 15; iter: 200; batch classifier loss: 1.416119; batch adversarial loss: 0.693864\n",
            "epoch 16; iter: 0; batch classifier loss: 1.787715; batch adversarial loss: 0.627331\n",
            "epoch 16; iter: 200; batch classifier loss: 1.631678; batch adversarial loss: 0.594325\n",
            "epoch 17; iter: 0; batch classifier loss: 1.498067; batch adversarial loss: 0.671339\n",
            "epoch 17; iter: 200; batch classifier loss: 1.669385; batch adversarial loss: 0.671121\n",
            "epoch 18; iter: 0; batch classifier loss: 1.488882; batch adversarial loss: 0.605365\n",
            "epoch 18; iter: 200; batch classifier loss: 1.673790; batch adversarial loss: 0.610908\n",
            "epoch 19; iter: 0; batch classifier loss: 1.628268; batch adversarial loss: 0.643879\n",
            "epoch 19; iter: 200; batch classifier loss: 1.748611; batch adversarial loss: 0.621848\n",
            "epoch 20; iter: 0; batch classifier loss: 1.432968; batch adversarial loss: 0.638353\n",
            "epoch 20; iter: 200; batch classifier loss: 1.354671; batch adversarial loss: 0.638374\n",
            "epoch 21; iter: 0; batch classifier loss: 1.481142; batch adversarial loss: 0.638368\n",
            "epoch 21; iter: 200; batch classifier loss: 1.643054; batch adversarial loss: 0.605430\n",
            "epoch 22; iter: 0; batch classifier loss: 2.100596; batch adversarial loss: 0.599809\n",
            "epoch 22; iter: 200; batch classifier loss: 1.556909; batch adversarial loss: 0.643914\n",
            "epoch 23; iter: 0; batch classifier loss: 1.559861; batch adversarial loss: 0.660215\n",
            "epoch 23; iter: 200; batch classifier loss: 1.211583; batch adversarial loss: 0.676836\n",
            "epoch 24; iter: 0; batch classifier loss: 1.796171; batch adversarial loss: 0.671335\n",
            "epoch 24; iter: 200; batch classifier loss: 1.570394; batch adversarial loss: 0.627421\n",
            "epoch 25; iter: 0; batch classifier loss: 1.721292; batch adversarial loss: 0.649416\n",
            "epoch 25; iter: 200; batch classifier loss: 1.450405; batch adversarial loss: 0.660697\n",
            "epoch 26; iter: 0; batch classifier loss: 1.672458; batch adversarial loss: 0.627366\n",
            "epoch 26; iter: 200; batch classifier loss: 1.915623; batch adversarial loss: 0.638364\n",
            "epoch 27; iter: 0; batch classifier loss: 1.390346; batch adversarial loss: 0.621864\n",
            "epoch 27; iter: 200; batch classifier loss: 1.513943; batch adversarial loss: 0.655063\n",
            "epoch 28; iter: 0; batch classifier loss: 1.549448; batch adversarial loss: 0.632854\n",
            "epoch 28; iter: 200; batch classifier loss: 1.997382; batch adversarial loss: 0.594390\n",
            "epoch 29; iter: 0; batch classifier loss: 1.414881; batch adversarial loss: 0.654934\n",
            "epoch 29; iter: 200; batch classifier loss: 1.372116; batch adversarial loss: 0.632848\n",
            "epoch 30; iter: 0; batch classifier loss: 1.886765; batch adversarial loss: 0.599908\n",
            "epoch 30; iter: 200; batch classifier loss: 1.295295; batch adversarial loss: 0.595523\n",
            "epoch 31; iter: 0; batch classifier loss: 2.630297; batch adversarial loss: 0.632861\n",
            "epoch 31; iter: 200; batch classifier loss: 3.163340; batch adversarial loss: 0.654863\n",
            "epoch 32; iter: 0; batch classifier loss: 3.000941; batch adversarial loss: 0.610938\n",
            "epoch 32; iter: 200; batch classifier loss: 2.072449; batch adversarial loss: 0.638337\n",
            "epoch 33; iter: 0; batch classifier loss: 2.191441; batch adversarial loss: 0.643874\n",
            "epoch 33; iter: 200; batch classifier loss: 2.597542; batch adversarial loss: 0.611127\n",
            "epoch 34; iter: 0; batch classifier loss: 2.637769; batch adversarial loss: 0.649513\n",
            "epoch 34; iter: 200; batch classifier loss: 1.904660; batch adversarial loss: 0.611052\n",
            "epoch 35; iter: 0; batch classifier loss: 2.089337; batch adversarial loss: 0.632852\n",
            "epoch 35; iter: 200; batch classifier loss: 2.365198; batch adversarial loss: 0.632865\n",
            "epoch 36; iter: 0; batch classifier loss: 1.951417; batch adversarial loss: 0.643927\n",
            "epoch 36; iter: 200; batch classifier loss: 0.673459; batch adversarial loss: 0.605786\n",
            "epoch 37; iter: 0; batch classifier loss: 0.911252; batch adversarial loss: 0.638833\n",
            "epoch 37; iter: 200; batch classifier loss: 1.984427; batch adversarial loss: 0.660432\n",
            "epoch 38; iter: 0; batch classifier loss: 2.243686; batch adversarial loss: 0.621880\n",
            "epoch 38; iter: 200; batch classifier loss: 2.162220; batch adversarial loss: 0.649222\n",
            "epoch 39; iter: 0; batch classifier loss: 1.750846; batch adversarial loss: 0.583409\n",
            "epoch 39; iter: 200; batch classifier loss: 2.037545; batch adversarial loss: 0.627333\n",
            "epoch 40; iter: 0; batch classifier loss: 2.138011; batch adversarial loss: 0.566552\n",
            "epoch 40; iter: 200; batch classifier loss: 2.918464; batch adversarial loss: 0.627339\n",
            "epoch 41; iter: 0; batch classifier loss: 1.661463; batch adversarial loss: 0.632850\n",
            "epoch 41; iter: 200; batch classifier loss: 2.266629; batch adversarial loss: 0.638354\n",
            "epoch 42; iter: 0; batch classifier loss: 2.053871; batch adversarial loss: 0.632857\n",
            "epoch 42; iter: 200; batch classifier loss: 1.729250; batch adversarial loss: 0.610709\n",
            "epoch 43; iter: 0; batch classifier loss: 1.649172; batch adversarial loss: 0.687935\n",
            "epoch 43; iter: 200; batch classifier loss: 1.732047; batch adversarial loss: 0.693512\n",
            "epoch 44; iter: 0; batch classifier loss: 2.210849; batch adversarial loss: 0.588786\n",
            "epoch 44; iter: 200; batch classifier loss: 2.010026; batch adversarial loss: 0.654929\n",
            "epoch 45; iter: 0; batch classifier loss: 1.661128; batch adversarial loss: 0.654898\n",
            "epoch 45; iter: 200; batch classifier loss: 2.301126; batch adversarial loss: 0.616358\n",
            "epoch 46; iter: 0; batch classifier loss: 1.766731; batch adversarial loss: 0.632860\n",
            "epoch 46; iter: 200; batch classifier loss: 1.963216; batch adversarial loss: 0.610943\n",
            "epoch 47; iter: 0; batch classifier loss: 2.480339; batch adversarial loss: 0.676812\n",
            "epoch 47; iter: 200; batch classifier loss: 2.629036; batch adversarial loss: 0.577725\n",
            "epoch 48; iter: 0; batch classifier loss: 2.039395; batch adversarial loss: 0.599745\n",
            "epoch 48; iter: 200; batch classifier loss: 1.961874; batch adversarial loss: 0.588487\n",
            "epoch 49; iter: 0; batch classifier loss: 2.255205; batch adversarial loss: 0.605229\n",
            "epoch 49; iter: 200; batch classifier loss: 2.011202; batch adversarial loss: 0.627333\n",
            "\n",
            " Welcome to Fold number 2 \n",
            "\n",
            "epoch 0; iter: 0; batch classifier loss: 0.719072; batch adversarial loss: 0.635898\n",
            "epoch 0; iter: 200; batch classifier loss: 0.522683; batch adversarial loss: 0.617990\n",
            "epoch 1; iter: 0; batch classifier loss: 0.483502; batch adversarial loss: 0.637180\n",
            "epoch 1; iter: 200; batch classifier loss: 0.346002; batch adversarial loss: 0.618439\n",
            "epoch 2; iter: 0; batch classifier loss: 0.370813; batch adversarial loss: 0.631278\n",
            "epoch 2; iter: 200; batch classifier loss: 0.470160; batch adversarial loss: 0.599999\n",
            "epoch 3; iter: 0; batch classifier loss: 0.491466; batch adversarial loss: 0.509121\n",
            "epoch 3; iter: 200; batch classifier loss: 0.493399; batch adversarial loss: 0.665101\n",
            "epoch 4; iter: 0; batch classifier loss: 0.402929; batch adversarial loss: 0.560376\n",
            "epoch 4; iter: 200; batch classifier loss: 0.395455; batch adversarial loss: 0.569614\n",
            "epoch 5; iter: 0; batch classifier loss: 0.431012; batch adversarial loss: 0.594685\n",
            "epoch 5; iter: 200; batch classifier loss: 0.376376; batch adversarial loss: 0.526758\n",
            "epoch 6; iter: 0; batch classifier loss: 0.494162; batch adversarial loss: 0.526295\n",
            "epoch 6; iter: 200; batch classifier loss: 0.423327; batch adversarial loss: 0.542622\n",
            "epoch 7; iter: 0; batch classifier loss: 0.411314; batch adversarial loss: 0.494537\n",
            "epoch 7; iter: 200; batch classifier loss: 0.452102; batch adversarial loss: 0.603378\n",
            "epoch 8; iter: 0; batch classifier loss: 0.436045; batch adversarial loss: 0.514412\n",
            "epoch 8; iter: 200; batch classifier loss: 0.468927; batch adversarial loss: 0.556952\n",
            "epoch 9; iter: 0; batch classifier loss: 0.434781; batch adversarial loss: 0.456252\n",
            "epoch 9; iter: 200; batch classifier loss: 0.395668; batch adversarial loss: 0.545841\n",
            "epoch 10; iter: 0; batch classifier loss: 0.420240; batch adversarial loss: 0.543598\n",
            "epoch 10; iter: 200; batch classifier loss: 0.452901; batch adversarial loss: 0.555350\n",
            "epoch 11; iter: 0; batch classifier loss: 0.382688; batch adversarial loss: 0.552392\n",
            "epoch 11; iter: 200; batch classifier loss: 0.457171; batch adversarial loss: 0.540323\n",
            "epoch 12; iter: 0; batch classifier loss: 0.492471; batch adversarial loss: 0.607824\n",
            "epoch 12; iter: 200; batch classifier loss: 0.306280; batch adversarial loss: 0.492199\n",
            "epoch 13; iter: 0; batch classifier loss: 0.418837; batch adversarial loss: 0.549828\n",
            "epoch 13; iter: 200; batch classifier loss: 0.478021; batch adversarial loss: 0.506305\n",
            "epoch 14; iter: 0; batch classifier loss: 0.326373; batch adversarial loss: 0.524780\n",
            "epoch 14; iter: 200; batch classifier loss: 0.391683; batch adversarial loss: 0.487387\n",
            "epoch 15; iter: 0; batch classifier loss: 0.483424; batch adversarial loss: 0.467439\n",
            "epoch 15; iter: 200; batch classifier loss: 0.440329; batch adversarial loss: 0.489588\n",
            "epoch 16; iter: 0; batch classifier loss: 0.407395; batch adversarial loss: 0.580019\n",
            "epoch 16; iter: 200; batch classifier loss: 0.425718; batch adversarial loss: 0.473460\n",
            "epoch 17; iter: 0; batch classifier loss: 0.484287; batch adversarial loss: 0.463048\n",
            "epoch 17; iter: 200; batch classifier loss: 0.390842; batch adversarial loss: 0.509064\n",
            "epoch 18; iter: 0; batch classifier loss: 0.447973; batch adversarial loss: 0.477223\n",
            "epoch 18; iter: 200; batch classifier loss: 0.505507; batch adversarial loss: 0.541640\n",
            "epoch 19; iter: 0; batch classifier loss: 0.393632; batch adversarial loss: 0.469228\n",
            "epoch 19; iter: 200; batch classifier loss: 0.406536; batch adversarial loss: 0.515218\n",
            "epoch 20; iter: 0; batch classifier loss: 0.441684; batch adversarial loss: 0.587869\n",
            "epoch 20; iter: 200; batch classifier loss: 0.361525; batch adversarial loss: 0.561828\n",
            "epoch 21; iter: 0; batch classifier loss: 0.356292; batch adversarial loss: 0.550225\n",
            "epoch 21; iter: 200; batch classifier loss: 0.451851; batch adversarial loss: 0.597157\n",
            "epoch 22; iter: 0; batch classifier loss: 0.402825; batch adversarial loss: 0.550008\n",
            "epoch 22; iter: 200; batch classifier loss: 0.521540; batch adversarial loss: 0.568931\n",
            "epoch 23; iter: 0; batch classifier loss: 0.483727; batch adversarial loss: 0.507268\n",
            "epoch 23; iter: 200; batch classifier loss: 0.504098; batch adversarial loss: 0.614883\n",
            "epoch 24; iter: 0; batch classifier loss: 0.392075; batch adversarial loss: 0.592355\n",
            "epoch 24; iter: 200; batch classifier loss: 0.422851; batch adversarial loss: 0.601694\n",
            "epoch 25; iter: 0; batch classifier loss: 0.371978; batch adversarial loss: 0.601799\n",
            "epoch 25; iter: 200; batch classifier loss: 0.465950; batch adversarial loss: 0.520884\n",
            "epoch 26; iter: 0; batch classifier loss: 0.436945; batch adversarial loss: 0.619385\n",
            "epoch 26; iter: 200; batch classifier loss: 0.412416; batch adversarial loss: 0.662813\n",
            "epoch 27; iter: 0; batch classifier loss: 0.531729; batch adversarial loss: 0.537398\n",
            "epoch 27; iter: 200; batch classifier loss: 0.515688; batch adversarial loss: 0.585107\n",
            "epoch 28; iter: 0; batch classifier loss: 0.379651; batch adversarial loss: 0.568311\n",
            "epoch 28; iter: 200; batch classifier loss: 0.387696; batch adversarial loss: 0.477213\n",
            "epoch 29; iter: 0; batch classifier loss: 0.458224; batch adversarial loss: 0.480390\n",
            "epoch 29; iter: 200; batch classifier loss: 0.594537; batch adversarial loss: 0.492860\n",
            "epoch 30; iter: 0; batch classifier loss: 0.354278; batch adversarial loss: 0.514258\n",
            "epoch 30; iter: 200; batch classifier loss: 0.485434; batch adversarial loss: 0.491830\n",
            "epoch 31; iter: 0; batch classifier loss: 0.500151; batch adversarial loss: 0.482131\n",
            "epoch 31; iter: 200; batch classifier loss: 0.379579; batch adversarial loss: 0.546696\n",
            "epoch 32; iter: 0; batch classifier loss: 0.454840; batch adversarial loss: 0.567805\n",
            "epoch 32; iter: 200; batch classifier loss: 0.370533; batch adversarial loss: 0.461692\n",
            "epoch 33; iter: 0; batch classifier loss: 0.444357; batch adversarial loss: 0.438486\n",
            "epoch 33; iter: 200; batch classifier loss: 0.461228; batch adversarial loss: 0.515294\n",
            "epoch 34; iter: 0; batch classifier loss: 0.370743; batch adversarial loss: 0.519722\n",
            "epoch 34; iter: 200; batch classifier loss: 0.413147; batch adversarial loss: 0.502591\n",
            "epoch 35; iter: 0; batch classifier loss: 0.348604; batch adversarial loss: 0.533487\n",
            "epoch 35; iter: 200; batch classifier loss: 0.425756; batch adversarial loss: 0.607718\n",
            "epoch 36; iter: 0; batch classifier loss: 0.359680; batch adversarial loss: 0.475779\n",
            "epoch 36; iter: 200; batch classifier loss: 0.399890; batch adversarial loss: 0.573294\n",
            "epoch 37; iter: 0; batch classifier loss: 0.464478; batch adversarial loss: 0.561090\n",
            "epoch 37; iter: 200; batch classifier loss: 0.433012; batch adversarial loss: 0.573609\n",
            "epoch 38; iter: 0; batch classifier loss: 0.417939; batch adversarial loss: 0.507380\n",
            "epoch 38; iter: 200; batch classifier loss: 0.401615; batch adversarial loss: 0.485191\n",
            "epoch 39; iter: 0; batch classifier loss: 0.460990; batch adversarial loss: 0.567821\n",
            "epoch 39; iter: 200; batch classifier loss: 0.545558; batch adversarial loss: 0.490761\n",
            "epoch 40; iter: 0; batch classifier loss: 0.486208; batch adversarial loss: 0.487381\n",
            "epoch 40; iter: 200; batch classifier loss: 0.310692; batch adversarial loss: 0.586957\n",
            "epoch 41; iter: 0; batch classifier loss: 0.368346; batch adversarial loss: 0.540191\n",
            "epoch 41; iter: 200; batch classifier loss: 0.423768; batch adversarial loss: 0.522443\n",
            "epoch 42; iter: 0; batch classifier loss: 0.344033; batch adversarial loss: 0.541144\n",
            "epoch 42; iter: 200; batch classifier loss: 0.440126; batch adversarial loss: 0.466621\n",
            "epoch 43; iter: 0; batch classifier loss: 0.384268; batch adversarial loss: 0.518105\n",
            "epoch 43; iter: 200; batch classifier loss: 0.422276; batch adversarial loss: 0.564388\n",
            "epoch 44; iter: 0; batch classifier loss: 0.397496; batch adversarial loss: 0.494343\n",
            "epoch 44; iter: 200; batch classifier loss: 0.348793; batch adversarial loss: 0.512488\n",
            "epoch 45; iter: 0; batch classifier loss: 0.457589; batch adversarial loss: 0.584126\n",
            "epoch 45; iter: 200; batch classifier loss: 0.406712; batch adversarial loss: 0.573094\n",
            "epoch 46; iter: 0; batch classifier loss: 0.430842; batch adversarial loss: 0.446480\n",
            "epoch 46; iter: 200; batch classifier loss: 0.409977; batch adversarial loss: 0.521101\n",
            "epoch 47; iter: 0; batch classifier loss: 0.455458; batch adversarial loss: 0.581375\n",
            "epoch 47; iter: 200; batch classifier loss: 0.396111; batch adversarial loss: 0.585624\n",
            "epoch 48; iter: 0; batch classifier loss: 0.384268; batch adversarial loss: 0.589076\n",
            "epoch 48; iter: 200; batch classifier loss: 0.453699; batch adversarial loss: 0.548477\n",
            "epoch 49; iter: 0; batch classifier loss: 0.332029; batch adversarial loss: 0.589226\n",
            "epoch 49; iter: 200; batch classifier loss: 0.420971; batch adversarial loss: 0.514977\n",
            "epoch 0; iter: 0; batch classifier loss: 0.691351; batch adversarial loss: 0.739198\n",
            "epoch 0; iter: 200; batch classifier loss: 0.508200; batch adversarial loss: 0.663444\n",
            "epoch 1; iter: 0; batch classifier loss: 0.463886; batch adversarial loss: 0.653424\n",
            "epoch 1; iter: 200; batch classifier loss: 0.378657; batch adversarial loss: 0.615975\n",
            "epoch 2; iter: 0; batch classifier loss: 0.557754; batch adversarial loss: 0.596831\n",
            "epoch 2; iter: 200; batch classifier loss: 0.411962; batch adversarial loss: 0.592627\n",
            "epoch 3; iter: 0; batch classifier loss: 0.430803; batch adversarial loss: 0.604730\n",
            "epoch 3; iter: 200; batch classifier loss: 0.417216; batch adversarial loss: 0.571220\n",
            "epoch 4; iter: 0; batch classifier loss: 0.363927; batch adversarial loss: 0.606464\n",
            "epoch 4; iter: 200; batch classifier loss: 0.354359; batch adversarial loss: 0.616217\n",
            "epoch 5; iter: 0; batch classifier loss: 0.416879; batch adversarial loss: 0.581854\n",
            "epoch 5; iter: 200; batch classifier loss: 0.417060; batch adversarial loss: 0.582612\n",
            "epoch 6; iter: 0; batch classifier loss: 0.430455; batch adversarial loss: 0.602650\n",
            "epoch 6; iter: 200; batch classifier loss: 0.398145; batch adversarial loss: 0.638456\n",
            "epoch 7; iter: 0; batch classifier loss: 0.401364; batch adversarial loss: 0.607688\n",
            "epoch 7; iter: 200; batch classifier loss: 0.462589; batch adversarial loss: 0.558638\n",
            "epoch 8; iter: 0; batch classifier loss: 0.510040; batch adversarial loss: 0.605247\n",
            "epoch 8; iter: 200; batch classifier loss: 0.398257; batch adversarial loss: 0.554735\n",
            "epoch 9; iter: 0; batch classifier loss: 0.334797; batch adversarial loss: 0.606081\n",
            "epoch 9; iter: 200; batch classifier loss: 0.427908; batch adversarial loss: 0.609150\n",
            "epoch 10; iter: 0; batch classifier loss: 0.486819; batch adversarial loss: 0.563731\n",
            "epoch 10; iter: 200; batch classifier loss: 0.506438; batch adversarial loss: 0.647220\n",
            "epoch 11; iter: 0; batch classifier loss: 0.436157; batch adversarial loss: 0.655824\n",
            "epoch 11; iter: 200; batch classifier loss: 0.344623; batch adversarial loss: 0.600628\n",
            "epoch 12; iter: 0; batch classifier loss: 0.433556; batch adversarial loss: 0.586071\n",
            "epoch 12; iter: 200; batch classifier loss: 0.399449; batch adversarial loss: 0.614508\n",
            "epoch 13; iter: 0; batch classifier loss: 0.467310; batch adversarial loss: 0.630431\n",
            "epoch 13; iter: 200; batch classifier loss: 0.466468; batch adversarial loss: 0.544466\n",
            "epoch 14; iter: 0; batch classifier loss: 0.430651; batch adversarial loss: 0.546633\n",
            "epoch 14; iter: 200; batch classifier loss: 0.403553; batch adversarial loss: 0.584721\n",
            "epoch 15; iter: 0; batch classifier loss: 0.464533; batch adversarial loss: 0.562875\n",
            "epoch 15; iter: 200; batch classifier loss: 0.425723; batch adversarial loss: 0.539890\n",
            "epoch 16; iter: 0; batch classifier loss: 0.433031; batch adversarial loss: 0.591109\n",
            "epoch 16; iter: 200; batch classifier loss: 0.473861; batch adversarial loss: 0.574330\n",
            "epoch 17; iter: 0; batch classifier loss: 0.317582; batch adversarial loss: 0.547373\n",
            "epoch 17; iter: 200; batch classifier loss: 0.511581; batch adversarial loss: 0.550947\n",
            "epoch 18; iter: 0; batch classifier loss: 0.552357; batch adversarial loss: 0.444792\n",
            "epoch 18; iter: 200; batch classifier loss: 0.420291; batch adversarial loss: 0.511714\n",
            "epoch 19; iter: 0; batch classifier loss: 0.483996; batch adversarial loss: 0.498605\n",
            "epoch 19; iter: 200; batch classifier loss: 0.426866; batch adversarial loss: 0.481969\n",
            "epoch 20; iter: 0; batch classifier loss: 0.354224; batch adversarial loss: 0.547599\n",
            "epoch 20; iter: 200; batch classifier loss: 0.413087; batch adversarial loss: 0.610516\n",
            "epoch 21; iter: 0; batch classifier loss: 0.431166; batch adversarial loss: 0.522772\n",
            "epoch 21; iter: 200; batch classifier loss: 0.489489; batch adversarial loss: 0.496467\n",
            "epoch 22; iter: 0; batch classifier loss: 0.383398; batch adversarial loss: 0.571625\n",
            "epoch 22; iter: 200; batch classifier loss: 0.470434; batch adversarial loss: 0.556488\n",
            "epoch 23; iter: 0; batch classifier loss: 0.458344; batch adversarial loss: 0.558106\n",
            "epoch 23; iter: 200; batch classifier loss: 0.475002; batch adversarial loss: 0.561394\n",
            "epoch 24; iter: 0; batch classifier loss: 0.388001; batch adversarial loss: 0.520934\n",
            "epoch 24; iter: 200; batch classifier loss: 0.449676; batch adversarial loss: 0.542077\n",
            "epoch 25; iter: 0; batch classifier loss: 0.433508; batch adversarial loss: 0.546008\n",
            "epoch 25; iter: 200; batch classifier loss: 0.415352; batch adversarial loss: 0.476989\n",
            "epoch 26; iter: 0; batch classifier loss: 0.356359; batch adversarial loss: 0.477094\n",
            "epoch 26; iter: 200; batch classifier loss: 0.384981; batch adversarial loss: 0.537027\n",
            "epoch 27; iter: 0; batch classifier loss: 0.462554; batch adversarial loss: 0.513509\n",
            "epoch 27; iter: 200; batch classifier loss: 0.428802; batch adversarial loss: 0.510176\n",
            "epoch 28; iter: 0; batch classifier loss: 0.466461; batch adversarial loss: 0.553112\n",
            "epoch 28; iter: 200; batch classifier loss: 0.494146; batch adversarial loss: 0.582105\n",
            "epoch 29; iter: 0; batch classifier loss: 0.409423; batch adversarial loss: 0.624171\n",
            "epoch 29; iter: 200; batch classifier loss: 0.419047; batch adversarial loss: 0.551323\n",
            "epoch 30; iter: 0; batch classifier loss: 0.438643; batch adversarial loss: 0.600141\n",
            "epoch 30; iter: 200; batch classifier loss: 0.462167; batch adversarial loss: 0.524552\n",
            "epoch 31; iter: 0; batch classifier loss: 0.396152; batch adversarial loss: 0.552784\n",
            "epoch 31; iter: 200; batch classifier loss: 0.431541; batch adversarial loss: 0.570911\n",
            "epoch 32; iter: 0; batch classifier loss: 0.349352; batch adversarial loss: 0.579714\n",
            "epoch 32; iter: 200; batch classifier loss: 0.504360; batch adversarial loss: 0.500289\n",
            "epoch 33; iter: 0; batch classifier loss: 0.480656; batch adversarial loss: 0.508369\n",
            "epoch 33; iter: 200; batch classifier loss: 0.341587; batch adversarial loss: 0.551009\n",
            "epoch 34; iter: 0; batch classifier loss: 0.424440; batch adversarial loss: 0.574764\n",
            "epoch 34; iter: 200; batch classifier loss: 0.368458; batch adversarial loss: 0.518110\n",
            "epoch 35; iter: 0; batch classifier loss: 0.455978; batch adversarial loss: 0.507954\n",
            "epoch 35; iter: 200; batch classifier loss: 0.381331; batch adversarial loss: 0.548841\n",
            "epoch 36; iter: 0; batch classifier loss: 0.470896; batch adversarial loss: 0.543958\n",
            "epoch 36; iter: 200; batch classifier loss: 0.363833; batch adversarial loss: 0.617155\n",
            "epoch 37; iter: 0; batch classifier loss: 0.417632; batch adversarial loss: 0.588773\n",
            "epoch 37; iter: 200; batch classifier loss: 0.458601; batch adversarial loss: 0.567364\n",
            "epoch 38; iter: 0; batch classifier loss: 0.478676; batch adversarial loss: 0.599033\n",
            "epoch 38; iter: 200; batch classifier loss: 0.434188; batch adversarial loss: 0.498906\n",
            "epoch 39; iter: 0; batch classifier loss: 0.429129; batch adversarial loss: 0.571063\n",
            "epoch 39; iter: 200; batch classifier loss: 0.388743; batch adversarial loss: 0.535356\n",
            "epoch 40; iter: 0; batch classifier loss: 0.397213; batch adversarial loss: 0.496809\n",
            "epoch 40; iter: 200; batch classifier loss: 0.416276; batch adversarial loss: 0.538195\n",
            "epoch 41; iter: 0; batch classifier loss: 0.368828; batch adversarial loss: 0.562856\n",
            "epoch 41; iter: 200; batch classifier loss: 0.430743; batch adversarial loss: 0.607574\n",
            "epoch 42; iter: 0; batch classifier loss: 0.440160; batch adversarial loss: 0.576475\n",
            "epoch 42; iter: 200; batch classifier loss: 0.466952; batch adversarial loss: 0.600967\n",
            "epoch 43; iter: 0; batch classifier loss: 0.495812; batch adversarial loss: 0.496424\n",
            "epoch 43; iter: 200; batch classifier loss: 0.381645; batch adversarial loss: 0.592777\n",
            "epoch 44; iter: 0; batch classifier loss: 0.405010; batch adversarial loss: 0.518015\n",
            "epoch 44; iter: 200; batch classifier loss: 0.339046; batch adversarial loss: 0.533642\n",
            "epoch 45; iter: 0; batch classifier loss: 0.373923; batch adversarial loss: 0.561782\n",
            "epoch 45; iter: 200; batch classifier loss: 0.385641; batch adversarial loss: 0.559132\n",
            "epoch 46; iter: 0; batch classifier loss: 0.424874; batch adversarial loss: 0.554109\n",
            "epoch 46; iter: 200; batch classifier loss: 0.358215; batch adversarial loss: 0.482772\n",
            "epoch 47; iter: 0; batch classifier loss: 0.315941; batch adversarial loss: 0.587866\n",
            "epoch 47; iter: 200; batch classifier loss: 0.475401; batch adversarial loss: 0.569372\n",
            "epoch 48; iter: 0; batch classifier loss: 0.351147; batch adversarial loss: 0.501736\n",
            "epoch 48; iter: 200; batch classifier loss: 0.384665; batch adversarial loss: 0.544449\n",
            "epoch 49; iter: 0; batch classifier loss: 0.356695; batch adversarial loss: 0.510764\n",
            "epoch 49; iter: 200; batch classifier loss: 0.375852; batch adversarial loss: 0.577408\n",
            "epoch 0; iter: 0; batch classifier loss: 0.666929; batch adversarial loss: 0.600047\n",
            "epoch 0; iter: 200; batch classifier loss: 1.172566; batch adversarial loss: 0.910074\n",
            "epoch 1; iter: 0; batch classifier loss: 1.386201; batch adversarial loss: 0.862488\n",
            "epoch 1; iter: 200; batch classifier loss: 1.198915; batch adversarial loss: 0.798383\n",
            "epoch 2; iter: 0; batch classifier loss: 1.213999; batch adversarial loss: 0.756946\n",
            "epoch 2; iter: 200; batch classifier loss: 1.463979; batch adversarial loss: 0.745359\n",
            "epoch 3; iter: 0; batch classifier loss: 1.074522; batch adversarial loss: 0.619870\n",
            "epoch 3; iter: 200; batch classifier loss: 0.476387; batch adversarial loss: 0.641505\n",
            "epoch 4; iter: 0; batch classifier loss: 0.469829; batch adversarial loss: 0.624045\n",
            "epoch 4; iter: 200; batch classifier loss: 0.513470; batch adversarial loss: 0.681731\n",
            "epoch 5; iter: 0; batch classifier loss: 0.421124; batch adversarial loss: 0.615893\n",
            "epoch 5; iter: 200; batch classifier loss: 0.590627; batch adversarial loss: 0.573443\n",
            "epoch 6; iter: 0; batch classifier loss: 0.400125; batch adversarial loss: 0.642818\n",
            "epoch 6; iter: 200; batch classifier loss: 0.474811; batch adversarial loss: 0.597355\n",
            "epoch 7; iter: 0; batch classifier loss: 0.581390; batch adversarial loss: 0.610635\n",
            "epoch 7; iter: 200; batch classifier loss: 0.418522; batch adversarial loss: 0.645316\n",
            "epoch 8; iter: 0; batch classifier loss: 0.400003; batch adversarial loss: 0.617327\n",
            "epoch 8; iter: 200; batch classifier loss: 0.544909; batch adversarial loss: 0.593669\n",
            "epoch 9; iter: 0; batch classifier loss: 0.472458; batch adversarial loss: 0.719985\n",
            "epoch 9; iter: 200; batch classifier loss: 0.446145; batch adversarial loss: 0.639609\n",
            "epoch 10; iter: 0; batch classifier loss: 0.516554; batch adversarial loss: 0.611421\n",
            "epoch 10; iter: 200; batch classifier loss: 0.485320; batch adversarial loss: 0.627501\n",
            "epoch 11; iter: 0; batch classifier loss: 0.518976; batch adversarial loss: 0.617966\n",
            "epoch 11; iter: 200; batch classifier loss: 0.460711; batch adversarial loss: 0.626435\n",
            "epoch 12; iter: 0; batch classifier loss: 0.459045; batch adversarial loss: 0.662088\n",
            "epoch 12; iter: 200; batch classifier loss: 0.326889; batch adversarial loss: 0.664925\n",
            "epoch 13; iter: 0; batch classifier loss: 0.492386; batch adversarial loss: 0.606369\n",
            "epoch 13; iter: 200; batch classifier loss: 0.481725; batch adversarial loss: 0.605256\n",
            "epoch 14; iter: 0; batch classifier loss: 0.424255; batch adversarial loss: 0.587394\n",
            "epoch 14; iter: 200; batch classifier loss: 0.510492; batch adversarial loss: 0.628196\n",
            "epoch 15; iter: 0; batch classifier loss: 0.437733; batch adversarial loss: 0.616242\n",
            "epoch 15; iter: 200; batch classifier loss: 0.465037; batch adversarial loss: 0.649329\n",
            "epoch 16; iter: 0; batch classifier loss: 0.388648; batch adversarial loss: 0.640282\n",
            "epoch 16; iter: 200; batch classifier loss: 0.461508; batch adversarial loss: 0.574266\n",
            "epoch 17; iter: 0; batch classifier loss: 0.357436; batch adversarial loss: 0.632102\n",
            "epoch 17; iter: 200; batch classifier loss: 0.422669; batch adversarial loss: 0.588044\n",
            "epoch 18; iter: 0; batch classifier loss: 0.480354; batch adversarial loss: 0.597643\n",
            "epoch 18; iter: 200; batch classifier loss: 0.394858; batch adversarial loss: 0.616305\n",
            "epoch 19; iter: 0; batch classifier loss: 0.536567; batch adversarial loss: 0.651479\n",
            "epoch 19; iter: 200; batch classifier loss: 0.440246; batch adversarial loss: 0.619162\n",
            "epoch 20; iter: 0; batch classifier loss: 0.424213; batch adversarial loss: 0.599744\n",
            "epoch 20; iter: 200; batch classifier loss: 0.409548; batch adversarial loss: 0.584147\n",
            "epoch 21; iter: 0; batch classifier loss: 0.491831; batch adversarial loss: 0.655904\n",
            "epoch 21; iter: 200; batch classifier loss: 0.403250; batch adversarial loss: 0.628329\n",
            "epoch 22; iter: 0; batch classifier loss: 0.431076; batch adversarial loss: 0.583526\n",
            "epoch 22; iter: 200; batch classifier loss: 0.454544; batch adversarial loss: 0.640414\n",
            "epoch 23; iter: 0; batch classifier loss: 0.539942; batch adversarial loss: 0.599210\n",
            "epoch 23; iter: 200; batch classifier loss: 0.508148; batch adversarial loss: 0.614811\n",
            "epoch 24; iter: 0; batch classifier loss: 0.440043; batch adversarial loss: 0.656129\n",
            "epoch 24; iter: 200; batch classifier loss: 0.513067; batch adversarial loss: 0.615536\n",
            "epoch 25; iter: 0; batch classifier loss: 0.453749; batch adversarial loss: 0.607778\n",
            "epoch 25; iter: 200; batch classifier loss: 0.399688; batch adversarial loss: 0.634844\n",
            "epoch 26; iter: 0; batch classifier loss: 0.466273; batch adversarial loss: 0.621416\n",
            "epoch 26; iter: 200; batch classifier loss: 0.392857; batch adversarial loss: 0.578634\n",
            "epoch 27; iter: 0; batch classifier loss: 0.425077; batch adversarial loss: 0.611310\n",
            "epoch 27; iter: 200; batch classifier loss: 0.568701; batch adversarial loss: 0.629461\n",
            "epoch 28; iter: 0; batch classifier loss: 0.490243; batch adversarial loss: 0.583111\n",
            "epoch 28; iter: 200; batch classifier loss: 0.524748; batch adversarial loss: 0.606042\n",
            "epoch 29; iter: 0; batch classifier loss: 0.491258; batch adversarial loss: 0.608826\n",
            "epoch 29; iter: 200; batch classifier loss: 0.498694; batch adversarial loss: 0.659160\n",
            "epoch 30; iter: 0; batch classifier loss: 0.455046; batch adversarial loss: 0.625652\n",
            "epoch 30; iter: 200; batch classifier loss: 0.461850; batch adversarial loss: 0.619651\n",
            "epoch 31; iter: 0; batch classifier loss: 0.442539; batch adversarial loss: 0.601686\n",
            "epoch 31; iter: 200; batch classifier loss: 0.381803; batch adversarial loss: 0.579457\n",
            "epoch 32; iter: 0; batch classifier loss: 0.530121; batch adversarial loss: 0.617334\n",
            "epoch 32; iter: 200; batch classifier loss: 0.428433; batch adversarial loss: 0.586062\n",
            "epoch 33; iter: 0; batch classifier loss: 0.368594; batch adversarial loss: 0.630168\n",
            "epoch 33; iter: 200; batch classifier loss: 0.537380; batch adversarial loss: 0.600423\n",
            "epoch 34; iter: 0; batch classifier loss: 0.457434; batch adversarial loss: 0.712446\n",
            "epoch 34; iter: 200; batch classifier loss: 0.530949; batch adversarial loss: 0.612821\n",
            "epoch 35; iter: 0; batch classifier loss: 0.423155; batch adversarial loss: 0.660588\n",
            "epoch 35; iter: 200; batch classifier loss: 0.430250; batch adversarial loss: 0.606372\n",
            "epoch 36; iter: 0; batch classifier loss: 0.565436; batch adversarial loss: 0.578726\n",
            "epoch 36; iter: 200; batch classifier loss: 0.578329; batch adversarial loss: 0.587155\n",
            "epoch 37; iter: 0; batch classifier loss: 0.510044; batch adversarial loss: 0.624507\n",
            "epoch 37; iter: 200; batch classifier loss: 0.452626; batch adversarial loss: 0.601686\n",
            "epoch 38; iter: 0; batch classifier loss: 0.438282; batch adversarial loss: 0.654134\n",
            "epoch 38; iter: 200; batch classifier loss: 0.586494; batch adversarial loss: 0.592423\n",
            "epoch 39; iter: 0; batch classifier loss: 0.446258; batch adversarial loss: 0.619843\n",
            "epoch 39; iter: 200; batch classifier loss: 0.442289; batch adversarial loss: 0.582768\n",
            "epoch 40; iter: 0; batch classifier loss: 0.383716; batch adversarial loss: 0.708821\n",
            "epoch 40; iter: 200; batch classifier loss: 0.529881; batch adversarial loss: 0.628654\n",
            "epoch 41; iter: 0; batch classifier loss: 0.383555; batch adversarial loss: 0.631571\n",
            "epoch 41; iter: 200; batch classifier loss: 0.438801; batch adversarial loss: 0.583412\n",
            "epoch 42; iter: 0; batch classifier loss: 0.409924; batch adversarial loss: 0.642031\n",
            "epoch 42; iter: 200; batch classifier loss: 0.549622; batch adversarial loss: 0.676185\n",
            "epoch 43; iter: 0; batch classifier loss: 0.405509; batch adversarial loss: 0.695126\n",
            "epoch 43; iter: 200; batch classifier loss: 0.331894; batch adversarial loss: 0.631799\n",
            "epoch 44; iter: 0; batch classifier loss: 0.482751; batch adversarial loss: 0.669123\n",
            "epoch 44; iter: 200; batch classifier loss: 0.392074; batch adversarial loss: 0.619874\n",
            "epoch 45; iter: 0; batch classifier loss: 0.477068; batch adversarial loss: 0.642492\n",
            "epoch 45; iter: 200; batch classifier loss: 0.566316; batch adversarial loss: 0.624152\n",
            "epoch 46; iter: 0; batch classifier loss: 0.575810; batch adversarial loss: 0.558429\n",
            "epoch 46; iter: 200; batch classifier loss: 0.439263; batch adversarial loss: 0.635471\n",
            "epoch 47; iter: 0; batch classifier loss: 0.454299; batch adversarial loss: 0.570641\n",
            "epoch 47; iter: 200; batch classifier loss: 0.550544; batch adversarial loss: 0.611197\n",
            "epoch 48; iter: 0; batch classifier loss: 0.413432; batch adversarial loss: 0.619924\n",
            "epoch 48; iter: 200; batch classifier loss: 0.549305; batch adversarial loss: 0.658034\n",
            "epoch 49; iter: 0; batch classifier loss: 0.496210; batch adversarial loss: 0.709513\n",
            "epoch 49; iter: 200; batch classifier loss: 0.513641; batch adversarial loss: 0.626929\n",
            "epoch 0; iter: 0; batch classifier loss: 0.671804; batch adversarial loss: 0.670049\n",
            "epoch 0; iter: 200; batch classifier loss: 0.720213; batch adversarial loss: 0.666605\n",
            "epoch 1; iter: 0; batch classifier loss: 0.755090; batch adversarial loss: 0.654324\n",
            "epoch 1; iter: 200; batch classifier loss: 0.914492; batch adversarial loss: 0.617078\n",
            "epoch 2; iter: 0; batch classifier loss: 0.770819; batch adversarial loss: 0.647454\n",
            "epoch 2; iter: 200; batch classifier loss: 0.844900; batch adversarial loss: 0.619722\n",
            "epoch 3; iter: 0; batch classifier loss: 0.968868; batch adversarial loss: 0.666444\n",
            "epoch 3; iter: 200; batch classifier loss: 0.984031; batch adversarial loss: 0.643288\n",
            "epoch 4; iter: 0; batch classifier loss: 0.902903; batch adversarial loss: 0.706686\n",
            "epoch 4; iter: 200; batch classifier loss: 1.031787; batch adversarial loss: 0.627474\n",
            "epoch 5; iter: 0; batch classifier loss: 1.132371; batch adversarial loss: 0.603636\n",
            "epoch 5; iter: 200; batch classifier loss: 1.197723; batch adversarial loss: 0.624635\n",
            "epoch 6; iter: 0; batch classifier loss: 1.059298; batch adversarial loss: 0.650293\n",
            "epoch 6; iter: 200; batch classifier loss: 0.868262; batch adversarial loss: 0.635818\n",
            "epoch 7; iter: 0; batch classifier loss: 1.022819; batch adversarial loss: 0.676628\n",
            "epoch 7; iter: 200; batch classifier loss: 0.672440; batch adversarial loss: 0.617473\n",
            "epoch 8; iter: 0; batch classifier loss: 0.526134; batch adversarial loss: 0.612152\n",
            "epoch 8; iter: 200; batch classifier loss: 0.541547; batch adversarial loss: 0.640169\n",
            "epoch 9; iter: 0; batch classifier loss: 0.551110; batch adversarial loss: 0.671674\n",
            "epoch 9; iter: 200; batch classifier loss: 0.756923; batch adversarial loss: 0.675512\n",
            "epoch 10; iter: 0; batch classifier loss: 0.744591; batch adversarial loss: 0.648890\n",
            "epoch 10; iter: 200; batch classifier loss: 0.689906; batch adversarial loss: 0.641762\n",
            "epoch 11; iter: 0; batch classifier loss: 0.856353; batch adversarial loss: 0.627487\n",
            "epoch 11; iter: 200; batch classifier loss: 0.740785; batch adversarial loss: 0.630153\n",
            "epoch 12; iter: 0; batch classifier loss: 0.987012; batch adversarial loss: 0.585983\n",
            "epoch 12; iter: 200; batch classifier loss: 0.614808; batch adversarial loss: 0.624921\n",
            "epoch 13; iter: 0; batch classifier loss: 0.655886; batch adversarial loss: 0.652528\n",
            "epoch 13; iter: 200; batch classifier loss: 0.598831; batch adversarial loss: 0.650491\n",
            "epoch 14; iter: 0; batch classifier loss: 0.553218; batch adversarial loss: 0.610102\n",
            "epoch 14; iter: 200; batch classifier loss: 0.570295; batch adversarial loss: 0.630491\n",
            "epoch 15; iter: 0; batch classifier loss: 0.608712; batch adversarial loss: 0.575045\n",
            "epoch 15; iter: 200; batch classifier loss: 0.559635; batch adversarial loss: 0.612507\n",
            "epoch 16; iter: 0; batch classifier loss: 0.574167; batch adversarial loss: 0.635967\n",
            "epoch 16; iter: 200; batch classifier loss: 0.497945; batch adversarial loss: 0.596220\n",
            "epoch 17; iter: 0; batch classifier loss: 0.515620; batch adversarial loss: 0.620544\n",
            "epoch 17; iter: 200; batch classifier loss: 0.578406; batch adversarial loss: 0.633650\n",
            "epoch 18; iter: 0; batch classifier loss: 0.515550; batch adversarial loss: 0.654468\n",
            "epoch 18; iter: 200; batch classifier loss: 0.589850; batch adversarial loss: 0.660293\n",
            "epoch 19; iter: 0; batch classifier loss: 0.630093; batch adversarial loss: 0.636885\n",
            "epoch 19; iter: 200; batch classifier loss: 0.471494; batch adversarial loss: 0.639169\n",
            "epoch 20; iter: 0; batch classifier loss: 0.580543; batch adversarial loss: 0.625677\n",
            "epoch 20; iter: 200; batch classifier loss: 0.608568; batch adversarial loss: 0.614960\n",
            "epoch 21; iter: 0; batch classifier loss: 0.552903; batch adversarial loss: 0.594229\n",
            "epoch 21; iter: 200; batch classifier loss: 0.617671; batch adversarial loss: 0.567882\n",
            "epoch 22; iter: 0; batch classifier loss: 0.522559; batch adversarial loss: 0.643829\n",
            "epoch 22; iter: 200; batch classifier loss: 0.628585; batch adversarial loss: 0.600890\n",
            "epoch 23; iter: 0; batch classifier loss: 0.550285; batch adversarial loss: 0.644004\n",
            "epoch 23; iter: 200; batch classifier loss: 0.870942; batch adversarial loss: 0.623401\n",
            "epoch 24; iter: 0; batch classifier loss: 0.548651; batch adversarial loss: 0.681627\n",
            "epoch 24; iter: 200; batch classifier loss: 0.696802; batch adversarial loss: 0.664457\n",
            "epoch 25; iter: 0; batch classifier loss: 0.617208; batch adversarial loss: 0.638168\n",
            "epoch 25; iter: 200; batch classifier loss: 0.522465; batch adversarial loss: 0.632366\n",
            "epoch 26; iter: 0; batch classifier loss: 0.701778; batch adversarial loss: 0.607251\n",
            "epoch 26; iter: 200; batch classifier loss: 0.443331; batch adversarial loss: 0.628495\n",
            "epoch 27; iter: 0; batch classifier loss: 0.535974; batch adversarial loss: 0.608031\n",
            "epoch 27; iter: 200; batch classifier loss: 0.501741; batch adversarial loss: 0.611987\n",
            "epoch 28; iter: 0; batch classifier loss: 0.569536; batch adversarial loss: 0.632158\n",
            "epoch 28; iter: 200; batch classifier loss: 0.716904; batch adversarial loss: 0.649587\n",
            "epoch 29; iter: 0; batch classifier loss: 0.450858; batch adversarial loss: 0.640594\n",
            "epoch 29; iter: 200; batch classifier loss: 0.593788; batch adversarial loss: 0.663768\n",
            "epoch 30; iter: 0; batch classifier loss: 0.789552; batch adversarial loss: 0.605289\n",
            "epoch 30; iter: 200; batch classifier loss: 0.686284; batch adversarial loss: 0.632007\n",
            "epoch 31; iter: 0; batch classifier loss: 0.691981; batch adversarial loss: 0.628770\n",
            "epoch 31; iter: 200; batch classifier loss: 0.519414; batch adversarial loss: 0.681042\n",
            "epoch 32; iter: 0; batch classifier loss: 0.462103; batch adversarial loss: 0.628742\n",
            "epoch 32; iter: 200; batch classifier loss: 0.495263; batch adversarial loss: 0.639649\n",
            "epoch 33; iter: 0; batch classifier loss: 0.538854; batch adversarial loss: 0.624304\n",
            "epoch 33; iter: 200; batch classifier loss: 0.693967; batch adversarial loss: 0.650259\n",
            "epoch 34; iter: 0; batch classifier loss: 0.616975; batch adversarial loss: 0.659110\n",
            "epoch 34; iter: 200; batch classifier loss: 0.581869; batch adversarial loss: 0.643125\n",
            "epoch 35; iter: 0; batch classifier loss: 0.745495; batch adversarial loss: 0.605234\n",
            "epoch 35; iter: 200; batch classifier loss: 0.521616; batch adversarial loss: 0.650449\n",
            "epoch 36; iter: 0; batch classifier loss: 0.509123; batch adversarial loss: 0.599724\n",
            "epoch 36; iter: 200; batch classifier loss: 0.701838; batch adversarial loss: 0.632592\n",
            "epoch 37; iter: 0; batch classifier loss: 0.552795; batch adversarial loss: 0.665541\n",
            "epoch 37; iter: 200; batch classifier loss: 0.696269; batch adversarial loss: 0.600354\n",
            "epoch 38; iter: 0; batch classifier loss: 0.832699; batch adversarial loss: 0.676822\n",
            "epoch 38; iter: 200; batch classifier loss: 0.702811; batch adversarial loss: 0.604390\n",
            "epoch 39; iter: 0; batch classifier loss: 0.594674; batch adversarial loss: 0.659852\n",
            "epoch 39; iter: 200; batch classifier loss: 0.472166; batch adversarial loss: 0.632648\n",
            "epoch 40; iter: 0; batch classifier loss: 0.584154; batch adversarial loss: 0.647841\n",
            "epoch 40; iter: 200; batch classifier loss: 0.634044; batch adversarial loss: 0.662890\n",
            "epoch 41; iter: 0; batch classifier loss: 0.658270; batch adversarial loss: 0.665348\n",
            "epoch 41; iter: 200; batch classifier loss: 0.512371; batch adversarial loss: 0.638722\n",
            "epoch 42; iter: 0; batch classifier loss: 0.589297; batch adversarial loss: 0.611158\n",
            "epoch 42; iter: 200; batch classifier loss: 0.601758; batch adversarial loss: 0.647757\n",
            "epoch 43; iter: 0; batch classifier loss: 0.601609; batch adversarial loss: 0.649889\n",
            "epoch 43; iter: 200; batch classifier loss: 0.667365; batch adversarial loss: 0.601859\n",
            "epoch 44; iter: 0; batch classifier loss: 0.727205; batch adversarial loss: 0.653555\n",
            "epoch 44; iter: 200; batch classifier loss: 0.646112; batch adversarial loss: 0.587861\n",
            "epoch 45; iter: 0; batch classifier loss: 0.678569; batch adversarial loss: 0.620406\n",
            "epoch 45; iter: 200; batch classifier loss: 0.595034; batch adversarial loss: 0.627574\n",
            "epoch 46; iter: 0; batch classifier loss: 0.545037; batch adversarial loss: 0.638935\n",
            "epoch 46; iter: 200; batch classifier loss: 0.623681; batch adversarial loss: 0.650127\n",
            "epoch 47; iter: 0; batch classifier loss: 0.511217; batch adversarial loss: 0.621806\n",
            "epoch 47; iter: 200; batch classifier loss: 0.701550; batch adversarial loss: 0.621274\n",
            "epoch 48; iter: 0; batch classifier loss: 0.554920; batch adversarial loss: 0.687177\n",
            "epoch 48; iter: 200; batch classifier loss: 0.645096; batch adversarial loss: 0.612464\n",
            "epoch 49; iter: 0; batch classifier loss: 0.681727; batch adversarial loss: 0.643380\n",
            "epoch 49; iter: 200; batch classifier loss: 0.606744; batch adversarial loss: 0.647192\n",
            "epoch 0; iter: 0; batch classifier loss: 0.712710; batch adversarial loss: 0.737793\n",
            "epoch 0; iter: 200; batch classifier loss: 2.442304; batch adversarial loss: 0.684142\n",
            "epoch 1; iter: 0; batch classifier loss: 3.187113; batch adversarial loss: 0.692671\n",
            "epoch 1; iter: 200; batch classifier loss: 3.595172; batch adversarial loss: 0.649780\n",
            "epoch 2; iter: 0; batch classifier loss: 4.298185; batch adversarial loss: 0.645570\n",
            "epoch 2; iter: 200; batch classifier loss: 4.421206; batch adversarial loss: 0.673036\n",
            "epoch 3; iter: 0; batch classifier loss: 4.503067; batch adversarial loss: 0.628868\n",
            "epoch 3; iter: 200; batch classifier loss: 4.025086; batch adversarial loss: 0.631966\n",
            "epoch 4; iter: 0; batch classifier loss: 4.688488; batch adversarial loss: 0.629945\n",
            "epoch 4; iter: 200; batch classifier loss: 4.785912; batch adversarial loss: 0.641083\n",
            "epoch 5; iter: 0; batch classifier loss: 4.265887; batch adversarial loss: 0.621708\n",
            "epoch 5; iter: 200; batch classifier loss: 3.411800; batch adversarial loss: 0.615913\n",
            "epoch 6; iter: 0; batch classifier loss: 3.176821; batch adversarial loss: 0.602475\n",
            "epoch 6; iter: 200; batch classifier loss: 2.669569; batch adversarial loss: 0.608109\n",
            "epoch 7; iter: 0; batch classifier loss: 2.295294; batch adversarial loss: 0.627024\n",
            "epoch 7; iter: 200; batch classifier loss: 0.949006; batch adversarial loss: 0.698964\n",
            "epoch 8; iter: 0; batch classifier loss: 0.861387; batch adversarial loss: 0.670651\n",
            "epoch 8; iter: 200; batch classifier loss: 1.819725; batch adversarial loss: 0.622958\n",
            "epoch 9; iter: 0; batch classifier loss: 1.990094; batch adversarial loss: 0.586102\n",
            "epoch 9; iter: 200; batch classifier loss: 1.828471; batch adversarial loss: 0.670506\n",
            "epoch 10; iter: 0; batch classifier loss: 1.352385; batch adversarial loss: 0.670506\n",
            "epoch 10; iter: 200; batch classifier loss: 1.951786; batch adversarial loss: 0.643704\n",
            "epoch 11; iter: 0; batch classifier loss: 1.864004; batch adversarial loss: 0.627495\n",
            "epoch 11; iter: 200; batch classifier loss: 1.413445; batch adversarial loss: 0.606010\n",
            "epoch 12; iter: 0; batch classifier loss: 1.367997; batch adversarial loss: 0.627418\n",
            "epoch 12; iter: 200; batch classifier loss: 1.665962; batch adversarial loss: 0.605597\n",
            "epoch 13; iter: 0; batch classifier loss: 1.469769; batch adversarial loss: 0.616447\n",
            "epoch 13; iter: 200; batch classifier loss: 1.500731; batch adversarial loss: 0.572855\n",
            "epoch 14; iter: 0; batch classifier loss: 1.457487; batch adversarial loss: 0.632878\n",
            "epoch 14; iter: 200; batch classifier loss: 1.088590; batch adversarial loss: 0.627524\n",
            "epoch 15; iter: 0; batch classifier loss: 0.867959; batch adversarial loss: 0.638368\n",
            "epoch 15; iter: 200; batch classifier loss: 1.873788; batch adversarial loss: 0.653589\n",
            "epoch 16; iter: 0; batch classifier loss: 2.091626; batch adversarial loss: 0.643779\n",
            "epoch 16; iter: 200; batch classifier loss: 2.584739; batch adversarial loss: 0.621985\n",
            "epoch 17; iter: 0; batch classifier loss: 2.543006; batch adversarial loss: 0.621970\n",
            "epoch 17; iter: 200; batch classifier loss: 1.394741; batch adversarial loss: 0.687794\n",
            "epoch 18; iter: 0; batch classifier loss: 2.181713; batch adversarial loss: 0.643807\n",
            "epoch 18; iter: 200; batch classifier loss: 2.019311; batch adversarial loss: 0.610871\n",
            "epoch 19; iter: 0; batch classifier loss: 1.764972; batch adversarial loss: 0.627439\n",
            "epoch 19; iter: 200; batch classifier loss: 2.220057; batch adversarial loss: 0.692778\n",
            "epoch 20; iter: 0; batch classifier loss: 1.664794; batch adversarial loss: 0.649141\n",
            "epoch 20; iter: 200; batch classifier loss: 2.055852; batch adversarial loss: 0.622049\n",
            "epoch 21; iter: 0; batch classifier loss: 2.001608; batch adversarial loss: 0.589142\n",
            "epoch 21; iter: 200; batch classifier loss: 1.776486; batch adversarial loss: 0.632907\n",
            "epoch 22; iter: 0; batch classifier loss: 1.656475; batch adversarial loss: 0.638344\n",
            "epoch 22; iter: 200; batch classifier loss: 1.952161; batch adversarial loss: 0.638388\n",
            "epoch 23; iter: 0; batch classifier loss: 1.690457; batch adversarial loss: 0.654598\n",
            "epoch 23; iter: 200; batch classifier loss: 1.577203; batch adversarial loss: 0.665693\n",
            "epoch 24; iter: 0; batch classifier loss: 1.231325; batch adversarial loss: 0.638323\n",
            "epoch 24; iter: 200; batch classifier loss: 2.234250; batch adversarial loss: 0.654414\n",
            "epoch 25; iter: 0; batch classifier loss: 1.555793; batch adversarial loss: 0.649201\n",
            "epoch 25; iter: 200; batch classifier loss: 2.203774; batch adversarial loss: 0.578411\n",
            "epoch 26; iter: 0; batch classifier loss: 2.331365; batch adversarial loss: 0.621884\n",
            "epoch 26; iter: 200; batch classifier loss: 1.425743; batch adversarial loss: 0.594390\n",
            "epoch 27; iter: 0; batch classifier loss: 1.771867; batch adversarial loss: 0.611292\n",
            "epoch 27; iter: 200; batch classifier loss: 2.004133; batch adversarial loss: 0.687443\n",
            "epoch 28; iter: 0; batch classifier loss: 2.853513; batch adversarial loss: 0.600101\n",
            "epoch 28; iter: 200; batch classifier loss: 2.416967; batch adversarial loss: 0.594564\n",
            "epoch 29; iter: 0; batch classifier loss: 2.165319; batch adversarial loss: 0.665490\n",
            "epoch 29; iter: 200; batch classifier loss: 1.837511; batch adversarial loss: 0.632890\n",
            "epoch 30; iter: 0; batch classifier loss: 2.056020; batch adversarial loss: 0.703932\n",
            "epoch 30; iter: 200; batch classifier loss: 2.298765; batch adversarial loss: 0.643772\n",
            "epoch 31; iter: 0; batch classifier loss: 2.491576; batch adversarial loss: 0.638332\n",
            "epoch 31; iter: 200; batch classifier loss: 2.271293; batch adversarial loss: 0.621909\n",
            "epoch 32; iter: 0; batch classifier loss: 1.922045; batch adversarial loss: 0.649347\n",
            "epoch 32; iter: 200; batch classifier loss: 1.478822; batch adversarial loss: 0.631563\n",
            "epoch 33; iter: 0; batch classifier loss: 1.493317; batch adversarial loss: 0.597272\n",
            "epoch 33; iter: 200; batch classifier loss: 2.813623; batch adversarial loss: 0.649368\n",
            "epoch 34; iter: 0; batch classifier loss: 2.238912; batch adversarial loss: 0.671107\n",
            "epoch 34; iter: 200; batch classifier loss: 2.922643; batch adversarial loss: 0.672349\n",
            "epoch 35; iter: 0; batch classifier loss: 2.299401; batch adversarial loss: 0.654656\n",
            "epoch 35; iter: 200; batch classifier loss: 2.618521; batch adversarial loss: 0.579009\n",
            "epoch 36; iter: 0; batch classifier loss: 1.822286; batch adversarial loss: 0.660210\n",
            "epoch 36; iter: 200; batch classifier loss: 3.315660; batch adversarial loss: 0.616453\n",
            "epoch 37; iter: 0; batch classifier loss: 2.174027; batch adversarial loss: 0.573067\n",
            "epoch 37; iter: 200; batch classifier loss: 1.744703; batch adversarial loss: 0.643983\n",
            "epoch 38; iter: 0; batch classifier loss: 2.037431; batch adversarial loss: 0.639177\n",
            "epoch 38; iter: 200; batch classifier loss: 1.645373; batch adversarial loss: 0.670005\n",
            "epoch 39; iter: 0; batch classifier loss: 1.637977; batch adversarial loss: 0.634096\n",
            "epoch 39; iter: 200; batch classifier loss: 1.586411; batch adversarial loss: 0.653841\n",
            "epoch 40; iter: 0; batch classifier loss: 1.509179; batch adversarial loss: 0.632034\n",
            "epoch 40; iter: 200; batch classifier loss: 2.126944; batch adversarial loss: 0.608556\n",
            "epoch 41; iter: 0; batch classifier loss: 3.640247; batch adversarial loss: 0.627356\n",
            "epoch 41; iter: 200; batch classifier loss: 1.955347; batch adversarial loss: 0.703258\n",
            "epoch 42; iter: 0; batch classifier loss: 1.985820; batch adversarial loss: 0.659468\n",
            "epoch 42; iter: 200; batch classifier loss: 1.743439; batch adversarial loss: 0.637455\n",
            "epoch 43; iter: 0; batch classifier loss: 2.585109; batch adversarial loss: 0.656256\n",
            "epoch 43; iter: 200; batch classifier loss: 2.453035; batch adversarial loss: 0.631593\n",
            "epoch 44; iter: 0; batch classifier loss: 3.841235; batch adversarial loss: 0.643191\n",
            "epoch 44; iter: 200; batch classifier loss: 3.606770; batch adversarial loss: 0.594858\n",
            "epoch 45; iter: 0; batch classifier loss: 2.943449; batch adversarial loss: 0.605642\n",
            "epoch 45; iter: 200; batch classifier loss: 2.997811; batch adversarial loss: 0.605570\n",
            "epoch 46; iter: 0; batch classifier loss: 3.350312; batch adversarial loss: 0.660119\n",
            "epoch 46; iter: 200; batch classifier loss: 3.235449; batch adversarial loss: 0.606259\n",
            "epoch 47; iter: 0; batch classifier loss: 1.810145; batch adversarial loss: 0.649027\n",
            "epoch 47; iter: 200; batch classifier loss: 2.093793; batch adversarial loss: 0.604465\n",
            "epoch 48; iter: 0; batch classifier loss: 2.964877; batch adversarial loss: 0.668082\n",
            "epoch 48; iter: 200; batch classifier loss: 3.021966; batch adversarial loss: 0.643739\n",
            "epoch 49; iter: 0; batch classifier loss: 3.399131; batch adversarial loss: 0.649293\n",
            "epoch 49; iter: 200; batch classifier loss: 4.076871; batch adversarial loss: 0.589061\n",
            "\n",
            " Welcome to Fold number 3 \n",
            "\n",
            "epoch 0; iter: 0; batch classifier loss: 0.681437; batch adversarial loss: 0.715101\n",
            "epoch 0; iter: 200; batch classifier loss: 0.412675; batch adversarial loss: 0.674161\n",
            "epoch 1; iter: 0; batch classifier loss: 0.450967; batch adversarial loss: 0.672112\n",
            "epoch 1; iter: 200; batch classifier loss: 0.476162; batch adversarial loss: 0.647576\n",
            "epoch 2; iter: 0; batch classifier loss: 0.438346; batch adversarial loss: 0.617740\n",
            "epoch 2; iter: 200; batch classifier loss: 0.469536; batch adversarial loss: 0.608249\n",
            "epoch 3; iter: 0; batch classifier loss: 0.493092; batch adversarial loss: 0.600680\n",
            "epoch 3; iter: 200; batch classifier loss: 0.499911; batch adversarial loss: 0.655002\n",
            "epoch 4; iter: 0; batch classifier loss: 0.461095; batch adversarial loss: 0.596907\n",
            "epoch 4; iter: 200; batch classifier loss: 0.465066; batch adversarial loss: 0.655202\n",
            "epoch 5; iter: 0; batch classifier loss: 0.433875; batch adversarial loss: 0.562638\n",
            "epoch 5; iter: 200; batch classifier loss: 0.437745; batch adversarial loss: 0.636730\n",
            "epoch 6; iter: 0; batch classifier loss: 0.428814; batch adversarial loss: 0.594195\n",
            "epoch 6; iter: 200; batch classifier loss: 0.455807; batch adversarial loss: 0.621264\n",
            "epoch 7; iter: 0; batch classifier loss: 0.394911; batch adversarial loss: 0.649847\n",
            "epoch 7; iter: 200; batch classifier loss: 0.379585; batch adversarial loss: 0.672733\n",
            "epoch 8; iter: 0; batch classifier loss: 0.542426; batch adversarial loss: 0.562501\n",
            "epoch 8; iter: 200; batch classifier loss: 0.401661; batch adversarial loss: 0.559535\n",
            "epoch 9; iter: 0; batch classifier loss: 0.490420; batch adversarial loss: 0.598671\n",
            "epoch 9; iter: 200; batch classifier loss: 0.460578; batch adversarial loss: 0.571203\n",
            "epoch 10; iter: 0; batch classifier loss: 0.407860; batch adversarial loss: 0.582103\n",
            "epoch 10; iter: 200; batch classifier loss: 0.422353; batch adversarial loss: 0.585393\n",
            "epoch 11; iter: 0; batch classifier loss: 0.463374; batch adversarial loss: 0.574575\n",
            "epoch 11; iter: 200; batch classifier loss: 0.392432; batch adversarial loss: 0.583524\n",
            "epoch 12; iter: 0; batch classifier loss: 0.377631; batch adversarial loss: 0.592371\n",
            "epoch 12; iter: 200; batch classifier loss: 0.478850; batch adversarial loss: 0.580739\n",
            "epoch 13; iter: 0; batch classifier loss: 0.474780; batch adversarial loss: 0.524838\n",
            "epoch 13; iter: 200; batch classifier loss: 0.458017; batch adversarial loss: 0.555009\n",
            "epoch 14; iter: 0; batch classifier loss: 0.412414; batch adversarial loss: 0.524795\n",
            "epoch 14; iter: 200; batch classifier loss: 0.327195; batch adversarial loss: 0.520989\n",
            "epoch 15; iter: 0; batch classifier loss: 0.404302; batch adversarial loss: 0.482131\n",
            "epoch 15; iter: 200; batch classifier loss: 0.558815; batch adversarial loss: 0.483776\n",
            "epoch 16; iter: 0; batch classifier loss: 0.437288; batch adversarial loss: 0.513415\n",
            "epoch 16; iter: 200; batch classifier loss: 0.474151; batch adversarial loss: 0.619296\n",
            "epoch 17; iter: 0; batch classifier loss: 0.389156; batch adversarial loss: 0.533964\n",
            "epoch 17; iter: 200; batch classifier loss: 0.420812; batch adversarial loss: 0.573047\n",
            "epoch 18; iter: 0; batch classifier loss: 0.421780; batch adversarial loss: 0.487551\n",
            "epoch 18; iter: 200; batch classifier loss: 0.490727; batch adversarial loss: 0.569574\n",
            "epoch 19; iter: 0; batch classifier loss: 0.465122; batch adversarial loss: 0.502768\n",
            "epoch 19; iter: 200; batch classifier loss: 0.483549; batch adversarial loss: 0.483866\n",
            "epoch 20; iter: 0; batch classifier loss: 0.377609; batch adversarial loss: 0.543268\n",
            "epoch 20; iter: 200; batch classifier loss: 0.423324; batch adversarial loss: 0.543911\n",
            "epoch 21; iter: 0; batch classifier loss: 0.338212; batch adversarial loss: 0.523435\n",
            "epoch 21; iter: 200; batch classifier loss: 0.448093; batch adversarial loss: 0.501781\n",
            "epoch 22; iter: 0; batch classifier loss: 0.384751; batch adversarial loss: 0.524817\n",
            "epoch 22; iter: 200; batch classifier loss: 0.375869; batch adversarial loss: 0.541485\n",
            "epoch 23; iter: 0; batch classifier loss: 0.372063; batch adversarial loss: 0.551441\n",
            "epoch 23; iter: 200; batch classifier loss: 0.404828; batch adversarial loss: 0.482108\n",
            "epoch 24; iter: 0; batch classifier loss: 0.393744; batch adversarial loss: 0.483598\n",
            "epoch 24; iter: 200; batch classifier loss: 0.443638; batch adversarial loss: 0.474634\n",
            "epoch 25; iter: 0; batch classifier loss: 0.435210; batch adversarial loss: 0.556111\n",
            "epoch 25; iter: 200; batch classifier loss: 0.418554; batch adversarial loss: 0.483620\n",
            "epoch 26; iter: 0; batch classifier loss: 0.314812; batch adversarial loss: 0.500232\n",
            "epoch 26; iter: 200; batch classifier loss: 0.348628; batch adversarial loss: 0.545302\n",
            "epoch 27; iter: 0; batch classifier loss: 0.356258; batch adversarial loss: 0.504761\n",
            "epoch 27; iter: 200; batch classifier loss: 0.509679; batch adversarial loss: 0.510703\n",
            "epoch 28; iter: 0; batch classifier loss: 0.406722; batch adversarial loss: 0.608070\n",
            "epoch 28; iter: 200; batch classifier loss: 0.343204; batch adversarial loss: 0.620724\n",
            "epoch 29; iter: 0; batch classifier loss: 0.396874; batch adversarial loss: 0.567405\n",
            "epoch 29; iter: 200; batch classifier loss: 0.425226; batch adversarial loss: 0.535919\n",
            "epoch 30; iter: 0; batch classifier loss: 0.391010; batch adversarial loss: 0.529612\n",
            "epoch 30; iter: 200; batch classifier loss: 0.474035; batch adversarial loss: 0.490240\n",
            "epoch 31; iter: 0; batch classifier loss: 0.457888; batch adversarial loss: 0.509410\n",
            "epoch 31; iter: 200; batch classifier loss: 0.417660; batch adversarial loss: 0.489714\n",
            "epoch 32; iter: 0; batch classifier loss: 0.459516; batch adversarial loss: 0.530440\n",
            "epoch 32; iter: 200; batch classifier loss: 0.440063; batch adversarial loss: 0.514640\n",
            "epoch 33; iter: 0; batch classifier loss: 0.350082; batch adversarial loss: 0.542809\n",
            "epoch 33; iter: 200; batch classifier loss: 0.422484; batch adversarial loss: 0.557083\n",
            "epoch 34; iter: 0; batch classifier loss: 0.459523; batch adversarial loss: 0.546655\n",
            "epoch 34; iter: 200; batch classifier loss: 0.471954; batch adversarial loss: 0.552398\n",
            "epoch 35; iter: 0; batch classifier loss: 0.424578; batch adversarial loss: 0.519429\n",
            "epoch 35; iter: 200; batch classifier loss: 0.416357; batch adversarial loss: 0.540922\n",
            "epoch 36; iter: 0; batch classifier loss: 0.383470; batch adversarial loss: 0.525748\n",
            "epoch 36; iter: 200; batch classifier loss: 0.408706; batch adversarial loss: 0.476819\n",
            "epoch 37; iter: 0; batch classifier loss: 0.480992; batch adversarial loss: 0.553610\n",
            "epoch 37; iter: 200; batch classifier loss: 0.445768; batch adversarial loss: 0.481141\n",
            "epoch 38; iter: 0; batch classifier loss: 0.449370; batch adversarial loss: 0.528108\n",
            "epoch 38; iter: 200; batch classifier loss: 0.434565; batch adversarial loss: 0.519231\n",
            "epoch 39; iter: 0; batch classifier loss: 0.430999; batch adversarial loss: 0.400665\n",
            "epoch 39; iter: 200; batch classifier loss: 0.306123; batch adversarial loss: 0.483193\n",
            "epoch 40; iter: 0; batch classifier loss: 0.393149; batch adversarial loss: 0.535374\n",
            "epoch 40; iter: 200; batch classifier loss: 0.468133; batch adversarial loss: 0.503974\n",
            "epoch 41; iter: 0; batch classifier loss: 0.472011; batch adversarial loss: 0.534527\n",
            "epoch 41; iter: 200; batch classifier loss: 0.410205; batch adversarial loss: 0.600797\n",
            "epoch 42; iter: 0; batch classifier loss: 0.431470; batch adversarial loss: 0.567530\n",
            "epoch 42; iter: 200; batch classifier loss: 0.416224; batch adversarial loss: 0.552745\n",
            "epoch 43; iter: 0; batch classifier loss: 0.384449; batch adversarial loss: 0.549932\n",
            "epoch 43; iter: 200; batch classifier loss: 0.490532; batch adversarial loss: 0.609165\n",
            "epoch 44; iter: 0; batch classifier loss: 0.338445; batch adversarial loss: 0.533853\n",
            "epoch 44; iter: 200; batch classifier loss: 0.465290; batch adversarial loss: 0.585131\n",
            "epoch 45; iter: 0; batch classifier loss: 0.400592; batch adversarial loss: 0.529385\n",
            "epoch 45; iter: 200; batch classifier loss: 0.405401; batch adversarial loss: 0.566482\n",
            "epoch 46; iter: 0; batch classifier loss: 0.385441; batch adversarial loss: 0.514124\n",
            "epoch 46; iter: 200; batch classifier loss: 0.493129; batch adversarial loss: 0.612111\n",
            "epoch 47; iter: 0; batch classifier loss: 0.457973; batch adversarial loss: 0.572714\n",
            "epoch 47; iter: 200; batch classifier loss: 0.394818; batch adversarial loss: 0.575658\n",
            "epoch 48; iter: 0; batch classifier loss: 0.396934; batch adversarial loss: 0.594410\n",
            "epoch 48; iter: 200; batch classifier loss: 0.471048; batch adversarial loss: 0.563590\n",
            "epoch 49; iter: 0; batch classifier loss: 0.434612; batch adversarial loss: 0.498315\n",
            "epoch 49; iter: 200; batch classifier loss: 0.427044; batch adversarial loss: 0.555538\n",
            "epoch 0; iter: 0; batch classifier loss: 0.699650; batch adversarial loss: 0.700631\n",
            "epoch 0; iter: 200; batch classifier loss: 0.427220; batch adversarial loss: 0.654171\n",
            "epoch 1; iter: 0; batch classifier loss: 0.442109; batch adversarial loss: 0.654058\n",
            "epoch 1; iter: 200; batch classifier loss: 0.473431; batch adversarial loss: 0.649199\n",
            "epoch 2; iter: 0; batch classifier loss: 0.431130; batch adversarial loss: 0.639821\n",
            "epoch 2; iter: 200; batch classifier loss: 0.505401; batch adversarial loss: 0.614649\n",
            "epoch 3; iter: 0; batch classifier loss: 0.452594; batch adversarial loss: 0.592168\n",
            "epoch 3; iter: 200; batch classifier loss: 0.353042; batch adversarial loss: 0.645414\n",
            "epoch 4; iter: 0; batch classifier loss: 0.425564; batch adversarial loss: 0.575546\n",
            "epoch 4; iter: 200; batch classifier loss: 0.505066; batch adversarial loss: 0.633035\n",
            "epoch 5; iter: 0; batch classifier loss: 0.523642; batch adversarial loss: 0.625438\n",
            "epoch 5; iter: 200; batch classifier loss: 0.389671; batch adversarial loss: 0.535751\n",
            "epoch 6; iter: 0; batch classifier loss: 0.450611; batch adversarial loss: 0.611508\n",
            "epoch 6; iter: 200; batch classifier loss: 0.431186; batch adversarial loss: 0.590228\n",
            "epoch 7; iter: 0; batch classifier loss: 0.470422; batch adversarial loss: 0.563061\n",
            "epoch 7; iter: 200; batch classifier loss: 0.464267; batch adversarial loss: 0.543580\n",
            "epoch 8; iter: 0; batch classifier loss: 0.383433; batch adversarial loss: 0.618842\n",
            "epoch 8; iter: 200; batch classifier loss: 0.494855; batch adversarial loss: 0.536971\n",
            "epoch 9; iter: 0; batch classifier loss: 0.422380; batch adversarial loss: 0.598752\n",
            "epoch 9; iter: 200; batch classifier loss: 0.476501; batch adversarial loss: 0.581976\n",
            "epoch 10; iter: 0; batch classifier loss: 0.436304; batch adversarial loss: 0.537143\n",
            "epoch 10; iter: 200; batch classifier loss: 0.543083; batch adversarial loss: 0.538663\n",
            "epoch 11; iter: 0; batch classifier loss: 0.408472; batch adversarial loss: 0.516781\n",
            "epoch 11; iter: 200; batch classifier loss: 0.397429; batch adversarial loss: 0.534117\n",
            "epoch 12; iter: 0; batch classifier loss: 0.443463; batch adversarial loss: 0.546171\n",
            "epoch 12; iter: 200; batch classifier loss: 0.376903; batch adversarial loss: 0.525277\n",
            "epoch 13; iter: 0; batch classifier loss: 0.412079; batch adversarial loss: 0.512253\n",
            "epoch 13; iter: 200; batch classifier loss: 0.447278; batch adversarial loss: 0.450503\n",
            "epoch 14; iter: 0; batch classifier loss: 0.296383; batch adversarial loss: 0.528454\n",
            "epoch 14; iter: 200; batch classifier loss: 0.357417; batch adversarial loss: 0.523039\n",
            "epoch 15; iter: 0; batch classifier loss: 0.352123; batch adversarial loss: 0.521583\n",
            "epoch 15; iter: 200; batch classifier loss: 0.446523; batch adversarial loss: 0.435706\n",
            "epoch 16; iter: 0; batch classifier loss: 0.396856; batch adversarial loss: 0.470363\n",
            "epoch 16; iter: 200; batch classifier loss: 0.484101; batch adversarial loss: 0.420773\n",
            "epoch 17; iter: 0; batch classifier loss: 0.439528; batch adversarial loss: 0.464985\n",
            "epoch 17; iter: 200; batch classifier loss: 0.634846; batch adversarial loss: 0.435945\n",
            "epoch 18; iter: 0; batch classifier loss: 0.425464; batch adversarial loss: 0.435974\n",
            "epoch 18; iter: 200; batch classifier loss: 0.375192; batch adversarial loss: 0.533625\n",
            "epoch 19; iter: 0; batch classifier loss: 0.421675; batch adversarial loss: 0.554673\n",
            "epoch 19; iter: 200; batch classifier loss: 0.345264; batch adversarial loss: 0.491121\n",
            "epoch 20; iter: 0; batch classifier loss: 0.388819; batch adversarial loss: 0.550479\n",
            "epoch 20; iter: 200; batch classifier loss: 0.381578; batch adversarial loss: 0.564242\n",
            "epoch 21; iter: 0; batch classifier loss: 0.404164; batch adversarial loss: 0.631478\n",
            "epoch 21; iter: 200; batch classifier loss: 0.340011; batch adversarial loss: 0.622850\n",
            "epoch 22; iter: 0; batch classifier loss: 0.374396; batch adversarial loss: 0.647205\n",
            "epoch 22; iter: 200; batch classifier loss: 0.431795; batch adversarial loss: 0.572090\n",
            "epoch 23; iter: 0; batch classifier loss: 0.377088; batch adversarial loss: 0.579736\n",
            "epoch 23; iter: 200; batch classifier loss: 0.457983; batch adversarial loss: 0.602612\n",
            "epoch 24; iter: 0; batch classifier loss: 0.445325; batch adversarial loss: 0.554228\n",
            "epoch 24; iter: 200; batch classifier loss: 0.369595; batch adversarial loss: 0.548944\n",
            "epoch 25; iter: 0; batch classifier loss: 0.399780; batch adversarial loss: 0.629823\n",
            "epoch 25; iter: 200; batch classifier loss: 0.407479; batch adversarial loss: 0.583853\n",
            "epoch 26; iter: 0; batch classifier loss: 0.442722; batch adversarial loss: 0.599285\n",
            "epoch 26; iter: 200; batch classifier loss: 0.357805; batch adversarial loss: 0.455642\n",
            "epoch 27; iter: 0; batch classifier loss: 0.346517; batch adversarial loss: 0.563614\n",
            "epoch 27; iter: 200; batch classifier loss: 0.474226; batch adversarial loss: 0.559415\n",
            "epoch 28; iter: 0; batch classifier loss: 0.399917; batch adversarial loss: 0.568405\n",
            "epoch 28; iter: 200; batch classifier loss: 0.450449; batch adversarial loss: 0.543271\n",
            "epoch 29; iter: 0; batch classifier loss: 0.393446; batch adversarial loss: 0.531521\n",
            "epoch 29; iter: 200; batch classifier loss: 0.379745; batch adversarial loss: 0.578454\n",
            "epoch 30; iter: 0; batch classifier loss: 0.507905; batch adversarial loss: 0.523034\n",
            "epoch 30; iter: 200; batch classifier loss: 0.469418; batch adversarial loss: 0.514195\n",
            "epoch 31; iter: 0; batch classifier loss: 0.424508; batch adversarial loss: 0.517074\n",
            "epoch 31; iter: 200; batch classifier loss: 0.388151; batch adversarial loss: 0.580280\n",
            "epoch 32; iter: 0; batch classifier loss: 0.390825; batch adversarial loss: 0.550561\n",
            "epoch 32; iter: 200; batch classifier loss: 0.382569; batch adversarial loss: 0.567891\n",
            "epoch 33; iter: 0; batch classifier loss: 0.473020; batch adversarial loss: 0.531821\n",
            "epoch 33; iter: 200; batch classifier loss: 0.341160; batch adversarial loss: 0.566058\n",
            "epoch 34; iter: 0; batch classifier loss: 0.362639; batch adversarial loss: 0.509542\n",
            "epoch 34; iter: 200; batch classifier loss: 0.329932; batch adversarial loss: 0.505105\n",
            "epoch 35; iter: 0; batch classifier loss: 0.429587; batch adversarial loss: 0.512900\n",
            "epoch 35; iter: 200; batch classifier loss: 0.483485; batch adversarial loss: 0.505163\n",
            "epoch 36; iter: 0; batch classifier loss: 0.402740; batch adversarial loss: 0.536055\n",
            "epoch 36; iter: 200; batch classifier loss: 0.422984; batch adversarial loss: 0.590667\n",
            "epoch 37; iter: 0; batch classifier loss: 0.470825; batch adversarial loss: 0.514219\n",
            "epoch 37; iter: 200; batch classifier loss: 0.411202; batch adversarial loss: 0.471652\n",
            "epoch 38; iter: 0; batch classifier loss: 0.366616; batch adversarial loss: 0.592104\n",
            "epoch 38; iter: 200; batch classifier loss: 0.443352; batch adversarial loss: 0.538188\n",
            "epoch 39; iter: 0; batch classifier loss: 0.423845; batch adversarial loss: 0.491945\n",
            "epoch 39; iter: 200; batch classifier loss: 0.486469; batch adversarial loss: 0.493858\n",
            "epoch 40; iter: 0; batch classifier loss: 0.368208; batch adversarial loss: 0.532175\n",
            "epoch 40; iter: 200; batch classifier loss: 0.394309; batch adversarial loss: 0.549571\n",
            "epoch 41; iter: 0; batch classifier loss: 0.433652; batch adversarial loss: 0.505279\n",
            "epoch 41; iter: 200; batch classifier loss: 0.389135; batch adversarial loss: 0.568333\n",
            "epoch 42; iter: 0; batch classifier loss: 0.400124; batch adversarial loss: 0.550545\n",
            "epoch 42; iter: 200; batch classifier loss: 0.455572; batch adversarial loss: 0.576836\n",
            "epoch 43; iter: 0; batch classifier loss: 0.504457; batch adversarial loss: 0.573998\n",
            "epoch 43; iter: 200; batch classifier loss: 0.405165; batch adversarial loss: 0.502542\n",
            "epoch 44; iter: 0; batch classifier loss: 0.454269; batch adversarial loss: 0.579157\n",
            "epoch 44; iter: 200; batch classifier loss: 0.425147; batch adversarial loss: 0.552618\n",
            "epoch 45; iter: 0; batch classifier loss: 0.478499; batch adversarial loss: 0.502594\n",
            "epoch 45; iter: 200; batch classifier loss: 0.428284; batch adversarial loss: 0.494575\n",
            "epoch 46; iter: 0; batch classifier loss: 0.518861; batch adversarial loss: 0.500937\n",
            "epoch 46; iter: 200; batch classifier loss: 0.354823; batch adversarial loss: 0.547433\n",
            "epoch 47; iter: 0; batch classifier loss: 0.438542; batch adversarial loss: 0.551999\n",
            "epoch 47; iter: 200; batch classifier loss: 0.424021; batch adversarial loss: 0.605432\n",
            "epoch 48; iter: 0; batch classifier loss: 0.390806; batch adversarial loss: 0.544892\n",
            "epoch 48; iter: 200; batch classifier loss: 0.424090; batch adversarial loss: 0.497318\n",
            "epoch 49; iter: 0; batch classifier loss: 0.448806; batch adversarial loss: 0.596285\n",
            "epoch 49; iter: 200; batch classifier loss: 0.436059; batch adversarial loss: 0.582161\n",
            "epoch 0; iter: 0; batch classifier loss: 0.705294; batch adversarial loss: 0.774485\n",
            "epoch 0; iter: 200; batch classifier loss: 0.989243; batch adversarial loss: 1.019159\n",
            "epoch 1; iter: 0; batch classifier loss: 0.868083; batch adversarial loss: 0.856215\n",
            "epoch 1; iter: 200; batch classifier loss: 0.762357; batch adversarial loss: 0.779622\n",
            "epoch 2; iter: 0; batch classifier loss: 0.890399; batch adversarial loss: 0.757840\n",
            "epoch 2; iter: 200; batch classifier loss: 0.935591; batch adversarial loss: 0.702590\n",
            "epoch 3; iter: 0; batch classifier loss: 0.663560; batch adversarial loss: 0.666633\n",
            "epoch 3; iter: 200; batch classifier loss: 0.558185; batch adversarial loss: 0.622129\n",
            "epoch 4; iter: 0; batch classifier loss: 0.387764; batch adversarial loss: 0.629313\n",
            "epoch 4; iter: 200; batch classifier loss: 0.496067; batch adversarial loss: 0.594292\n",
            "epoch 5; iter: 0; batch classifier loss: 0.431498; batch adversarial loss: 0.612994\n",
            "epoch 5; iter: 200; batch classifier loss: 0.430537; batch adversarial loss: 0.661014\n",
            "epoch 6; iter: 0; batch classifier loss: 0.571110; batch adversarial loss: 0.619009\n",
            "epoch 6; iter: 200; batch classifier loss: 0.484346; batch adversarial loss: 0.668637\n",
            "epoch 7; iter: 0; batch classifier loss: 0.439845; batch adversarial loss: 0.696564\n",
            "epoch 7; iter: 200; batch classifier loss: 0.468468; batch adversarial loss: 0.632874\n",
            "epoch 8; iter: 0; batch classifier loss: 0.563474; batch adversarial loss: 0.607762\n",
            "epoch 8; iter: 200; batch classifier loss: 0.533761; batch adversarial loss: 0.650518\n",
            "epoch 9; iter: 0; batch classifier loss: 0.505677; batch adversarial loss: 0.622985\n",
            "epoch 9; iter: 200; batch classifier loss: 0.525479; batch adversarial loss: 0.675625\n",
            "epoch 10; iter: 0; batch classifier loss: 0.461261; batch adversarial loss: 0.617517\n",
            "epoch 10; iter: 200; batch classifier loss: 0.512537; batch adversarial loss: 0.621089\n",
            "epoch 11; iter: 0; batch classifier loss: 0.432276; batch adversarial loss: 0.620721\n",
            "epoch 11; iter: 200; batch classifier loss: 0.458726; batch adversarial loss: 0.635245\n",
            "epoch 12; iter: 0; batch classifier loss: 0.522016; batch adversarial loss: 0.595390\n",
            "epoch 12; iter: 200; batch classifier loss: 0.504309; batch adversarial loss: 0.645059\n",
            "epoch 13; iter: 0; batch classifier loss: 0.402551; batch adversarial loss: 0.651529\n",
            "epoch 13; iter: 200; batch classifier loss: 0.421397; batch adversarial loss: 0.614492\n",
            "epoch 14; iter: 0; batch classifier loss: 0.420242; batch adversarial loss: 0.594976\n",
            "epoch 14; iter: 200; batch classifier loss: 0.398356; batch adversarial loss: 0.597202\n",
            "epoch 15; iter: 0; batch classifier loss: 0.440532; batch adversarial loss: 0.657663\n",
            "epoch 15; iter: 200; batch classifier loss: 0.455101; batch adversarial loss: 0.632755\n",
            "epoch 16; iter: 0; batch classifier loss: 0.433528; batch adversarial loss: 0.686744\n",
            "epoch 16; iter: 200; batch classifier loss: 0.500628; batch adversarial loss: 0.633963\n",
            "epoch 17; iter: 0; batch classifier loss: 0.489593; batch adversarial loss: 0.634303\n",
            "epoch 17; iter: 200; batch classifier loss: 0.446056; batch adversarial loss: 0.660886\n",
            "epoch 18; iter: 0; batch classifier loss: 0.502946; batch adversarial loss: 0.634557\n",
            "epoch 18; iter: 200; batch classifier loss: 0.438047; batch adversarial loss: 0.626990\n",
            "epoch 19; iter: 0; batch classifier loss: 0.439679; batch adversarial loss: 0.651083\n",
            "epoch 19; iter: 200; batch classifier loss: 0.427457; batch adversarial loss: 0.599583\n",
            "epoch 20; iter: 0; batch classifier loss: 0.447236; batch adversarial loss: 0.589468\n",
            "epoch 20; iter: 200; batch classifier loss: 0.447766; batch adversarial loss: 0.612067\n",
            "epoch 21; iter: 0; batch classifier loss: 0.413583; batch adversarial loss: 0.673782\n",
            "epoch 21; iter: 200; batch classifier loss: 0.396558; batch adversarial loss: 0.625680\n",
            "epoch 22; iter: 0; batch classifier loss: 0.449421; batch adversarial loss: 0.625105\n",
            "epoch 22; iter: 200; batch classifier loss: 0.468526; batch adversarial loss: 0.711003\n",
            "epoch 23; iter: 0; batch classifier loss: 0.510415; batch adversarial loss: 0.631784\n",
            "epoch 23; iter: 200; batch classifier loss: 0.461114; batch adversarial loss: 0.600116\n",
            "epoch 24; iter: 0; batch classifier loss: 0.451420; batch adversarial loss: 0.634190\n",
            "epoch 24; iter: 200; batch classifier loss: 0.430958; batch adversarial loss: 0.611210\n",
            "epoch 25; iter: 0; batch classifier loss: 0.422497; batch adversarial loss: 0.598122\n",
            "epoch 25; iter: 200; batch classifier loss: 0.460828; batch adversarial loss: 0.678342\n",
            "epoch 26; iter: 0; batch classifier loss: 0.394061; batch adversarial loss: 0.631819\n",
            "epoch 26; iter: 200; batch classifier loss: 0.434385; batch adversarial loss: 0.610528\n",
            "epoch 27; iter: 0; batch classifier loss: 0.409100; batch adversarial loss: 0.666674\n",
            "epoch 27; iter: 200; batch classifier loss: 0.386352; batch adversarial loss: 0.618955\n",
            "epoch 28; iter: 0; batch classifier loss: 0.448908; batch adversarial loss: 0.652172\n",
            "epoch 28; iter: 200; batch classifier loss: 0.491955; batch adversarial loss: 0.607061\n",
            "epoch 29; iter: 0; batch classifier loss: 0.531639; batch adversarial loss: 0.674338\n",
            "epoch 29; iter: 200; batch classifier loss: 0.417608; batch adversarial loss: 0.647782\n",
            "epoch 30; iter: 0; batch classifier loss: 0.565238; batch adversarial loss: 0.674227\n",
            "epoch 30; iter: 200; batch classifier loss: 0.446190; batch adversarial loss: 0.668724\n",
            "epoch 31; iter: 0; batch classifier loss: 0.434384; batch adversarial loss: 0.610276\n",
            "epoch 31; iter: 200; batch classifier loss: 0.488445; batch adversarial loss: 0.636142\n",
            "epoch 32; iter: 0; batch classifier loss: 0.435339; batch adversarial loss: 0.584430\n",
            "epoch 32; iter: 200; batch classifier loss: 0.413995; batch adversarial loss: 0.652751\n",
            "epoch 33; iter: 0; batch classifier loss: 0.380182; batch adversarial loss: 0.612428\n",
            "epoch 33; iter: 200; batch classifier loss: 0.387079; batch adversarial loss: 0.600718\n",
            "epoch 34; iter: 0; batch classifier loss: 0.534591; batch adversarial loss: 0.608823\n",
            "epoch 34; iter: 200; batch classifier loss: 0.430060; batch adversarial loss: 0.615759\n",
            "epoch 35; iter: 0; batch classifier loss: 0.490097; batch adversarial loss: 0.570996\n",
            "epoch 35; iter: 200; batch classifier loss: 0.484950; batch adversarial loss: 0.600240\n",
            "epoch 36; iter: 0; batch classifier loss: 0.442498; batch adversarial loss: 0.651779\n",
            "epoch 36; iter: 200; batch classifier loss: 0.436016; batch adversarial loss: 0.639240\n",
            "epoch 37; iter: 0; batch classifier loss: 0.484260; batch adversarial loss: 0.606207\n",
            "epoch 37; iter: 200; batch classifier loss: 0.478150; batch adversarial loss: 0.621664\n",
            "epoch 38; iter: 0; batch classifier loss: 0.411528; batch adversarial loss: 0.610318\n",
            "epoch 38; iter: 200; batch classifier loss: 0.616695; batch adversarial loss: 0.581251\n",
            "epoch 39; iter: 0; batch classifier loss: 0.516010; batch adversarial loss: 0.641051\n",
            "epoch 39; iter: 200; batch classifier loss: 0.524060; batch adversarial loss: 0.581050\n",
            "epoch 40; iter: 0; batch classifier loss: 0.489941; batch adversarial loss: 0.623702\n",
            "epoch 40; iter: 200; batch classifier loss: 0.424468; batch adversarial loss: 0.596638\n",
            "epoch 41; iter: 0; batch classifier loss: 0.470999; batch adversarial loss: 0.590314\n",
            "epoch 41; iter: 200; batch classifier loss: 0.511800; batch adversarial loss: 0.608444\n",
            "epoch 42; iter: 0; batch classifier loss: 0.424674; batch adversarial loss: 0.632757\n",
            "epoch 42; iter: 200; batch classifier loss: 0.385089; batch adversarial loss: 0.648443\n",
            "epoch 43; iter: 0; batch classifier loss: 0.393192; batch adversarial loss: 0.639165\n",
            "epoch 43; iter: 200; batch classifier loss: 0.575033; batch adversarial loss: 0.623862\n",
            "epoch 44; iter: 0; batch classifier loss: 0.391928; batch adversarial loss: 0.681748\n",
            "epoch 44; iter: 200; batch classifier loss: 0.374356; batch adversarial loss: 0.578727\n",
            "epoch 45; iter: 0; batch classifier loss: 0.344232; batch adversarial loss: 0.657368\n",
            "epoch 45; iter: 200; batch classifier loss: 0.567100; batch adversarial loss: 0.593653\n",
            "epoch 46; iter: 0; batch classifier loss: 0.455250; batch adversarial loss: 0.573260\n",
            "epoch 46; iter: 200; batch classifier loss: 0.492896; batch adversarial loss: 0.579612\n",
            "epoch 47; iter: 0; batch classifier loss: 0.498735; batch adversarial loss: 0.661317\n",
            "epoch 47; iter: 200; batch classifier loss: 0.529069; batch adversarial loss: 0.646257\n",
            "epoch 48; iter: 0; batch classifier loss: 0.402388; batch adversarial loss: 0.591364\n",
            "epoch 48; iter: 200; batch classifier loss: 0.439800; batch adversarial loss: 0.627033\n",
            "epoch 49; iter: 0; batch classifier loss: 0.431943; batch adversarial loss: 0.660116\n",
            "epoch 49; iter: 200; batch classifier loss: 0.448294; batch adversarial loss: 0.630870\n",
            "epoch 0; iter: 0; batch classifier loss: 0.707901; batch adversarial loss: 0.688588\n",
            "epoch 0; iter: 200; batch classifier loss: 0.760080; batch adversarial loss: 0.672008\n",
            "epoch 1; iter: 0; batch classifier loss: 0.662271; batch adversarial loss: 0.652210\n",
            "epoch 1; iter: 200; batch classifier loss: 0.774438; batch adversarial loss: 0.647143\n",
            "epoch 2; iter: 0; batch classifier loss: 0.666870; batch adversarial loss: 0.635655\n",
            "epoch 2; iter: 200; batch classifier loss: 0.599494; batch adversarial loss: 0.678352\n",
            "epoch 3; iter: 0; batch classifier loss: 0.817059; batch adversarial loss: 0.629880\n",
            "epoch 3; iter: 200; batch classifier loss: 0.820312; batch adversarial loss: 0.612714\n",
            "epoch 4; iter: 0; batch classifier loss: 0.823818; batch adversarial loss: 0.686720\n",
            "epoch 4; iter: 200; batch classifier loss: 0.989392; batch adversarial loss: 0.642756\n",
            "epoch 5; iter: 0; batch classifier loss: 0.916591; batch adversarial loss: 0.648482\n",
            "epoch 5; iter: 200; batch classifier loss: 0.913295; batch adversarial loss: 0.644681\n",
            "epoch 6; iter: 0; batch classifier loss: 0.917769; batch adversarial loss: 0.590949\n",
            "epoch 6; iter: 200; batch classifier loss: 0.564703; batch adversarial loss: 0.605647\n",
            "epoch 7; iter: 0; batch classifier loss: 0.593393; batch adversarial loss: 0.670596\n",
            "epoch 7; iter: 200; batch classifier loss: 0.690520; batch adversarial loss: 0.590543\n",
            "epoch 8; iter: 0; batch classifier loss: 0.542513; batch adversarial loss: 0.655234\n",
            "epoch 8; iter: 200; batch classifier loss: 0.482915; batch adversarial loss: 0.644343\n",
            "epoch 9; iter: 0; batch classifier loss: 0.680943; batch adversarial loss: 0.632555\n",
            "epoch 9; iter: 200; batch classifier loss: 0.795416; batch adversarial loss: 0.623013\n",
            "epoch 10; iter: 0; batch classifier loss: 0.537935; batch adversarial loss: 0.650428\n",
            "epoch 10; iter: 200; batch classifier loss: 0.949312; batch adversarial loss: 0.673622\n",
            "epoch 11; iter: 0; batch classifier loss: 0.856142; batch adversarial loss: 0.614961\n",
            "epoch 11; iter: 200; batch classifier loss: 0.725193; batch adversarial loss: 0.640559\n",
            "epoch 12; iter: 0; batch classifier loss: 1.008439; batch adversarial loss: 0.586023\n",
            "epoch 12; iter: 200; batch classifier loss: 0.722541; batch adversarial loss: 0.642268\n",
            "epoch 13; iter: 0; batch classifier loss: 0.705608; batch adversarial loss: 0.660204\n",
            "epoch 13; iter: 200; batch classifier loss: 0.582997; batch adversarial loss: 0.673066\n",
            "epoch 14; iter: 0; batch classifier loss: 0.662093; batch adversarial loss: 0.679993\n",
            "epoch 14; iter: 200; batch classifier loss: 0.668196; batch adversarial loss: 0.626843\n",
            "epoch 15; iter: 0; batch classifier loss: 0.543961; batch adversarial loss: 0.680224\n",
            "epoch 15; iter: 200; batch classifier loss: 0.623954; batch adversarial loss: 0.628158\n",
            "epoch 16; iter: 0; batch classifier loss: 0.575675; batch adversarial loss: 0.675100\n",
            "epoch 16; iter: 200; batch classifier loss: 0.528277; batch adversarial loss: 0.647145\n",
            "epoch 17; iter: 0; batch classifier loss: 0.594118; batch adversarial loss: 0.638869\n",
            "epoch 17; iter: 200; batch classifier loss: 0.683579; batch adversarial loss: 0.663784\n",
            "epoch 18; iter: 0; batch classifier loss: 0.540834; batch adversarial loss: 0.621918\n",
            "epoch 18; iter: 200; batch classifier loss: 0.703055; batch adversarial loss: 0.647059\n",
            "epoch 19; iter: 0; batch classifier loss: 0.526232; batch adversarial loss: 0.649923\n",
            "epoch 19; iter: 200; batch classifier loss: 0.620590; batch adversarial loss: 0.650839\n",
            "epoch 20; iter: 0; batch classifier loss: 0.574954; batch adversarial loss: 0.600907\n",
            "epoch 20; iter: 200; batch classifier loss: 0.687685; batch adversarial loss: 0.680825\n",
            "epoch 21; iter: 0; batch classifier loss: 0.608972; batch adversarial loss: 0.601356\n",
            "epoch 21; iter: 200; batch classifier loss: 0.647066; batch adversarial loss: 0.633206\n",
            "epoch 22; iter: 0; batch classifier loss: 0.538876; batch adversarial loss: 0.600113\n",
            "epoch 22; iter: 200; batch classifier loss: 0.566524; batch adversarial loss: 0.611475\n",
            "epoch 23; iter: 0; batch classifier loss: 0.500909; batch adversarial loss: 0.643816\n",
            "epoch 23; iter: 200; batch classifier loss: 0.469872; batch adversarial loss: 0.628125\n",
            "epoch 24; iter: 0; batch classifier loss: 0.504514; batch adversarial loss: 0.668307\n",
            "epoch 24; iter: 200; batch classifier loss: 0.863664; batch adversarial loss: 0.647162\n",
            "epoch 25; iter: 0; batch classifier loss: 0.598655; batch adversarial loss: 0.658531\n",
            "epoch 25; iter: 200; batch classifier loss: 0.611197; batch adversarial loss: 0.657915\n",
            "epoch 26; iter: 0; batch classifier loss: 0.619504; batch adversarial loss: 0.643425\n",
            "epoch 26; iter: 200; batch classifier loss: 0.616975; batch adversarial loss: 0.618272\n",
            "epoch 27; iter: 0; batch classifier loss: 0.582970; batch adversarial loss: 0.676009\n",
            "epoch 27; iter: 200; batch classifier loss: 0.698880; batch adversarial loss: 0.591213\n",
            "epoch 28; iter: 0; batch classifier loss: 0.711762; batch adversarial loss: 0.641597\n",
            "epoch 28; iter: 200; batch classifier loss: 0.459564; batch adversarial loss: 0.640427\n",
            "epoch 29; iter: 0; batch classifier loss: 0.802379; batch adversarial loss: 0.595861\n",
            "epoch 29; iter: 200; batch classifier loss: 0.622413; batch adversarial loss: 0.646681\n",
            "epoch 30; iter: 0; batch classifier loss: 0.546743; batch adversarial loss: 0.655698\n",
            "epoch 30; iter: 200; batch classifier loss: 0.963923; batch adversarial loss: 0.618162\n",
            "epoch 31; iter: 0; batch classifier loss: 0.757023; batch adversarial loss: 0.611403\n",
            "epoch 31; iter: 200; batch classifier loss: 0.681341; batch adversarial loss: 0.628337\n",
            "epoch 32; iter: 0; batch classifier loss: 0.449575; batch adversarial loss: 0.607072\n",
            "epoch 32; iter: 200; batch classifier loss: 0.643161; batch adversarial loss: 0.662598\n",
            "epoch 33; iter: 0; batch classifier loss: 0.602538; batch adversarial loss: 0.687318\n",
            "epoch 33; iter: 200; batch classifier loss: 0.794501; batch adversarial loss: 0.629203\n",
            "epoch 34; iter: 0; batch classifier loss: 0.580199; batch adversarial loss: 0.627941\n",
            "epoch 34; iter: 200; batch classifier loss: 0.650696; batch adversarial loss: 0.589579\n",
            "epoch 35; iter: 0; batch classifier loss: 0.641591; batch adversarial loss: 0.659051\n",
            "epoch 35; iter: 200; batch classifier loss: 0.636623; batch adversarial loss: 0.587905\n",
            "epoch 36; iter: 0; batch classifier loss: 0.664522; batch adversarial loss: 0.623564\n",
            "epoch 36; iter: 200; batch classifier loss: 0.535290; batch adversarial loss: 0.639813\n",
            "epoch 37; iter: 0; batch classifier loss: 0.668425; batch adversarial loss: 0.582771\n",
            "epoch 37; iter: 200; batch classifier loss: 0.772796; batch adversarial loss: 0.604609\n",
            "epoch 38; iter: 0; batch classifier loss: 0.571493; batch adversarial loss: 0.621059\n",
            "epoch 38; iter: 200; batch classifier loss: 0.637158; batch adversarial loss: 0.581273\n",
            "epoch 39; iter: 0; batch classifier loss: 0.615772; batch adversarial loss: 0.605829\n",
            "epoch 39; iter: 200; batch classifier loss: 0.571623; batch adversarial loss: 0.601372\n",
            "epoch 40; iter: 0; batch classifier loss: 0.927339; batch adversarial loss: 0.574340\n",
            "epoch 40; iter: 200; batch classifier loss: 0.623492; batch adversarial loss: 0.617477\n",
            "epoch 41; iter: 0; batch classifier loss: 0.654811; batch adversarial loss: 0.608236\n",
            "epoch 41; iter: 200; batch classifier loss: 0.837302; batch adversarial loss: 0.594730\n",
            "epoch 42; iter: 0; batch classifier loss: 0.600735; batch adversarial loss: 0.696075\n",
            "epoch 42; iter: 200; batch classifier loss: 0.622976; batch adversarial loss: 0.617496\n",
            "epoch 43; iter: 0; batch classifier loss: 0.531463; batch adversarial loss: 0.660068\n",
            "epoch 43; iter: 200; batch classifier loss: 0.504094; batch adversarial loss: 0.642231\n",
            "epoch 44; iter: 0; batch classifier loss: 0.597656; batch adversarial loss: 0.694541\n",
            "epoch 44; iter: 200; batch classifier loss: 0.433624; batch adversarial loss: 0.698865\n",
            "epoch 45; iter: 0; batch classifier loss: 0.580649; batch adversarial loss: 0.599522\n",
            "epoch 45; iter: 200; batch classifier loss: 0.563111; batch adversarial loss: 0.641093\n",
            "epoch 46; iter: 0; batch classifier loss: 0.515640; batch adversarial loss: 0.649122\n",
            "epoch 46; iter: 200; batch classifier loss: 0.803030; batch adversarial loss: 0.601082\n",
            "epoch 47; iter: 0; batch classifier loss: 0.634364; batch adversarial loss: 0.606615\n",
            "epoch 47; iter: 200; batch classifier loss: 0.537389; batch adversarial loss: 0.643369\n",
            "epoch 48; iter: 0; batch classifier loss: 0.622612; batch adversarial loss: 0.621039\n",
            "epoch 48; iter: 200; batch classifier loss: 0.549471; batch adversarial loss: 0.702212\n",
            "epoch 49; iter: 0; batch classifier loss: 0.627300; batch adversarial loss: 0.596963\n",
            "epoch 49; iter: 200; batch classifier loss: 0.645780; batch adversarial loss: 0.651846\n",
            "epoch 0; iter: 0; batch classifier loss: 0.736207; batch adversarial loss: 1.052727\n",
            "epoch 0; iter: 200; batch classifier loss: 1.413437; batch adversarial loss: 1.164756\n",
            "epoch 1; iter: 0; batch classifier loss: 1.440706; batch adversarial loss: 1.041278\n",
            "epoch 1; iter: 200; batch classifier loss: 1.812764; batch adversarial loss: 0.854296\n",
            "epoch 2; iter: 0; batch classifier loss: 1.938040; batch adversarial loss: 0.812253\n",
            "epoch 2; iter: 200; batch classifier loss: 2.123777; batch adversarial loss: 0.725265\n",
            "epoch 3; iter: 0; batch classifier loss: 2.095299; batch adversarial loss: 0.678047\n",
            "epoch 3; iter: 200; batch classifier loss: 1.866160; batch adversarial loss: 0.592825\n",
            "epoch 4; iter: 0; batch classifier loss: 1.298334; batch adversarial loss: 0.599390\n",
            "epoch 4; iter: 200; batch classifier loss: 1.853525; batch adversarial loss: 0.594012\n",
            "epoch 5; iter: 0; batch classifier loss: 1.840335; batch adversarial loss: 0.671576\n",
            "epoch 5; iter: 200; batch classifier loss: 2.176186; batch adversarial loss: 0.638784\n",
            "epoch 6; iter: 0; batch classifier loss: 1.888662; batch adversarial loss: 0.624020\n",
            "epoch 6; iter: 200; batch classifier loss: 1.744756; batch adversarial loss: 0.678731\n",
            "epoch 7; iter: 0; batch classifier loss: 2.283998; batch adversarial loss: 0.613054\n",
            "epoch 7; iter: 200; batch classifier loss: 2.527335; batch adversarial loss: 0.612658\n",
            "epoch 8; iter: 0; batch classifier loss: 1.767400; batch adversarial loss: 0.622760\n",
            "epoch 8; iter: 200; batch classifier loss: 1.587087; batch adversarial loss: 0.617255\n",
            "epoch 9; iter: 0; batch classifier loss: 2.029272; batch adversarial loss: 0.638304\n",
            "epoch 9; iter: 200; batch classifier loss: 1.583249; batch adversarial loss: 0.638303\n",
            "epoch 10; iter: 0; batch classifier loss: 2.065299; batch adversarial loss: 0.659570\n",
            "epoch 10; iter: 200; batch classifier loss: 2.135785; batch adversarial loss: 0.632951\n",
            "epoch 11; iter: 0; batch classifier loss: 1.977641; batch adversarial loss: 0.654340\n",
            "epoch 11; iter: 200; batch classifier loss: 2.044224; batch adversarial loss: 0.616705\n",
            "epoch 12; iter: 0; batch classifier loss: 2.172052; batch adversarial loss: 0.697317\n",
            "epoch 12; iter: 200; batch classifier loss: 2.567410; batch adversarial loss: 0.670526\n",
            "epoch 13; iter: 0; batch classifier loss: 1.911522; batch adversarial loss: 0.616834\n",
            "epoch 13; iter: 200; batch classifier loss: 1.691176; batch adversarial loss: 0.574226\n",
            "epoch 14; iter: 0; batch classifier loss: 1.808469; batch adversarial loss: 0.686996\n",
            "epoch 14; iter: 200; batch classifier loss: 1.846309; batch adversarial loss: 0.616539\n",
            "epoch 15; iter: 0; batch classifier loss: 1.717306; batch adversarial loss: 0.611394\n",
            "epoch 15; iter: 200; batch classifier loss: 1.638606; batch adversarial loss: 0.665058\n",
            "epoch 16; iter: 0; batch classifier loss: 1.471102; batch adversarial loss: 0.681305\n",
            "epoch 16; iter: 200; batch classifier loss: 2.223157; batch adversarial loss: 0.638303\n",
            "epoch 17; iter: 0; batch classifier loss: 2.038835; batch adversarial loss: 0.654523\n",
            "epoch 17; iter: 200; batch classifier loss: 1.817379; batch adversarial loss: 0.670848\n",
            "epoch 18; iter: 0; batch classifier loss: 2.129038; batch adversarial loss: 0.611288\n",
            "epoch 18; iter: 200; batch classifier loss: 2.054881; batch adversarial loss: 0.670772\n",
            "epoch 19; iter: 0; batch classifier loss: 1.362911; batch adversarial loss: 0.638315\n",
            "epoch 19; iter: 200; batch classifier loss: 1.933698; batch adversarial loss: 0.638307\n",
            "epoch 20; iter: 0; batch classifier loss: 1.587390; batch adversarial loss: 0.606114\n",
            "epoch 20; iter: 200; batch classifier loss: 1.855856; batch adversarial loss: 0.659881\n",
            "epoch 21; iter: 0; batch classifier loss: 1.462565; batch adversarial loss: 0.713139\n",
            "epoch 21; iter: 200; batch classifier loss: 1.676820; batch adversarial loss: 0.611569\n",
            "epoch 22; iter: 0; batch classifier loss: 1.764146; batch adversarial loss: 0.649112\n",
            "epoch 22; iter: 200; batch classifier loss: 1.911499; batch adversarial loss: 0.595004\n",
            "epoch 23; iter: 0; batch classifier loss: 1.574963; batch adversarial loss: 0.686878\n",
            "epoch 23; iter: 200; batch classifier loss: 1.983060; batch adversarial loss: 0.627441\n",
            "epoch 24; iter: 0; batch classifier loss: 1.723013; batch adversarial loss: 0.649039\n",
            "epoch 24; iter: 200; batch classifier loss: 2.341680; batch adversarial loss: 0.600527\n",
            "epoch 25; iter: 0; batch classifier loss: 1.720170; batch adversarial loss: 0.643685\n",
            "epoch 25; iter: 200; batch classifier loss: 1.998101; batch adversarial loss: 0.616729\n",
            "epoch 26; iter: 0; batch classifier loss: 1.585906; batch adversarial loss: 0.638313\n",
            "epoch 26; iter: 200; batch classifier loss: 1.700448; batch adversarial loss: 0.692103\n",
            "epoch 27; iter: 0; batch classifier loss: 1.857499; batch adversarial loss: 0.632920\n",
            "epoch 27; iter: 200; batch classifier loss: 2.068346; batch adversarial loss: 0.600356\n",
            "epoch 28; iter: 0; batch classifier loss: 1.259271; batch adversarial loss: 0.670560\n",
            "epoch 28; iter: 200; batch classifier loss: 2.238592; batch adversarial loss: 0.643655\n",
            "epoch 29; iter: 0; batch classifier loss: 1.541403; batch adversarial loss: 0.665314\n",
            "epoch 29; iter: 200; batch classifier loss: 1.763817; batch adversarial loss: 0.627573\n",
            "epoch 30; iter: 0; batch classifier loss: 1.358075; batch adversarial loss: 0.659915\n",
            "epoch 30; iter: 200; batch classifier loss: 1.326462; batch adversarial loss: 0.568613\n",
            "epoch 31; iter: 0; batch classifier loss: 2.372813; batch adversarial loss: 0.665301\n",
            "epoch 31; iter: 200; batch classifier loss: 1.872107; batch adversarial loss: 0.627446\n",
            "epoch 32; iter: 0; batch classifier loss: 1.689225; batch adversarial loss: 0.632927\n",
            "epoch 32; iter: 200; batch classifier loss: 1.483007; batch adversarial loss: 0.659841\n",
            "epoch 33; iter: 0; batch classifier loss: 1.424208; batch adversarial loss: 0.643688\n",
            "epoch 33; iter: 200; batch classifier loss: 2.216596; batch adversarial loss: 0.627519\n",
            "epoch 34; iter: 0; batch classifier loss: 1.591808; batch adversarial loss: 0.697550\n",
            "epoch 34; iter: 200; batch classifier loss: 1.820610; batch adversarial loss: 0.627560\n",
            "epoch 35; iter: 0; batch classifier loss: 1.524402; batch adversarial loss: 0.595063\n",
            "epoch 35; iter: 200; batch classifier loss: 1.941168; batch adversarial loss: 0.573990\n",
            "epoch 36; iter: 0; batch classifier loss: 1.418028; batch adversarial loss: 0.708180\n",
            "epoch 36; iter: 200; batch classifier loss: 1.544321; batch adversarial loss: 0.659929\n",
            "epoch 37; iter: 0; batch classifier loss: 1.302253; batch adversarial loss: 0.632926\n",
            "epoch 37; iter: 200; batch classifier loss: 1.235322; batch adversarial loss: 0.606199\n",
            "epoch 38; iter: 0; batch classifier loss: 1.173239; batch adversarial loss: 0.656543\n",
            "epoch 38; iter: 200; batch classifier loss: 2.617671; batch adversarial loss: 0.632855\n",
            "epoch 39; iter: 0; batch classifier loss: 2.282841; batch adversarial loss: 0.627552\n",
            "epoch 39; iter: 200; batch classifier loss: 1.941672; batch adversarial loss: 0.675853\n",
            "epoch 40; iter: 0; batch classifier loss: 2.268981; batch adversarial loss: 0.622189\n",
            "epoch 40; iter: 200; batch classifier loss: 1.906313; batch adversarial loss: 0.670424\n",
            "epoch 41; iter: 0; batch classifier loss: 1.622374; batch adversarial loss: 0.611272\n",
            "epoch 41; iter: 200; batch classifier loss: 1.846997; batch adversarial loss: 0.616679\n",
            "epoch 42; iter: 0; batch classifier loss: 2.662579; batch adversarial loss: 0.659663\n",
            "epoch 42; iter: 200; batch classifier loss: 1.753554; batch adversarial loss: 0.670655\n",
            "epoch 43; iter: 0; batch classifier loss: 2.551761; batch adversarial loss: 0.638310\n",
            "epoch 43; iter: 200; batch classifier loss: 2.560961; batch adversarial loss: 0.638308\n",
            "epoch 44; iter: 0; batch classifier loss: 2.504761; batch adversarial loss: 0.670693\n",
            "epoch 44; iter: 200; batch classifier loss: 2.405045; batch adversarial loss: 0.649065\n",
            "epoch 45; iter: 0; batch classifier loss: 2.397647; batch adversarial loss: 0.638310\n",
            "epoch 45; iter: 200; batch classifier loss: 2.674896; batch adversarial loss: 0.616801\n",
            "epoch 46; iter: 0; batch classifier loss: 2.082785; batch adversarial loss: 0.632923\n",
            "epoch 46; iter: 200; batch classifier loss: 2.717522; batch adversarial loss: 0.665081\n",
            "epoch 47; iter: 0; batch classifier loss: 2.147820; batch adversarial loss: 0.632941\n",
            "epoch 47; iter: 200; batch classifier loss: 2.614110; batch adversarial loss: 0.611473\n",
            "epoch 48; iter: 0; batch classifier loss: 1.909009; batch adversarial loss: 0.632922\n",
            "epoch 48; iter: 200; batch classifier loss: 1.974765; batch adversarial loss: 0.675757\n",
            "epoch 49; iter: 0; batch classifier loss: 1.746937; batch adversarial loss: 0.643729\n",
            "epoch 49; iter: 200; batch classifier loss: 2.018284; batch adversarial loss: 0.605718\n",
            "\n",
            " Welcome to Fold number 4 \n",
            "\n",
            "epoch 0; iter: 0; batch classifier loss: 0.678558; batch adversarial loss: 0.626388\n",
            "epoch 0; iter: 200; batch classifier loss: 0.488727; batch adversarial loss: 0.651321\n",
            "epoch 1; iter: 0; batch classifier loss: 0.402861; batch adversarial loss: 0.649920\n",
            "epoch 1; iter: 200; batch classifier loss: 0.489827; batch adversarial loss: 0.664327\n",
            "epoch 2; iter: 0; batch classifier loss: 0.366099; batch adversarial loss: 0.600027\n",
            "epoch 2; iter: 200; batch classifier loss: 0.388946; batch adversarial loss: 0.562848\n",
            "epoch 3; iter: 0; batch classifier loss: 0.511565; batch adversarial loss: 0.613964\n",
            "epoch 3; iter: 200; batch classifier loss: 0.431961; batch adversarial loss: 0.640752\n",
            "epoch 4; iter: 0; batch classifier loss: 0.407805; batch adversarial loss: 0.601428\n",
            "epoch 4; iter: 200; batch classifier loss: 0.404456; batch adversarial loss: 0.599372\n",
            "epoch 5; iter: 0; batch classifier loss: 0.439047; batch adversarial loss: 0.594765\n",
            "epoch 5; iter: 200; batch classifier loss: 0.491661; batch adversarial loss: 0.553048\n",
            "epoch 6; iter: 0; batch classifier loss: 0.425922; batch adversarial loss: 0.555632\n",
            "epoch 6; iter: 200; batch classifier loss: 0.361415; batch adversarial loss: 0.643634\n",
            "epoch 7; iter: 0; batch classifier loss: 0.533799; batch adversarial loss: 0.540738\n",
            "epoch 7; iter: 200; batch classifier loss: 0.348724; batch adversarial loss: 0.652974\n",
            "epoch 8; iter: 0; batch classifier loss: 0.389180; batch adversarial loss: 0.566752\n",
            "epoch 8; iter: 200; batch classifier loss: 0.463371; batch adversarial loss: 0.546267\n",
            "epoch 9; iter: 0; batch classifier loss: 0.445631; batch adversarial loss: 0.493119\n",
            "epoch 9; iter: 200; batch classifier loss: 0.439419; batch adversarial loss: 0.565857\n",
            "epoch 10; iter: 0; batch classifier loss: 0.400909; batch adversarial loss: 0.538824\n",
            "epoch 10; iter: 200; batch classifier loss: 0.496365; batch adversarial loss: 0.470534\n",
            "epoch 11; iter: 0; batch classifier loss: 0.419020; batch adversarial loss: 0.508608\n",
            "epoch 11; iter: 200; batch classifier loss: 0.446890; batch adversarial loss: 0.501894\n",
            "epoch 12; iter: 0; batch classifier loss: 0.456835; batch adversarial loss: 0.538046\n",
            "epoch 12; iter: 200; batch classifier loss: 0.460983; batch adversarial loss: 0.495949\n",
            "epoch 13; iter: 0; batch classifier loss: 0.431243; batch adversarial loss: 0.555385\n",
            "epoch 13; iter: 200; batch classifier loss: 0.409127; batch adversarial loss: 0.554043\n",
            "epoch 14; iter: 0; batch classifier loss: 0.426148; batch adversarial loss: 0.527497\n",
            "epoch 14; iter: 200; batch classifier loss: 0.419777; batch adversarial loss: 0.546105\n",
            "epoch 15; iter: 0; batch classifier loss: 0.435428; batch adversarial loss: 0.530986\n",
            "epoch 15; iter: 200; batch classifier loss: 0.393930; batch adversarial loss: 0.424352\n",
            "epoch 16; iter: 0; batch classifier loss: 0.358185; batch adversarial loss: 0.473157\n",
            "epoch 16; iter: 200; batch classifier loss: 0.382276; batch adversarial loss: 0.510445\n",
            "epoch 17; iter: 0; batch classifier loss: 0.433638; batch adversarial loss: 0.484965\n",
            "epoch 17; iter: 200; batch classifier loss: 0.372933; batch adversarial loss: 0.443780\n",
            "epoch 18; iter: 0; batch classifier loss: 0.439331; batch adversarial loss: 0.493151\n",
            "epoch 18; iter: 200; batch classifier loss: 0.459867; batch adversarial loss: 0.572537\n",
            "epoch 19; iter: 0; batch classifier loss: 0.380492; batch adversarial loss: 0.524799\n",
            "epoch 19; iter: 200; batch classifier loss: 0.380166; batch adversarial loss: 0.549221\n",
            "epoch 20; iter: 0; batch classifier loss: 0.511488; batch adversarial loss: 0.510145\n",
            "epoch 20; iter: 200; batch classifier loss: 0.443280; batch adversarial loss: 0.546147\n",
            "epoch 21; iter: 0; batch classifier loss: 0.426110; batch adversarial loss: 0.514280\n",
            "epoch 21; iter: 200; batch classifier loss: 0.506239; batch adversarial loss: 0.537901\n",
            "epoch 22; iter: 0; batch classifier loss: 0.411135; batch adversarial loss: 0.526309\n",
            "epoch 22; iter: 200; batch classifier loss: 0.426643; batch adversarial loss: 0.589252\n",
            "epoch 23; iter: 0; batch classifier loss: 0.355381; batch adversarial loss: 0.444005\n",
            "epoch 23; iter: 200; batch classifier loss: 0.428051; batch adversarial loss: 0.502545\n",
            "epoch 24; iter: 0; batch classifier loss: 0.387892; batch adversarial loss: 0.543983\n",
            "epoch 24; iter: 200; batch classifier loss: 0.397374; batch adversarial loss: 0.486928\n",
            "epoch 25; iter: 0; batch classifier loss: 0.496674; batch adversarial loss: 0.531924\n",
            "epoch 25; iter: 200; batch classifier loss: 0.392236; batch adversarial loss: 0.538794\n",
            "epoch 26; iter: 0; batch classifier loss: 0.488960; batch adversarial loss: 0.611101\n",
            "epoch 26; iter: 200; batch classifier loss: 0.324681; batch adversarial loss: 0.654962\n",
            "epoch 27; iter: 0; batch classifier loss: 0.313882; batch adversarial loss: 0.583061\n",
            "epoch 27; iter: 200; batch classifier loss: 0.358165; batch adversarial loss: 0.586996\n",
            "epoch 28; iter: 0; batch classifier loss: 0.420462; batch adversarial loss: 0.598241\n",
            "epoch 28; iter: 200; batch classifier loss: 0.407842; batch adversarial loss: 0.525016\n",
            "epoch 29; iter: 0; batch classifier loss: 0.455257; batch adversarial loss: 0.585690\n",
            "epoch 29; iter: 200; batch classifier loss: 0.507199; batch adversarial loss: 0.546507\n",
            "epoch 30; iter: 0; batch classifier loss: 0.512164; batch adversarial loss: 0.553377\n",
            "epoch 30; iter: 200; batch classifier loss: 0.413694; batch adversarial loss: 0.563579\n",
            "epoch 31; iter: 0; batch classifier loss: 0.392280; batch adversarial loss: 0.504719\n",
            "epoch 31; iter: 200; batch classifier loss: 0.471483; batch adversarial loss: 0.491021\n",
            "epoch 32; iter: 0; batch classifier loss: 0.573294; batch adversarial loss: 0.532236\n",
            "epoch 32; iter: 200; batch classifier loss: 0.429028; batch adversarial loss: 0.511487\n",
            "epoch 33; iter: 0; batch classifier loss: 0.448114; batch adversarial loss: 0.515117\n",
            "epoch 33; iter: 200; batch classifier loss: 0.596422; batch adversarial loss: 0.464384\n",
            "epoch 34; iter: 0; batch classifier loss: 0.362233; batch adversarial loss: 0.476462\n",
            "epoch 34; iter: 200; batch classifier loss: 0.443601; batch adversarial loss: 0.525122\n",
            "epoch 35; iter: 0; batch classifier loss: 0.394968; batch adversarial loss: 0.541918\n",
            "epoch 35; iter: 200; batch classifier loss: 0.406495; batch adversarial loss: 0.489700\n",
            "epoch 36; iter: 0; batch classifier loss: 0.438548; batch adversarial loss: 0.517492\n",
            "epoch 36; iter: 200; batch classifier loss: 0.482663; batch adversarial loss: 0.535983\n",
            "epoch 37; iter: 0; batch classifier loss: 0.450794; batch adversarial loss: 0.527354\n",
            "epoch 37; iter: 200; batch classifier loss: 0.339800; batch adversarial loss: 0.504459\n",
            "epoch 38; iter: 0; batch classifier loss: 0.375596; batch adversarial loss: 0.479889\n",
            "epoch 38; iter: 200; batch classifier loss: 0.460689; batch adversarial loss: 0.488461\n",
            "epoch 39; iter: 0; batch classifier loss: 0.457471; batch adversarial loss: 0.622533\n",
            "epoch 39; iter: 200; batch classifier loss: 0.464999; batch adversarial loss: 0.438225\n",
            "epoch 40; iter: 0; batch classifier loss: 0.353756; batch adversarial loss: 0.546236\n",
            "epoch 40; iter: 200; batch classifier loss: 0.398000; batch adversarial loss: 0.572695\n",
            "epoch 41; iter: 0; batch classifier loss: 0.449041; batch adversarial loss: 0.501279\n",
            "epoch 41; iter: 200; batch classifier loss: 0.452532; batch adversarial loss: 0.536492\n",
            "epoch 42; iter: 0; batch classifier loss: 0.402030; batch adversarial loss: 0.517135\n",
            "epoch 42; iter: 200; batch classifier loss: 0.432815; batch adversarial loss: 0.449538\n",
            "epoch 43; iter: 0; batch classifier loss: 0.424742; batch adversarial loss: 0.577137\n",
            "epoch 43; iter: 200; batch classifier loss: 0.340743; batch adversarial loss: 0.609310\n",
            "epoch 44; iter: 0; batch classifier loss: 0.427184; batch adversarial loss: 0.446137\n",
            "epoch 44; iter: 200; batch classifier loss: 0.484159; batch adversarial loss: 0.475431\n",
            "epoch 45; iter: 0; batch classifier loss: 0.352002; batch adversarial loss: 0.577263\n",
            "epoch 45; iter: 200; batch classifier loss: 0.417496; batch adversarial loss: 0.551768\n",
            "epoch 46; iter: 0; batch classifier loss: 0.449028; batch adversarial loss: 0.481925\n",
            "epoch 46; iter: 200; batch classifier loss: 0.417188; batch adversarial loss: 0.473328\n",
            "epoch 47; iter: 0; batch classifier loss: 0.387927; batch adversarial loss: 0.553919\n",
            "epoch 47; iter: 200; batch classifier loss: 0.427595; batch adversarial loss: 0.605995\n",
            "epoch 48; iter: 0; batch classifier loss: 0.422405; batch adversarial loss: 0.485968\n",
            "epoch 48; iter: 200; batch classifier loss: 0.366100; batch adversarial loss: 0.544122\n",
            "epoch 49; iter: 0; batch classifier loss: 0.466046; batch adversarial loss: 0.526944\n",
            "epoch 49; iter: 200; batch classifier loss: 0.381500; batch adversarial loss: 0.555248\n",
            "epoch 0; iter: 0; batch classifier loss: 0.666207; batch adversarial loss: 0.670108\n",
            "epoch 0; iter: 200; batch classifier loss: 0.380951; batch adversarial loss: 0.663781\n",
            "epoch 1; iter: 0; batch classifier loss: 0.453035; batch adversarial loss: 0.600704\n",
            "epoch 1; iter: 200; batch classifier loss: 0.447854; batch adversarial loss: 0.608075\n",
            "epoch 2; iter: 0; batch classifier loss: 0.488202; batch adversarial loss: 0.595655\n",
            "epoch 2; iter: 200; batch classifier loss: 0.439701; batch adversarial loss: 0.615163\n",
            "epoch 3; iter: 0; batch classifier loss: 0.379459; batch adversarial loss: 0.562210\n",
            "epoch 3; iter: 200; batch classifier loss: 0.464311; batch adversarial loss: 0.546183\n",
            "epoch 4; iter: 0; batch classifier loss: 0.441451; batch adversarial loss: 0.583282\n",
            "epoch 4; iter: 200; batch classifier loss: 0.312310; batch adversarial loss: 0.625481\n",
            "epoch 5; iter: 0; batch classifier loss: 0.508613; batch adversarial loss: 0.576703\n",
            "epoch 5; iter: 200; batch classifier loss: 0.437335; batch adversarial loss: 0.561709\n",
            "epoch 6; iter: 0; batch classifier loss: 0.388901; batch adversarial loss: 0.620964\n",
            "epoch 6; iter: 200; batch classifier loss: 0.362455; batch adversarial loss: 0.608342\n",
            "epoch 7; iter: 0; batch classifier loss: 0.396142; batch adversarial loss: 0.625742\n",
            "epoch 7; iter: 200; batch classifier loss: 0.489793; batch adversarial loss: 0.636410\n",
            "epoch 8; iter: 0; batch classifier loss: 0.364354; batch adversarial loss: 0.640569\n",
            "epoch 8; iter: 200; batch classifier loss: 0.445075; batch adversarial loss: 0.625693\n",
            "epoch 9; iter: 0; batch classifier loss: 0.413762; batch adversarial loss: 0.571523\n",
            "epoch 9; iter: 200; batch classifier loss: 0.412437; batch adversarial loss: 0.559741\n",
            "epoch 10; iter: 0; batch classifier loss: 0.471142; batch adversarial loss: 0.530530\n",
            "epoch 10; iter: 200; batch classifier loss: 0.471461; batch adversarial loss: 0.572567\n",
            "epoch 11; iter: 0; batch classifier loss: 0.475823; batch adversarial loss: 0.566730\n",
            "epoch 11; iter: 200; batch classifier loss: 0.463890; batch adversarial loss: 0.508929\n",
            "epoch 12; iter: 0; batch classifier loss: 0.411575; batch adversarial loss: 0.541877\n",
            "epoch 12; iter: 200; batch classifier loss: 0.422764; batch adversarial loss: 0.533029\n",
            "epoch 13; iter: 0; batch classifier loss: 0.389321; batch adversarial loss: 0.573573\n",
            "epoch 13; iter: 200; batch classifier loss: 0.407011; batch adversarial loss: 0.574563\n",
            "epoch 14; iter: 0; batch classifier loss: 0.432459; batch adversarial loss: 0.628154\n",
            "epoch 14; iter: 200; batch classifier loss: 0.462493; batch adversarial loss: 0.562441\n",
            "epoch 15; iter: 0; batch classifier loss: 0.372270; batch adversarial loss: 0.586508\n",
            "epoch 15; iter: 200; batch classifier loss: 0.420955; batch adversarial loss: 0.545858\n",
            "epoch 16; iter: 0; batch classifier loss: 0.453173; batch adversarial loss: 0.558275\n",
            "epoch 16; iter: 200; batch classifier loss: 0.375433; batch adversarial loss: 0.593227\n",
            "epoch 17; iter: 0; batch classifier loss: 0.431222; batch adversarial loss: 0.521651\n",
            "epoch 17; iter: 200; batch classifier loss: 0.408488; batch adversarial loss: 0.532200\n",
            "epoch 18; iter: 0; batch classifier loss: 0.440898; batch adversarial loss: 0.537950\n",
            "epoch 18; iter: 200; batch classifier loss: 0.405651; batch adversarial loss: 0.555317\n",
            "epoch 19; iter: 0; batch classifier loss: 0.421552; batch adversarial loss: 0.575282\n",
            "epoch 19; iter: 200; batch classifier loss: 0.446176; batch adversarial loss: 0.553262\n",
            "epoch 20; iter: 0; batch classifier loss: 0.430102; batch adversarial loss: 0.577779\n",
            "epoch 20; iter: 200; batch classifier loss: 0.383999; batch adversarial loss: 0.535751\n",
            "epoch 21; iter: 0; batch classifier loss: 0.331643; batch adversarial loss: 0.527290\n",
            "epoch 21; iter: 200; batch classifier loss: 0.465956; batch adversarial loss: 0.484688\n",
            "epoch 22; iter: 0; batch classifier loss: 0.444037; batch adversarial loss: 0.528400\n",
            "epoch 22; iter: 200; batch classifier loss: 0.325243; batch adversarial loss: 0.462153\n",
            "epoch 23; iter: 0; batch classifier loss: 0.466351; batch adversarial loss: 0.421846\n",
            "epoch 23; iter: 200; batch classifier loss: 0.448983; batch adversarial loss: 0.473676\n",
            "epoch 24; iter: 0; batch classifier loss: 0.364236; batch adversarial loss: 0.553171\n",
            "epoch 24; iter: 200; batch classifier loss: 0.453714; batch adversarial loss: 0.496684\n",
            "epoch 25; iter: 0; batch classifier loss: 0.354166; batch adversarial loss: 0.480452\n",
            "epoch 25; iter: 200; batch classifier loss: 0.420100; batch adversarial loss: 0.538572\n",
            "epoch 26; iter: 0; batch classifier loss: 0.446558; batch adversarial loss: 0.498755\n",
            "epoch 26; iter: 200; batch classifier loss: 0.388908; batch adversarial loss: 0.542965\n",
            "epoch 27; iter: 0; batch classifier loss: 0.408521; batch adversarial loss: 0.553396\n",
            "epoch 27; iter: 200; batch classifier loss: 0.411305; batch adversarial loss: 0.516435\n",
            "epoch 28; iter: 0; batch classifier loss: 0.423626; batch adversarial loss: 0.518697\n",
            "epoch 28; iter: 200; batch classifier loss: 0.411754; batch adversarial loss: 0.539504\n",
            "epoch 29; iter: 0; batch classifier loss: 0.497344; batch adversarial loss: 0.584944\n",
            "epoch 29; iter: 200; batch classifier loss: 0.365919; batch adversarial loss: 0.536599\n",
            "epoch 30; iter: 0; batch classifier loss: 0.395151; batch adversarial loss: 0.510862\n",
            "epoch 30; iter: 200; batch classifier loss: 0.340435; batch adversarial loss: 0.563447\n",
            "epoch 31; iter: 0; batch classifier loss: 0.417368; batch adversarial loss: 0.523584\n",
            "epoch 31; iter: 200; batch classifier loss: 0.404608; batch adversarial loss: 0.525719\n",
            "epoch 32; iter: 0; batch classifier loss: 0.523006; batch adversarial loss: 0.527201\n",
            "epoch 32; iter: 200; batch classifier loss: 0.375904; batch adversarial loss: 0.515779\n",
            "epoch 33; iter: 0; batch classifier loss: 0.437267; batch adversarial loss: 0.471589\n",
            "epoch 33; iter: 200; batch classifier loss: 0.367484; batch adversarial loss: 0.550276\n",
            "epoch 34; iter: 0; batch classifier loss: 0.395357; batch adversarial loss: 0.517797\n",
            "epoch 34; iter: 200; batch classifier loss: 0.382962; batch adversarial loss: 0.554865\n",
            "epoch 35; iter: 0; batch classifier loss: 0.443923; batch adversarial loss: 0.561458\n",
            "epoch 35; iter: 200; batch classifier loss: 0.506538; batch adversarial loss: 0.580031\n",
            "epoch 36; iter: 0; batch classifier loss: 0.392961; batch adversarial loss: 0.615688\n",
            "epoch 36; iter: 200; batch classifier loss: 0.464769; batch adversarial loss: 0.574648\n",
            "epoch 37; iter: 0; batch classifier loss: 0.436443; batch adversarial loss: 0.542852\n",
            "epoch 37; iter: 200; batch classifier loss: 0.410647; batch adversarial loss: 0.592076\n",
            "epoch 38; iter: 0; batch classifier loss: 0.406782; batch adversarial loss: 0.534645\n",
            "epoch 38; iter: 200; batch classifier loss: 0.345893; batch adversarial loss: 0.590682\n",
            "epoch 39; iter: 0; batch classifier loss: 0.432024; batch adversarial loss: 0.548811\n",
            "epoch 39; iter: 200; batch classifier loss: 0.448057; batch adversarial loss: 0.629319\n",
            "epoch 40; iter: 0; batch classifier loss: 0.362601; batch adversarial loss: 0.503172\n",
            "epoch 40; iter: 200; batch classifier loss: 0.453012; batch adversarial loss: 0.599573\n",
            "epoch 41; iter: 0; batch classifier loss: 0.415297; batch adversarial loss: 0.555120\n",
            "epoch 41; iter: 200; batch classifier loss: 0.446221; batch adversarial loss: 0.514406\n",
            "epoch 42; iter: 0; batch classifier loss: 0.420708; batch adversarial loss: 0.482925\n",
            "epoch 42; iter: 200; batch classifier loss: 0.458817; batch adversarial loss: 0.569989\n",
            "epoch 43; iter: 0; batch classifier loss: 0.386566; batch adversarial loss: 0.559358\n",
            "epoch 43; iter: 200; batch classifier loss: 0.485370; batch adversarial loss: 0.585664\n",
            "epoch 44; iter: 0; batch classifier loss: 0.498518; batch adversarial loss: 0.543016\n",
            "epoch 44; iter: 200; batch classifier loss: 0.412366; batch adversarial loss: 0.522501\n",
            "epoch 45; iter: 0; batch classifier loss: 0.435753; batch adversarial loss: 0.496034\n",
            "epoch 45; iter: 200; batch classifier loss: 0.459908; batch adversarial loss: 0.540499\n",
            "epoch 46; iter: 0; batch classifier loss: 0.468802; batch adversarial loss: 0.494962\n",
            "epoch 46; iter: 200; batch classifier loss: 0.346994; batch adversarial loss: 0.507371\n",
            "epoch 47; iter: 0; batch classifier loss: 0.483295; batch adversarial loss: 0.504945\n",
            "epoch 47; iter: 200; batch classifier loss: 0.395279; batch adversarial loss: 0.531978\n",
            "epoch 48; iter: 0; batch classifier loss: 0.418140; batch adversarial loss: 0.468056\n",
            "epoch 48; iter: 200; batch classifier loss: 0.353150; batch adversarial loss: 0.480720\n",
            "epoch 49; iter: 0; batch classifier loss: 0.403321; batch adversarial loss: 0.526258\n",
            "epoch 49; iter: 200; batch classifier loss: 0.496586; batch adversarial loss: 0.493899\n",
            "epoch 0; iter: 0; batch classifier loss: 0.724699; batch adversarial loss: 0.671103\n",
            "epoch 0; iter: 200; batch classifier loss: 0.586971; batch adversarial loss: 0.667757\n",
            "epoch 1; iter: 0; batch classifier loss: 0.559778; batch adversarial loss: 0.645100\n",
            "epoch 1; iter: 200; batch classifier loss: 0.479662; batch adversarial loss: 0.652995\n",
            "epoch 2; iter: 0; batch classifier loss: 0.456037; batch adversarial loss: 0.632437\n",
            "epoch 2; iter: 200; batch classifier loss: 0.486372; batch adversarial loss: 0.616249\n",
            "epoch 3; iter: 0; batch classifier loss: 0.540496; batch adversarial loss: 0.634953\n",
            "epoch 3; iter: 200; batch classifier loss: 0.463486; batch adversarial loss: 0.635395\n",
            "epoch 4; iter: 0; batch classifier loss: 0.412048; batch adversarial loss: 0.622037\n",
            "epoch 4; iter: 200; batch classifier loss: 0.402011; batch adversarial loss: 0.682128\n",
            "epoch 5; iter: 0; batch classifier loss: 0.452243; batch adversarial loss: 0.579556\n",
            "epoch 5; iter: 200; batch classifier loss: 0.465526; batch adversarial loss: 0.608362\n",
            "epoch 6; iter: 0; batch classifier loss: 0.399858; batch adversarial loss: 0.611235\n",
            "epoch 6; iter: 200; batch classifier loss: 0.386194; batch adversarial loss: 0.610365\n",
            "epoch 7; iter: 0; batch classifier loss: 0.405780; batch adversarial loss: 0.627038\n",
            "epoch 7; iter: 200; batch classifier loss: 0.433318; batch adversarial loss: 0.631952\n",
            "epoch 8; iter: 0; batch classifier loss: 0.456492; batch adversarial loss: 0.643233\n",
            "epoch 8; iter: 200; batch classifier loss: 0.422024; batch adversarial loss: 0.599124\n",
            "epoch 9; iter: 0; batch classifier loss: 0.572413; batch adversarial loss: 0.642883\n",
            "epoch 9; iter: 200; batch classifier loss: 0.524905; batch adversarial loss: 0.599521\n",
            "epoch 10; iter: 0; batch classifier loss: 0.573096; batch adversarial loss: 0.612293\n",
            "epoch 10; iter: 200; batch classifier loss: 0.493766; batch adversarial loss: 0.632585\n",
            "epoch 11; iter: 0; batch classifier loss: 0.494123; batch adversarial loss: 0.649571\n",
            "epoch 11; iter: 200; batch classifier loss: 0.474077; batch adversarial loss: 0.655868\n",
            "epoch 12; iter: 0; batch classifier loss: 0.461863; batch adversarial loss: 0.589374\n",
            "epoch 12; iter: 200; batch classifier loss: 0.475234; batch adversarial loss: 0.651939\n",
            "epoch 13; iter: 0; batch classifier loss: 0.548647; batch adversarial loss: 0.572966\n",
            "epoch 13; iter: 200; batch classifier loss: 0.415779; batch adversarial loss: 0.669669\n",
            "epoch 14; iter: 0; batch classifier loss: 0.557211; batch adversarial loss: 0.587294\n",
            "epoch 14; iter: 200; batch classifier loss: 0.532849; batch adversarial loss: 0.574584\n",
            "epoch 15; iter: 0; batch classifier loss: 0.431266; batch adversarial loss: 0.575859\n",
            "epoch 15; iter: 200; batch classifier loss: 0.427653; batch adversarial loss: 0.594349\n",
            "epoch 16; iter: 0; batch classifier loss: 0.488316; batch adversarial loss: 0.623493\n",
            "epoch 16; iter: 200; batch classifier loss: 0.450547; batch adversarial loss: 0.593321\n",
            "epoch 17; iter: 0; batch classifier loss: 0.377825; batch adversarial loss: 0.586316\n",
            "epoch 17; iter: 200; batch classifier loss: 0.587027; batch adversarial loss: 0.684647\n",
            "epoch 18; iter: 0; batch classifier loss: 0.388310; batch adversarial loss: 0.565334\n",
            "epoch 18; iter: 200; batch classifier loss: 0.382587; batch adversarial loss: 0.697930\n",
            "epoch 19; iter: 0; batch classifier loss: 0.550608; batch adversarial loss: 0.619108\n",
            "epoch 19; iter: 200; batch classifier loss: 0.505356; batch adversarial loss: 0.654725\n",
            "epoch 20; iter: 0; batch classifier loss: 0.368775; batch adversarial loss: 0.593066\n",
            "epoch 20; iter: 200; batch classifier loss: 0.367816; batch adversarial loss: 0.609287\n",
            "epoch 21; iter: 0; batch classifier loss: 0.484662; batch adversarial loss: 0.603928\n",
            "epoch 21; iter: 200; batch classifier loss: 0.436945; batch adversarial loss: 0.661449\n",
            "epoch 22; iter: 0; batch classifier loss: 0.486397; batch adversarial loss: 0.633756\n",
            "epoch 22; iter: 200; batch classifier loss: 0.610914; batch adversarial loss: 0.614620\n",
            "epoch 23; iter: 0; batch classifier loss: 0.412399; batch adversarial loss: 0.650246\n",
            "epoch 23; iter: 200; batch classifier loss: 0.421540; batch adversarial loss: 0.596308\n",
            "epoch 24; iter: 0; batch classifier loss: 0.432496; batch adversarial loss: 0.623961\n",
            "epoch 24; iter: 200; batch classifier loss: 0.471573; batch adversarial loss: 0.650676\n",
            "epoch 25; iter: 0; batch classifier loss: 0.445177; batch adversarial loss: 0.608312\n",
            "epoch 25; iter: 200; batch classifier loss: 0.457158; batch adversarial loss: 0.581525\n",
            "epoch 26; iter: 0; batch classifier loss: 0.410962; batch adversarial loss: 0.619557\n",
            "epoch 26; iter: 200; batch classifier loss: 0.459829; batch adversarial loss: 0.585078\n",
            "epoch 27; iter: 0; batch classifier loss: 0.425042; batch adversarial loss: 0.648429\n",
            "epoch 27; iter: 200; batch classifier loss: 0.432631; batch adversarial loss: 0.628624\n",
            "epoch 28; iter: 0; batch classifier loss: 0.496993; batch adversarial loss: 0.635289\n",
            "epoch 28; iter: 200; batch classifier loss: 0.477254; batch adversarial loss: 0.556030\n",
            "epoch 29; iter: 0; batch classifier loss: 0.459305; batch adversarial loss: 0.669981\n",
            "epoch 29; iter: 200; batch classifier loss: 0.440392; batch adversarial loss: 0.566831\n",
            "epoch 30; iter: 0; batch classifier loss: 0.542769; batch adversarial loss: 0.619573\n",
            "epoch 30; iter: 200; batch classifier loss: 0.477601; batch adversarial loss: 0.623009\n",
            "epoch 31; iter: 0; batch classifier loss: 0.475654; batch adversarial loss: 0.608189\n",
            "epoch 31; iter: 200; batch classifier loss: 0.492440; batch adversarial loss: 0.617561\n",
            "epoch 32; iter: 0; batch classifier loss: 0.572802; batch adversarial loss: 0.597400\n",
            "epoch 32; iter: 200; batch classifier loss: 0.450702; batch adversarial loss: 0.636229\n",
            "epoch 33; iter: 0; batch classifier loss: 0.468318; batch adversarial loss: 0.577002\n",
            "epoch 33; iter: 200; batch classifier loss: 0.471400; batch adversarial loss: 0.641731\n",
            "epoch 34; iter: 0; batch classifier loss: 0.371572; batch adversarial loss: 0.616076\n",
            "epoch 34; iter: 200; batch classifier loss: 0.540051; batch adversarial loss: 0.599641\n",
            "epoch 35; iter: 0; batch classifier loss: 0.431307; batch adversarial loss: 0.644231\n",
            "epoch 35; iter: 200; batch classifier loss: 0.428679; batch adversarial loss: 0.627360\n",
            "epoch 36; iter: 0; batch classifier loss: 0.482439; batch adversarial loss: 0.585961\n",
            "epoch 36; iter: 200; batch classifier loss: 0.475073; batch adversarial loss: 0.659145\n",
            "epoch 37; iter: 0; batch classifier loss: 0.523287; batch adversarial loss: 0.585196\n",
            "epoch 37; iter: 200; batch classifier loss: 0.536984; batch adversarial loss: 0.585045\n",
            "epoch 38; iter: 0; batch classifier loss: 0.478877; batch adversarial loss: 0.648303\n",
            "epoch 38; iter: 200; batch classifier loss: 0.510234; batch adversarial loss: 0.631693\n",
            "epoch 39; iter: 0; batch classifier loss: 0.475650; batch adversarial loss: 0.645421\n",
            "epoch 39; iter: 200; batch classifier loss: 0.476017; batch adversarial loss: 0.639827\n",
            "epoch 40; iter: 0; batch classifier loss: 0.574740; batch adversarial loss: 0.667707\n",
            "epoch 40; iter: 200; batch classifier loss: 0.497317; batch adversarial loss: 0.661773\n",
            "epoch 41; iter: 0; batch classifier loss: 0.555368; batch adversarial loss: 0.572702\n",
            "epoch 41; iter: 200; batch classifier loss: 0.511793; batch adversarial loss: 0.596964\n",
            "epoch 42; iter: 0; batch classifier loss: 0.435315; batch adversarial loss: 0.593657\n",
            "epoch 42; iter: 200; batch classifier loss: 0.492326; batch adversarial loss: 0.666504\n",
            "epoch 43; iter: 0; batch classifier loss: 0.416454; batch adversarial loss: 0.615216\n",
            "epoch 43; iter: 200; batch classifier loss: 0.504861; batch adversarial loss: 0.622233\n",
            "epoch 44; iter: 0; batch classifier loss: 0.563700; batch adversarial loss: 0.569851\n",
            "epoch 44; iter: 200; batch classifier loss: 0.461593; batch adversarial loss: 0.662802\n",
            "epoch 45; iter: 0; batch classifier loss: 0.372829; batch adversarial loss: 0.645140\n",
            "epoch 45; iter: 200; batch classifier loss: 0.365793; batch adversarial loss: 0.604831\n",
            "epoch 46; iter: 0; batch classifier loss: 0.533352; batch adversarial loss: 0.615216\n",
            "epoch 46; iter: 200; batch classifier loss: 0.535740; batch adversarial loss: 0.629130\n",
            "epoch 47; iter: 0; batch classifier loss: 0.596798; batch adversarial loss: 0.619581\n",
            "epoch 47; iter: 200; batch classifier loss: 0.419454; batch adversarial loss: 0.665051\n",
            "epoch 48; iter: 0; batch classifier loss: 0.553846; batch adversarial loss: 0.656187\n",
            "epoch 48; iter: 200; batch classifier loss: 0.477628; batch adversarial loss: 0.654177\n",
            "epoch 49; iter: 0; batch classifier loss: 0.487289; batch adversarial loss: 0.658323\n",
            "epoch 49; iter: 200; batch classifier loss: 0.469846; batch adversarial loss: 0.653928\n",
            "epoch 0; iter: 0; batch classifier loss: 0.708057; batch adversarial loss: 0.638507\n",
            "epoch 0; iter: 200; batch classifier loss: 1.354043; batch adversarial loss: 0.667909\n",
            "epoch 1; iter: 0; batch classifier loss: 0.926475; batch adversarial loss: 0.649175\n",
            "epoch 1; iter: 200; batch classifier loss: 1.083727; batch adversarial loss: 0.633479\n",
            "epoch 2; iter: 0; batch classifier loss: 0.605533; batch adversarial loss: 0.675319\n",
            "epoch 2; iter: 200; batch classifier loss: 0.983733; batch adversarial loss: 0.626811\n",
            "epoch 3; iter: 0; batch classifier loss: 0.686318; batch adversarial loss: 0.634822\n",
            "epoch 3; iter: 200; batch classifier loss: 0.946390; batch adversarial loss: 0.613922\n",
            "epoch 4; iter: 0; batch classifier loss: 0.773797; batch adversarial loss: 0.618240\n",
            "epoch 4; iter: 200; batch classifier loss: 0.525111; batch adversarial loss: 0.606689\n",
            "epoch 5; iter: 0; batch classifier loss: 0.590523; batch adversarial loss: 0.592488\n",
            "epoch 5; iter: 200; batch classifier loss: 0.703741; batch adversarial loss: 0.621484\n",
            "epoch 6; iter: 0; batch classifier loss: 0.394296; batch adversarial loss: 0.622367\n",
            "epoch 6; iter: 200; batch classifier loss: 0.806580; batch adversarial loss: 0.638125\n",
            "epoch 7; iter: 0; batch classifier loss: 0.862144; batch adversarial loss: 0.594344\n",
            "epoch 7; iter: 200; batch classifier loss: 0.581512; batch adversarial loss: 0.677028\n",
            "epoch 8; iter: 0; batch classifier loss: 0.540372; batch adversarial loss: 0.654155\n",
            "epoch 8; iter: 200; batch classifier loss: 0.573764; batch adversarial loss: 0.676804\n",
            "epoch 9; iter: 0; batch classifier loss: 0.639581; batch adversarial loss: 0.609472\n",
            "epoch 9; iter: 200; batch classifier loss: 0.497132; batch adversarial loss: 0.656868\n",
            "epoch 10; iter: 0; batch classifier loss: 0.625342; batch adversarial loss: 0.610850\n",
            "epoch 10; iter: 200; batch classifier loss: 0.748026; batch adversarial loss: 0.583563\n",
            "epoch 11; iter: 0; batch classifier loss: 0.672630; batch adversarial loss: 0.674164\n",
            "epoch 11; iter: 200; batch classifier loss: 0.732231; batch adversarial loss: 0.626792\n",
            "epoch 12; iter: 0; batch classifier loss: 0.489094; batch adversarial loss: 0.677375\n",
            "epoch 12; iter: 200; batch classifier loss: 0.519063; batch adversarial loss: 0.630372\n",
            "epoch 13; iter: 0; batch classifier loss: 0.715663; batch adversarial loss: 0.671263\n",
            "epoch 13; iter: 200; batch classifier loss: 0.711884; batch adversarial loss: 0.651648\n",
            "epoch 14; iter: 0; batch classifier loss: 0.660395; batch adversarial loss: 0.620779\n",
            "epoch 14; iter: 200; batch classifier loss: 0.824553; batch adversarial loss: 0.662455\n",
            "epoch 15; iter: 0; batch classifier loss: 0.552188; batch adversarial loss: 0.681963\n",
            "epoch 15; iter: 200; batch classifier loss: 0.790251; batch adversarial loss: 0.653700\n",
            "epoch 16; iter: 0; batch classifier loss: 0.596933; batch adversarial loss: 0.628272\n",
            "epoch 16; iter: 200; batch classifier loss: 0.491031; batch adversarial loss: 0.648593\n",
            "epoch 17; iter: 0; batch classifier loss: 0.537783; batch adversarial loss: 0.639552\n",
            "epoch 17; iter: 200; batch classifier loss: 0.763395; batch adversarial loss: 0.621511\n",
            "epoch 18; iter: 0; batch classifier loss: 0.593999; batch adversarial loss: 0.599704\n",
            "epoch 18; iter: 200; batch classifier loss: 0.857770; batch adversarial loss: 0.631119\n",
            "epoch 19; iter: 0; batch classifier loss: 0.621775; batch adversarial loss: 0.644386\n",
            "epoch 19; iter: 200; batch classifier loss: 0.798858; batch adversarial loss: 0.631598\n",
            "epoch 20; iter: 0; batch classifier loss: 0.511839; batch adversarial loss: 0.664484\n",
            "epoch 20; iter: 200; batch classifier loss: 0.675462; batch adversarial loss: 0.639258\n",
            "epoch 21; iter: 0; batch classifier loss: 0.599140; batch adversarial loss: 0.615506\n",
            "epoch 21; iter: 200; batch classifier loss: 0.672877; batch adversarial loss: 0.662460\n",
            "epoch 22; iter: 0; batch classifier loss: 1.041024; batch adversarial loss: 0.611282\n",
            "epoch 22; iter: 200; batch classifier loss: 0.493134; batch adversarial loss: 0.658446\n",
            "epoch 23; iter: 0; batch classifier loss: 0.817778; batch adversarial loss: 0.649780\n",
            "epoch 23; iter: 200; batch classifier loss: 0.507048; batch adversarial loss: 0.595659\n",
            "epoch 24; iter: 0; batch classifier loss: 0.486244; batch adversarial loss: 0.625694\n",
            "epoch 24; iter: 200; batch classifier loss: 0.574173; batch adversarial loss: 0.648013\n",
            "epoch 25; iter: 0; batch classifier loss: 0.576659; batch adversarial loss: 0.603301\n",
            "epoch 25; iter: 200; batch classifier loss: 0.741588; batch adversarial loss: 0.685583\n",
            "epoch 26; iter: 0; batch classifier loss: 0.565725; batch adversarial loss: 0.649372\n",
            "epoch 26; iter: 200; batch classifier loss: 0.616098; batch adversarial loss: 0.600978\n",
            "epoch 27; iter: 0; batch classifier loss: 0.747554; batch adversarial loss: 0.706602\n",
            "epoch 27; iter: 200; batch classifier loss: 0.499531; batch adversarial loss: 0.705163\n",
            "epoch 28; iter: 0; batch classifier loss: 0.545940; batch adversarial loss: 0.609470\n",
            "epoch 28; iter: 200; batch classifier loss: 0.595501; batch adversarial loss: 0.640691\n",
            "epoch 29; iter: 0; batch classifier loss: 0.630667; batch adversarial loss: 0.608060\n",
            "epoch 29; iter: 200; batch classifier loss: 0.640469; batch adversarial loss: 0.608557\n",
            "epoch 30; iter: 0; batch classifier loss: 0.595976; batch adversarial loss: 0.598506\n",
            "epoch 30; iter: 200; batch classifier loss: 0.670703; batch adversarial loss: 0.660168\n",
            "epoch 31; iter: 0; batch classifier loss: 0.813915; batch adversarial loss: 0.596434\n",
            "epoch 31; iter: 200; batch classifier loss: 0.676317; batch adversarial loss: 0.659015\n",
            "epoch 32; iter: 0; batch classifier loss: 0.532283; batch adversarial loss: 0.671866\n",
            "epoch 32; iter: 200; batch classifier loss: 0.474525; batch adversarial loss: 0.673771\n",
            "epoch 33; iter: 0; batch classifier loss: 0.659047; batch adversarial loss: 0.613516\n",
            "epoch 33; iter: 200; batch classifier loss: 0.704709; batch adversarial loss: 0.616331\n",
            "epoch 34; iter: 0; batch classifier loss: 0.666746; batch adversarial loss: 0.674111\n",
            "epoch 34; iter: 200; batch classifier loss: 0.812580; batch adversarial loss: 0.600379\n",
            "epoch 35; iter: 0; batch classifier loss: 0.823033; batch adversarial loss: 0.640160\n",
            "epoch 35; iter: 200; batch classifier loss: 0.698165; batch adversarial loss: 0.614016\n",
            "epoch 36; iter: 0; batch classifier loss: 0.700027; batch adversarial loss: 0.596539\n",
            "epoch 36; iter: 200; batch classifier loss: 0.602285; batch adversarial loss: 0.600680\n",
            "epoch 37; iter: 0; batch classifier loss: 0.716792; batch adversarial loss: 0.626028\n",
            "epoch 37; iter: 200; batch classifier loss: 0.973573; batch adversarial loss: 0.621802\n",
            "epoch 38; iter: 0; batch classifier loss: 0.612963; batch adversarial loss: 0.668704\n",
            "epoch 38; iter: 200; batch classifier loss: 0.623415; batch adversarial loss: 0.669153\n",
            "epoch 39; iter: 0; batch classifier loss: 0.693412; batch adversarial loss: 0.589556\n",
            "epoch 39; iter: 200; batch classifier loss: 0.655660; batch adversarial loss: 0.639566\n",
            "epoch 40; iter: 0; batch classifier loss: 0.502914; batch adversarial loss: 0.617800\n",
            "epoch 40; iter: 200; batch classifier loss: 0.444425; batch adversarial loss: 0.659319\n",
            "epoch 41; iter: 0; batch classifier loss: 0.625512; batch adversarial loss: 0.659714\n",
            "epoch 41; iter: 200; batch classifier loss: 0.535242; batch adversarial loss: 0.649910\n",
            "epoch 42; iter: 0; batch classifier loss: 0.814591; batch adversarial loss: 0.623230\n",
            "epoch 42; iter: 200; batch classifier loss: 0.694561; batch adversarial loss: 0.611968\n",
            "epoch 43; iter: 0; batch classifier loss: 0.680860; batch adversarial loss: 0.618471\n",
            "epoch 43; iter: 200; batch classifier loss: 0.585542; batch adversarial loss: 0.621592\n",
            "epoch 44; iter: 0; batch classifier loss: 0.581453; batch adversarial loss: 0.652111\n",
            "epoch 44; iter: 200; batch classifier loss: 0.895066; batch adversarial loss: 0.655758\n",
            "epoch 45; iter: 0; batch classifier loss: 0.843787; batch adversarial loss: 0.620216\n",
            "epoch 45; iter: 200; batch classifier loss: 0.633870; batch adversarial loss: 0.605216\n",
            "epoch 46; iter: 0; batch classifier loss: 0.493751; batch adversarial loss: 0.639194\n",
            "epoch 46; iter: 200; batch classifier loss: 0.626329; batch adversarial loss: 0.648521\n",
            "epoch 47; iter: 0; batch classifier loss: 0.674586; batch adversarial loss: 0.632886\n",
            "epoch 47; iter: 200; batch classifier loss: 0.536596; batch adversarial loss: 0.637222\n",
            "epoch 48; iter: 0; batch classifier loss: 0.484289; batch adversarial loss: 0.632683\n",
            "epoch 48; iter: 200; batch classifier loss: 0.626515; batch adversarial loss: 0.633306\n",
            "epoch 49; iter: 0; batch classifier loss: 0.757612; batch adversarial loss: 0.606678\n",
            "epoch 49; iter: 200; batch classifier loss: 0.480172; batch adversarial loss: 0.650756\n",
            "epoch 0; iter: 0; batch classifier loss: 0.697165; batch adversarial loss: 0.606379\n",
            "epoch 0; iter: 200; batch classifier loss: 1.155691; batch adversarial loss: 0.716686\n",
            "epoch 1; iter: 0; batch classifier loss: 1.288884; batch adversarial loss: 0.692570\n",
            "epoch 1; iter: 200; batch classifier loss: 1.623482; batch adversarial loss: 0.611324\n",
            "epoch 2; iter: 0; batch classifier loss: 1.476511; batch adversarial loss: 0.636172\n",
            "epoch 2; iter: 200; batch classifier loss: 1.671468; batch adversarial loss: 0.600379\n",
            "epoch 3; iter: 0; batch classifier loss: 1.475413; batch adversarial loss: 0.611816\n",
            "epoch 3; iter: 200; batch classifier loss: 2.496040; batch adversarial loss: 0.653390\n",
            "epoch 4; iter: 0; batch classifier loss: 2.474677; batch adversarial loss: 0.628156\n",
            "epoch 4; iter: 200; batch classifier loss: 1.808070; batch adversarial loss: 0.643604\n",
            "epoch 5; iter: 0; batch classifier loss: 1.294643; batch adversarial loss: 0.643607\n",
            "epoch 5; iter: 200; batch classifier loss: 1.797221; batch adversarial loss: 0.627451\n",
            "epoch 6; iter: 0; batch classifier loss: 1.827456; batch adversarial loss: 0.573218\n",
            "epoch 6; iter: 200; batch classifier loss: 1.983964; batch adversarial loss: 0.693202\n",
            "epoch 7; iter: 0; batch classifier loss: 2.540112; batch adversarial loss: 0.616517\n",
            "epoch 7; iter: 200; batch classifier loss: 2.659209; batch adversarial loss: 0.616287\n",
            "epoch 8; iter: 0; batch classifier loss: 1.996783; batch adversarial loss: 0.638336\n",
            "epoch 8; iter: 200; batch classifier loss: 2.276193; batch adversarial loss: 0.655179\n",
            "epoch 9; iter: 0; batch classifier loss: 2.215426; batch adversarial loss: 0.610879\n",
            "epoch 9; iter: 200; batch classifier loss: 1.719552; batch adversarial loss: 0.654734\n",
            "epoch 10; iter: 0; batch classifier loss: 2.452964; batch adversarial loss: 0.654934\n",
            "epoch 10; iter: 200; batch classifier loss: 1.659134; batch adversarial loss: 0.660464\n",
            "epoch 11; iter: 0; batch classifier loss: 2.118679; batch adversarial loss: 0.643901\n",
            "epoch 11; iter: 200; batch classifier loss: 1.945819; batch adversarial loss: 0.605352\n",
            "epoch 12; iter: 0; batch classifier loss: 1.511713; batch adversarial loss: 0.632830\n",
            "epoch 12; iter: 200; batch classifier loss: 1.630632; batch adversarial loss: 0.605789\n",
            "epoch 13; iter: 0; batch classifier loss: 1.378410; batch adversarial loss: 0.605109\n",
            "epoch 13; iter: 200; batch classifier loss: 1.449776; batch adversarial loss: 0.627318\n",
            "epoch 14; iter: 0; batch classifier loss: 1.673582; batch adversarial loss: 0.627324\n",
            "epoch 14; iter: 200; batch classifier loss: 1.580443; batch adversarial loss: 0.632852\n",
            "epoch 15; iter: 0; batch classifier loss: 1.641342; batch adversarial loss: 0.621803\n",
            "epoch 15; iter: 200; batch classifier loss: 1.621238; batch adversarial loss: 0.627268\n",
            "epoch 16; iter: 0; batch classifier loss: 1.450199; batch adversarial loss: 0.665836\n",
            "epoch 16; iter: 200; batch classifier loss: 1.323386; batch adversarial loss: 0.572779\n",
            "epoch 17; iter: 0; batch classifier loss: 1.372377; batch adversarial loss: 0.682649\n",
            "epoch 17; iter: 200; batch classifier loss: 1.268643; batch adversarial loss: 0.566710\n",
            "epoch 18; iter: 0; batch classifier loss: 1.496949; batch adversarial loss: 0.604985\n",
            "epoch 18; iter: 200; batch classifier loss: 1.258168; batch adversarial loss: 0.638342\n",
            "epoch 19; iter: 0; batch classifier loss: 1.766365; batch adversarial loss: 0.583391\n",
            "epoch 19; iter: 200; batch classifier loss: 1.573105; batch adversarial loss: 0.588188\n",
            "epoch 20; iter: 0; batch classifier loss: 1.364383; batch adversarial loss: 0.621872\n",
            "epoch 20; iter: 200; batch classifier loss: 1.160180; batch adversarial loss: 0.660386\n",
            "epoch 21; iter: 0; batch classifier loss: 1.300045; batch adversarial loss: 0.638355\n",
            "epoch 21; iter: 200; batch classifier loss: 1.226449; batch adversarial loss: 0.610905\n",
            "epoch 22; iter: 0; batch classifier loss: 1.343127; batch adversarial loss: 0.621737\n",
            "epoch 22; iter: 200; batch classifier loss: 1.260634; batch adversarial loss: 0.643914\n",
            "epoch 23; iter: 0; batch classifier loss: 1.220706; batch adversarial loss: 0.660445\n",
            "epoch 23; iter: 200; batch classifier loss: 1.225805; batch adversarial loss: 0.632851\n",
            "epoch 24; iter: 0; batch classifier loss: 1.005892; batch adversarial loss: 0.649426\n",
            "epoch 24; iter: 200; batch classifier loss: 0.868598; batch adversarial loss: 0.666936\n",
            "epoch 25; iter: 0; batch classifier loss: 1.576165; batch adversarial loss: 0.616270\n",
            "epoch 25; iter: 200; batch classifier loss: 2.191162; batch adversarial loss: 0.616306\n",
            "epoch 26; iter: 0; batch classifier loss: 2.262539; batch adversarial loss: 0.638362\n",
            "epoch 26; iter: 200; batch classifier loss: 2.331348; batch adversarial loss: 0.671625\n",
            "epoch 27; iter: 0; batch classifier loss: 2.113616; batch adversarial loss: 0.605255\n",
            "epoch 27; iter: 200; batch classifier loss: 2.022699; batch adversarial loss: 0.643937\n",
            "epoch 28; iter: 0; batch classifier loss: 1.881513; batch adversarial loss: 0.676784\n",
            "epoch 28; iter: 200; batch classifier loss: 1.753341; batch adversarial loss: 0.605177\n",
            "epoch 29; iter: 0; batch classifier loss: 2.043788; batch adversarial loss: 0.643828\n",
            "epoch 29; iter: 200; batch classifier loss: 1.935372; batch adversarial loss: 0.671571\n",
            "epoch 30; iter: 0; batch classifier loss: 2.120769; batch adversarial loss: 0.599897\n",
            "epoch 30; iter: 200; batch classifier loss: 1.269746; batch adversarial loss: 0.654737\n",
            "epoch 31; iter: 0; batch classifier loss: 1.901352; batch adversarial loss: 0.665759\n",
            "epoch 31; iter: 200; batch classifier loss: 1.177351; batch adversarial loss: 0.633201\n",
            "epoch 32; iter: 0; batch classifier loss: 2.057377; batch adversarial loss: 0.605595\n",
            "epoch 32; iter: 200; batch classifier loss: 2.217118; batch adversarial loss: 0.638364\n",
            "epoch 33; iter: 0; batch classifier loss: 2.131149; batch adversarial loss: 0.610798\n",
            "epoch 33; iter: 200; batch classifier loss: 2.504467; batch adversarial loss: 0.616280\n",
            "epoch 34; iter: 0; batch classifier loss: 2.409272; batch adversarial loss: 0.610786\n",
            "epoch 34; iter: 200; batch classifier loss: 2.158281; batch adversarial loss: 0.632807\n",
            "epoch 35; iter: 0; batch classifier loss: 1.877981; batch adversarial loss: 0.599778\n",
            "epoch 35; iter: 200; batch classifier loss: 2.003622; batch adversarial loss: 0.632833\n",
            "epoch 36; iter: 0; batch classifier loss: 1.876247; batch adversarial loss: 0.610924\n",
            "epoch 36; iter: 200; batch classifier loss: 1.727239; batch adversarial loss: 0.588638\n",
            "epoch 37; iter: 0; batch classifier loss: 1.433928; batch adversarial loss: 0.616418\n",
            "epoch 37; iter: 200; batch classifier loss: 3.204900; batch adversarial loss: 0.699188\n",
            "epoch 38; iter: 0; batch classifier loss: 2.367990; batch adversarial loss: 0.654966\n",
            "epoch 38; iter: 200; batch classifier loss: 2.136005; batch adversarial loss: 0.555410\n",
            "epoch 39; iter: 0; batch classifier loss: 2.424359; batch adversarial loss: 0.649425\n",
            "epoch 39; iter: 200; batch classifier loss: 2.814224; batch adversarial loss: 0.632858\n",
            "epoch 40; iter: 0; batch classifier loss: 1.749866; batch adversarial loss: 0.671480\n",
            "epoch 40; iter: 200; batch classifier loss: 2.038649; batch adversarial loss: 0.654932\n",
            "epoch 41; iter: 0; batch classifier loss: 2.575283; batch adversarial loss: 0.649375\n",
            "epoch 41; iter: 200; batch classifier loss: 1.969593; batch adversarial loss: 0.632865\n",
            "epoch 42; iter: 0; batch classifier loss: 1.847351; batch adversarial loss: 0.649376\n",
            "epoch 42; iter: 200; batch classifier loss: 1.928525; batch adversarial loss: 0.660575\n",
            "epoch 43; iter: 0; batch classifier loss: 2.258305; batch adversarial loss: 0.643900\n",
            "epoch 43; iter: 200; batch classifier loss: 0.791195; batch adversarial loss: 0.602204\n",
            "epoch 44; iter: 0; batch classifier loss: 1.807987; batch adversarial loss: 0.582084\n",
            "epoch 44; iter: 200; batch classifier loss: 2.681145; batch adversarial loss: 0.666054\n",
            "epoch 45; iter: 0; batch classifier loss: 2.012915; batch adversarial loss: 0.627279\n",
            "epoch 45; iter: 200; batch classifier loss: 1.904479; batch adversarial loss: 0.649345\n",
            "epoch 46; iter: 0; batch classifier loss: 1.748560; batch adversarial loss: 0.627352\n",
            "epoch 46; iter: 200; batch classifier loss: 2.201653; batch adversarial loss: 0.621953\n",
            "epoch 47; iter: 0; batch classifier loss: 1.413313; batch adversarial loss: 0.643301\n",
            "epoch 47; iter: 200; batch classifier loss: 2.362056; batch adversarial loss: 0.616462\n",
            "epoch 48; iter: 0; batch classifier loss: 2.218951; batch adversarial loss: 0.732455\n",
            "epoch 48; iter: 200; batch classifier loss: 2.324984; batch adversarial loss: 0.594246\n",
            "epoch 49; iter: 0; batch classifier loss: 1.643734; batch adversarial loss: 0.649311\n",
            "epoch 49; iter: 200; batch classifier loss: 2.748462; batch adversarial loss: 0.631832\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Part 4: Plots and figures"
      ],
      "metadata": {
        "id": "02_6rQZpD_6P"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def target_distribution(y_var, data):\n",
        "    #reference:https://www.kaggle.com/code/nathanlauga/ethics-and-ai-how-to-prevent-bias-on-ml/notebook\n",
        "    val = data[y_var]\n",
        "\n",
        "    plt.style.use('seaborn-dark')\n",
        "    plt.rcParams.update({'font.size': 13})\n",
        "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 7))\n",
        "\n",
        "    cnt = val.value_counts().sort_values(ascending=True)\n",
        "    labels = cnt.index.values\n",
        "\n",
        "    sizes = cnt.values\n",
        "    colors = sns.color_palette(\"PuBu\", len(labels))\n",
        "\n",
        "    #------------COUNT-----------------------\n",
        "    ax1.barh(cnt.index.values, cnt.values, color=colors)\n",
        "    ax1.set_title('Count plot of '+y_var)\n",
        "\n",
        "    #------------PERCENTAGE-------------------\n",
        "    ax2.pie(sizes, labels=labels, colors=colors,autopct='%1.0f%%', shadow=True, startangle=130)\n",
        "    ax2.axis('equal')\n",
        "    ax2.set_title('Distribution of '+y_var)\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "zTRjGbrZXzDU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_histo(data, col, Y_columns):\n",
        "    #reference:https://www.kaggle.com/code/nathanlauga/ethics-and-ai-how-to-prevent-bias-on-ml/notebook\n",
        "    df = data.copy()\n",
        "    fig, axs = plt.subplots(1,2,figsize=(20,6))\n",
        "    \n",
        "    for i in range(0,2):\n",
        "        cnt = []; y_col = Y_columns[i]\n",
        "        Y_values = df[y_col].dropna().drop_duplicates().values\n",
        "        for val in Y_values:\n",
        "            cnt += [df[df[y_col] == val][col].values]\n",
        "        bins = df[col].nunique()\n",
        "\n",
        "        axs[i].hist(cnt, bins=bins, stacked=True)\n",
        "        axs[i].legend(Y_values,loc='upper right')\n",
        "        axs[i].set_title(\"Histogram of the \"+col+\" column by \"+y_col)\n",
        "        fig.autofmt_xdate()\n",
        "\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "p9cOx5BUYcbV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%matplotlib inline"
      ],
      "metadata": {
        "id": "H38AXhx4nuTo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X, y, sample_weight = fetch_adult()#for this dataset the protected variables are race and sex\n",
        "X_, y_ = fetch_german()#on the other hand this dataset has sex,age and foreign_worker as protected features"
      ],
      "metadata": {
        "id": "IUYtsqEdm9Ob"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "var = 'race'\n",
        "target_distribution(y_var=var, data=X)"
      ],
      "metadata": {
        "id": "Jn303iPGYMbg",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 445
        },
        "outputId": "c84d449e-29dc-4e15-8b6d-b7598d1568a4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1080x504 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "var = 'foreign_worker'\n",
        "target_distribution(y_var=var, data=X_)"
      ],
      "metadata": {
        "id": "az28zQL2AQcD",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 445
        },
        "outputId": "16f87337-de19-4f9e-d19a-620e7a051b13"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1080x504 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "Y_columns = ['race', 'sex', 'age']\n",
        "plot_histo(data=X, col='relationship',Y_columns=Y_columns)"
      ],
      "metadata": {
        "id": "GavTwIWqYdE0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 396
        },
        "outputId": "b31b7d0d-312b-43f0-d298-877b46706da8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1440x432 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "Y_columns = ['sex', 'foreign_worker', 'age']\n",
        "plot_histo(data=X_, col='job',Y_columns=Y_columns)"
      ],
      "metadata": {
        "id": "j2LjgOhSB56w",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 431
        },
        "outputId": "a1b40fc3-92e0-44f9-9efd-17a0eaad267f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1440x432 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def get_model_performance(X_test, y_true, y_pred, probs):\n",
        "    #reference:https://www.kaggle.com/code/nathanlauga/ethics-and-ai-how-to-prevent-bias-on-ml/notebook\n",
        "    accuracy = accuracy_score(y_true, y_pred)\n",
        "    matrix = confusion_matrix(y_true, y_pred)\n",
        "    f1 = f1_score(y_true, y_pred)\n",
        "    preds = probs[:, 1]\n",
        "    fpr, tpr, threshold = roc_curve(y_true, preds)\n",
        "    roc_auc = auc(fpr, tpr)\n",
        "\n",
        "    return accuracy, matrix, f1, fpr, tpr, roc_auc\n",
        "\n",
        "def plot_model_performance(model, X_test, y_true):\n",
        "    #reference:https://www.kaggle.com/code/nathanlauga/ethics-and-ai-how-to-prevent-bias-on-ml/notebook\n",
        "    y_pred = model.predict(X_test)\n",
        "    probs = model.predict_proba(X_test)\n",
        "    accuracy, matrix, f1, fpr, tpr, roc_auc = get_model_performance(X_test, y_true, y_pred, probs)\n",
        "\n",
        "    display(Markdown('#### Accuracy of the model :'))\n",
        "    print(accuracy)\n",
        "    display(Markdown('#### F1 score of the model :'))\n",
        "    print(f1)\n",
        "\n",
        "    fig = plt.figure(figsize=(15, 6))\n",
        "    ax = fig.add_subplot(1, 2, 1)\n",
        "    sns.heatmap(matrix, annot=True, cmap='Blues', fmt='g')\n",
        "    plt.ylabel('True label')\n",
        "    plt.xlabel('Predicted label')\n",
        "    plt.title('Confusion Matrix')\n",
        "\n",
        "    ax = fig.add_subplot(1, 2, 2)\n",
        "    lw = 2\n",
        "    plt.plot(fpr, tpr, color='darkorange', lw=lw, label='ROC curve (area = %0.2f)' % roc_auc)\n",
        "    plt.plot([0, 1], [0, 1], color='navy', lw=lw, linestyle='--')\n",
        "    plt.xlim([0.0, 1.0])\n",
        "    plt.ylim([0.0, 1.05])\n",
        "    plt.xlabel('False Positive Rate')\n",
        "    plt.ylabel('True Positive Rate')\n",
        "    plt.title('Receiver Operating Characteristic curve')\n",
        "    plt.legend(loc=\"lower right\")"
      ],
      "metadata": {
        "id": "m2vF55UhElCr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn import tree\n",
        "\n",
        "\n",
        "privileged_groups = [{'sex':1,'race': 1}]\n",
        "unprivileged_groups = [{'sex':0,'race': 0}]\n",
        "dataset_orig = load_preproc_data_adult(['sex','race'])\n",
        "\n",
        "#dataset_orig_fix = dataset_orig.convert_to_dataframe()[0].drop(['race', 'sex'], axis=1)\n",
        "#STEP 3: We split between training and test set.\n",
        "train_, test_ = dataset_orig.split([0.7], shuffle=True)\n",
        "print(\"training data size\", train_.features.shape)\n",
        "print(\"dataset feature names\", train_.feature_names)\n",
        "\n",
        "# Different criterions\n",
        "criterions = ['gini', 'entropy']\n",
        "# Maximum number of levels in tree\n",
        "max_depth = [int(x) for x in np.linspace(2, 100, num = 5)]\n",
        "max_depth.append(None)\n",
        "\n",
        "#Normalize the dataset, both train and test. This should always be done in any machine learning pipeline!\n",
        "scale = StandardScaler()\n",
        "\n",
        "X_train_ = scale.fit_transform(train_.features)#do not read the first two column (sex,race) while fitting to the data\n",
        "y_train_ = train_.labels.ravel()\n",
        "\n",
        "X_test_ = scale.transform(test_.features) #do not read the first two column (sex,race) while fitting to the data\n",
        "y_test_ = test_.labels.ravel()\n",
        "\n",
        "#STEP 5: Mitigate the bias, e.g. by transforming the original dataset via reweighing.\n",
        "di = DisparateImpactRemover(sensitive_attribute='sex',repair_level = 0.85)\n",
        "#We obtain a set of weights for the training set, to use in scikit-learn.\n",
        "trainining_ = di.fit_transform(train_)\n",
        "\n",
        "learner = DecisionTreeClassifier(criterion='entropy',max_depth=3)  \n",
        "learner.fit(X_train_,y_train_,sample_weight=trainining_.instance_weights)\n",
        "predictions = learner.predict(X_test_)\n",
        "model_acc = sum(predictions==y_test_)/len(y_test_)\n",
        "\n",
        "test_pred = test_.copy()\n",
        "test_pred.labels = predictions\n",
        "\n",
        "classified_metric = ClassificationMetric(test_, test_pred, unprivileged_groups=unprivileged_groups, privileged_groups=privileged_groups)\n",
        "\n",
        "#retrieving metrics from the classifier. The accuracies include percentage of true pred, statistical parity difference, disparate impact and equal opportunity.\n",
        "result = {'Accuracy_pre':model_acc,'Accuracy_post':classified_metric.accuracy(),'statistical_parity_difference': classified_metric.statistical_parity_difference(),\n",
        "      'disparate_impact': classified_metric.disparate_impact(),'equal_opportunity_difference': classified_metric.equal_opportunity_difference()}\n",
        "\n",
        "results_[str(fold_num)+'DecisionTree_'+str(criterion_1)+str(depth)]=result#saving the results of the decision tree classifier "
      ],
      "metadata": {
        "id": "7LzOShnVHEOr",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "48a7d1c2-ee79-412c-f04b-53b682dae13e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "training data size (34189, 18)\n",
            "dataset feature names ['race', 'sex', 'Age (decade)=10', 'Age (decade)=20', 'Age (decade)=30', 'Age (decade)=40', 'Age (decade)=50', 'Age (decade)=60', 'Age (decade)=>=70', 'Education Years=6', 'Education Years=7', 'Education Years=8', 'Education Years=9', 'Education Years=10', 'Education Years=11', 'Education Years=12', 'Education Years=<6', 'Education Years=>12']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "plot_model_performance(learner, test_.features, test_pred.labels)"
      ],
      "metadata": {
        "id": "lNuCU2nfI1v9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 520
        },
        "outputId": "85ab7d67-eca2-4a1d-8418-574332bf607e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "#### Accuracy of the model :"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.9484747150754111\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "#### F1 score of the model :"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.851055434997041\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1080x432 with 3 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import graphviz\n",
        "# DOT data\n",
        "dot_data = tree.export_graphviz(learner, out_file=None, \n",
        "                                feature_names=train_.feature_names,  \n",
        "                                class_names=['exceeds $50K/yr','below $50K/yr'],\n",
        "                                filled=True)\n",
        "\n",
        "# Draw graph\n",
        "graph = graphviz.Source(dot_data, format=\"png\") \n",
        "graph"
      ],
      "metadata": {
        "id": "yVzbDmETKucw",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 618
        },
        "outputId": "8b08b636-6d35-4623-958b-92a8575e75e8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<graphviz.files.Source at 0x7f491389a590>"
            ],
            "image/svg+xml": "<?xml version=\"1.0\" encoding=\"UTF-8\" standalone=\"no\"?>\n<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n<!-- Generated by graphviz version 2.40.1 (20161225.0304)\n -->\n<!-- Title: Tree Pages: 1 -->\n<svg width=\"1524pt\" height=\"433pt\"\n viewBox=\"0.00 0.00 1524.00 433.00\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n<g id=\"graph0\" class=\"graph\" transform=\"scale(1 1) rotate(0) translate(4 429)\">\n<title>Tree</title>\n<polygon fill=\"#ffffff\" stroke=\"transparent\" points=\"-4,4 -4,-429 1520,-429 1520,4 -4,4\"/>\n<!-- 0 -->\n<g id=\"node1\" class=\"node\">\n<title>0</title>\n<polygon fill=\"#eda977\" stroke=\"#000000\" points=\"868,-425 652,-425 652,-342 868,-342 868,-425\"/>\n<text text-anchor=\"middle\" x=\"760\" y=\"-409.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\" fill=\"#000000\">Education Years=&gt;12 &lt;= 0.582</text>\n<text text-anchor=\"middle\" x=\"760\" y=\"-394.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\" fill=\"#000000\">entropy = 0.795</text>\n<text text-anchor=\"middle\" x=\"760\" y=\"-379.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\" fill=\"#000000\">samples = 34189</text>\n<text text-anchor=\"middle\" x=\"760\" y=\"-364.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\" fill=\"#000000\">value = [25989, 8200]</text>\n<text text-anchor=\"middle\" x=\"760\" y=\"-349.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\" fill=\"#000000\">class = exceeds $50K/yr</text>\n</g>\n<!-- 1 -->\n<g id=\"node2\" class=\"node\">\n<title>1</title>\n<polygon fill=\"#ea995f\" stroke=\"#000000\" points=\"661,-306 471,-306 471,-223 661,-223 661,-306\"/>\n<text text-anchor=\"middle\" x=\"566\" y=\"-290.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\" fill=\"#000000\">Age (decade)=20 &lt;= 0.591</text>\n<text text-anchor=\"middle\" x=\"566\" y=\"-275.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\" fill=\"#000000\">entropy = 0.633</text>\n<text text-anchor=\"middle\" x=\"566\" y=\"-260.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\" fill=\"#000000\">samples = 25691</text>\n<text text-anchor=\"middle\" x=\"566\" y=\"-245.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\" fill=\"#000000\">value = [21596, 4095]</text>\n<text text-anchor=\"middle\" x=\"566\" y=\"-230.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\" fill=\"#000000\">class = exceeds $50K/yr</text>\n</g>\n<!-- 0&#45;&gt;1 -->\n<g id=\"edge1\" class=\"edge\">\n<title>0&#45;&gt;1</title>\n<path fill=\"none\" stroke=\"#000000\" d=\"M692.1482,-341.8796C676.1989,-332.0962 659.0904,-321.6019 642.7802,-311.5971\"/>\n<polygon fill=\"#000000\" stroke=\"#000000\" points=\"644.1677,-308.3422 633.8135,-306.0969 640.5075,-314.3091 644.1677,-308.3422\"/>\n<text text-anchor=\"middle\" x=\"639.639\" y=\"-326.7087\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\" fill=\"#000000\">True</text>\n</g>\n<!-- 8 -->\n<g id=\"node9\" class=\"node\">\n<title>8</title>\n<polygon fill=\"#fdf7f2\" stroke=\"#000000\" points=\"1051,-306 861,-306 861,-223 1051,-223 1051,-306\"/>\n<text text-anchor=\"middle\" x=\"956\" y=\"-290.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\" fill=\"#000000\">Age (decade)=20 &lt;= 0.591</text>\n<text text-anchor=\"middle\" x=\"956\" y=\"-275.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\" fill=\"#000000\">entropy = 0.999</text>\n<text text-anchor=\"middle\" x=\"956\" y=\"-260.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\" fill=\"#000000\">samples = 8498</text>\n<text text-anchor=\"middle\" x=\"956\" y=\"-245.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\" fill=\"#000000\">value = [4393, 4105]</text>\n<text text-anchor=\"middle\" x=\"956\" y=\"-230.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\" fill=\"#000000\">class = exceeds $50K/yr</text>\n</g>\n<!-- 0&#45;&gt;8 -->\n<g id=\"edge8\" class=\"edge\">\n<title>0&#45;&gt;8</title>\n<path fill=\"none\" stroke=\"#000000\" d=\"M828.5513,-341.8796C844.8143,-332.0056 862.2701,-321.4075 878.8858,-311.3193\"/>\n<polygon fill=\"#000000\" stroke=\"#000000\" points=\"880.756,-314.2785 887.4874,-306.0969 877.1231,-308.295 880.756,-314.2785\"/>\n<text text-anchor=\"middle\" x=\"881.5511\" y=\"-326.6819\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\" fill=\"#000000\">False</text>\n</g>\n<!-- 2 -->\n<g id=\"node3\" class=\"node\">\n<title>2</title>\n<polygon fill=\"#eca16b\" stroke=\"#000000\" points=\"370,-187 194,-187 194,-104 370,-104 370,-187\"/>\n<text text-anchor=\"middle\" x=\"282\" y=\"-171.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\" fill=\"#000000\">sex &lt;= &#45;0.355</text>\n<text text-anchor=\"middle\" x=\"282\" y=\"-156.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\" fill=\"#000000\">entropy = 0.724</text>\n<text text-anchor=\"middle\" x=\"282\" y=\"-141.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\" fill=\"#000000\">samples = 19005</text>\n<text text-anchor=\"middle\" x=\"282\" y=\"-126.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\" fill=\"#000000\">value = [15185, 3820]</text>\n<text text-anchor=\"middle\" x=\"282\" y=\"-111.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\" fill=\"#000000\">class = exceeds $50K/yr</text>\n</g>\n<!-- 1&#45;&gt;2 -->\n<g id=\"edge2\" class=\"edge\">\n<title>1&#45;&gt;2</title>\n<path fill=\"none\" stroke=\"#000000\" d=\"M470.6421,-224.5437C441.4155,-212.2973 409.2234,-198.8084 379.8343,-186.4939\"/>\n<polygon fill=\"#000000\" stroke=\"#000000\" points=\"380.9739,-183.1767 370.3982,-182.5401 378.2686,-189.6328 380.9739,-183.1767\"/>\n</g>\n<!-- 5 -->\n<g id=\"node6\" class=\"node\">\n<title>5</title>\n<polygon fill=\"#e68641\" stroke=\"#000000\" points=\"654,-187 478,-187 478,-104 654,-104 654,-187\"/>\n<text text-anchor=\"middle\" x=\"566\" y=\"-171.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\" fill=\"#000000\">sex &lt;= &#45;0.355</text>\n<text text-anchor=\"middle\" x=\"566\" y=\"-156.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\" fill=\"#000000\">entropy = 0.247</text>\n<text text-anchor=\"middle\" x=\"566\" y=\"-141.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\" fill=\"#000000\">samples = 6686</text>\n<text text-anchor=\"middle\" x=\"566\" y=\"-126.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\" fill=\"#000000\">value = [6411, 275]</text>\n<text text-anchor=\"middle\" x=\"566\" y=\"-111.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\" fill=\"#000000\">class = exceeds $50K/yr</text>\n</g>\n<!-- 1&#45;&gt;5 -->\n<g id=\"edge5\" class=\"edge\">\n<title>1&#45;&gt;5</title>\n<path fill=\"none\" stroke=\"#000000\" d=\"M566,-222.8796C566,-214.6838 566,-205.9891 566,-197.5013\"/>\n<polygon fill=\"#000000\" stroke=\"#000000\" points=\"569.5001,-197.298 566,-187.2981 562.5001,-197.2981 569.5001,-197.298\"/>\n</g>\n<!-- 3 -->\n<g id=\"node4\" class=\"node\">\n<title>3</title>\n<polygon fill=\"#e78c4b\" stroke=\"#000000\" points=\"176,-68 0,-68 0,0 176,0 176,-68\"/>\n<text text-anchor=\"middle\" x=\"88\" y=\"-52.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\" fill=\"#000000\">entropy = 0.413</text>\n<text text-anchor=\"middle\" x=\"88\" y=\"-37.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\" fill=\"#000000\">samples = 6268</text>\n<text text-anchor=\"middle\" x=\"88\" y=\"-22.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\" fill=\"#000000\">value = [5747, 521]</text>\n<text text-anchor=\"middle\" x=\"88\" y=\"-7.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\" fill=\"#000000\">class = exceeds $50K/yr</text>\n</g>\n<!-- 2&#45;&gt;3 -->\n<g id=\"edge3\" class=\"edge\">\n<title>2&#45;&gt;3</title>\n<path fill=\"none\" stroke=\"#000000\" d=\"M209.7616,-103.9815C192.2397,-93.911 173.5626,-83.1764 156.2324,-73.2161\"/>\n<polygon fill=\"#000000\" stroke=\"#000000\" points=\"157.7201,-70.0342 147.3059,-68.0856 154.2319,-76.1032 157.7201,-70.0342\"/>\n</g>\n<!-- 4 -->\n<g id=\"node5\" class=\"node\">\n<title>4</title>\n<polygon fill=\"#eead7e\" stroke=\"#000000\" points=\"370,-68 194,-68 194,0 370,0 370,-68\"/>\n<text text-anchor=\"middle\" x=\"282\" y=\"-52.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\" fill=\"#000000\">entropy = 0.825</text>\n<text text-anchor=\"middle\" x=\"282\" y=\"-37.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\" fill=\"#000000\">samples = 12737</text>\n<text text-anchor=\"middle\" x=\"282\" y=\"-22.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\" fill=\"#000000\">value = [9438, 3299]</text>\n<text text-anchor=\"middle\" x=\"282\" y=\"-7.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\" fill=\"#000000\">class = exceeds $50K/yr</text>\n</g>\n<!-- 2&#45;&gt;4 -->\n<g id=\"edge4\" class=\"edge\">\n<title>2&#45;&gt;4</title>\n<path fill=\"none\" stroke=\"#000000\" d=\"M282,-103.9815C282,-95.618 282,-86.7965 282,-78.3409\"/>\n<polygon fill=\"#000000\" stroke=\"#000000\" points=\"285.5001,-78.2636 282,-68.2637 278.5001,-78.2637 285.5001,-78.2636\"/>\n</g>\n<!-- 6 -->\n<g id=\"node7\" class=\"node\">\n<title>6</title>\n<polygon fill=\"#e6843d\" stroke=\"#000000\" points=\"564,-68 388,-68 388,0 564,0 564,-68\"/>\n<text text-anchor=\"middle\" x=\"476\" y=\"-52.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\" fill=\"#000000\">entropy = 0.142</text>\n<text text-anchor=\"middle\" x=\"476\" y=\"-37.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\" fill=\"#000000\">samples = 2597</text>\n<text text-anchor=\"middle\" x=\"476\" y=\"-22.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\" fill=\"#000000\">value = [2545, 52]</text>\n<text text-anchor=\"middle\" x=\"476\" y=\"-7.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\" fill=\"#000000\">class = exceeds $50K/yr</text>\n</g>\n<!-- 5&#45;&gt;6 -->\n<g id=\"edge6\" class=\"edge\">\n<title>5&#45;&gt;6</title>\n<path fill=\"none\" stroke=\"#000000\" d=\"M532.4873,-103.9815C525.143,-94.8828 517.3612,-85.242 509.9981,-76.1199\"/>\n<polygon fill=\"#000000\" stroke=\"#000000\" points=\"512.6612,-73.8467 503.6568,-68.2637 507.2143,-78.2434 512.6612,-73.8467\"/>\n</g>\n<!-- 7 -->\n<g id=\"node8\" class=\"node\">\n<title>7</title>\n<polygon fill=\"#e68844\" stroke=\"#000000\" points=\"758,-68 582,-68 582,0 758,0 758,-68\"/>\n<text text-anchor=\"middle\" x=\"670\" y=\"-52.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\" fill=\"#000000\">entropy = 0.305</text>\n<text text-anchor=\"middle\" x=\"670\" y=\"-37.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\" fill=\"#000000\">samples = 4089</text>\n<text text-anchor=\"middle\" x=\"670\" y=\"-22.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\" fill=\"#000000\">value = [3866, 223]</text>\n<text text-anchor=\"middle\" x=\"670\" y=\"-7.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\" fill=\"#000000\">class = exceeds $50K/yr</text>\n</g>\n<!-- 5&#45;&gt;7 -->\n<g id=\"edge7\" class=\"edge\">\n<title>5&#45;&gt;7</title>\n<path fill=\"none\" stroke=\"#000000\" d=\"M604.7258,-103.9815C613.2982,-94.7908 622.3865,-85.0472 630.971,-75.8436\"/>\n<polygon fill=\"#000000\" stroke=\"#000000\" points=\"633.7796,-77.9637 638.0411,-68.2637 628.6607,-73.1891 633.7796,-77.9637\"/>\n</g>\n<!-- 9 -->\n<g id=\"node10\" class=\"node\">\n<title>9</title>\n<polygon fill=\"#cfe7f9\" stroke=\"#000000\" points=\"1035,-187 877,-187 877,-104 1035,-104 1035,-187\"/>\n<text text-anchor=\"middle\" x=\"956\" y=\"-171.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\" fill=\"#000000\">sex &lt;= &#45;0.355</text>\n<text text-anchor=\"middle\" x=\"956\" y=\"-156.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\" fill=\"#000000\">entropy = 0.986</text>\n<text text-anchor=\"middle\" x=\"956\" y=\"-141.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\" fill=\"#000000\">samples = 6789</text>\n<text text-anchor=\"middle\" x=\"956\" y=\"-126.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\" fill=\"#000000\">value = [2927, 3862]</text>\n<text text-anchor=\"middle\" x=\"956\" y=\"-111.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\" fill=\"#000000\">class = below $50K/yr</text>\n</g>\n<!-- 8&#45;&gt;9 -->\n<g id=\"edge9\" class=\"edge\">\n<title>8&#45;&gt;9</title>\n<path fill=\"none\" stroke=\"#000000\" d=\"M956,-222.8796C956,-214.6838 956,-205.9891 956,-197.5013\"/>\n<polygon fill=\"#000000\" stroke=\"#000000\" points=\"959.5001,-197.298 956,-187.2981 952.5001,-197.2981 959.5001,-197.298\"/>\n</g>\n<!-- 12 -->\n<g id=\"node13\" class=\"node\">\n<title>12</title>\n<polygon fill=\"#e9965a\" stroke=\"#000000\" points=\"1322,-187 1146,-187 1146,-104 1322,-104 1322,-187\"/>\n<text text-anchor=\"middle\" x=\"1234\" y=\"-171.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\" fill=\"#000000\">sex &lt;= &#45;0.355</text>\n<text text-anchor=\"middle\" x=\"1234\" y=\"-156.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\" fill=\"#000000\">entropy = 0.59</text>\n<text text-anchor=\"middle\" x=\"1234\" y=\"-141.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\" fill=\"#000000\">samples = 1709</text>\n<text text-anchor=\"middle\" x=\"1234\" y=\"-126.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\" fill=\"#000000\">value = [1466, 243]</text>\n<text text-anchor=\"middle\" x=\"1234\" y=\"-111.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\" fill=\"#000000\">class = exceeds $50K/yr</text>\n</g>\n<!-- 8&#45;&gt;12 -->\n<g id=\"edge12\" class=\"edge\">\n<title>8&#45;&gt;12</title>\n<path fill=\"none\" stroke=\"#000000\" d=\"M1051.2829,-223.7134C1078.8198,-211.926 1108.9053,-199.0477 1136.571,-187.2052\"/>\n<polygon fill=\"#000000\" stroke=\"#000000\" points=\"1138.0375,-190.3847 1145.8533,-183.2318 1135.2828,-183.9495 1138.0375,-190.3847\"/>\n</g>\n<!-- 10 -->\n<g id=\"node11\" class=\"node\">\n<title>10</title>\n<polygon fill=\"#f3c3a0\" stroke=\"#000000\" points=\"952,-68 776,-68 776,0 952,0 952,-68\"/>\n<text text-anchor=\"middle\" x=\"864\" y=\"-52.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\" fill=\"#000000\">entropy = 0.927</text>\n<text text-anchor=\"middle\" x=\"864\" y=\"-37.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\" fill=\"#000000\">samples = 1736</text>\n<text text-anchor=\"middle\" x=\"864\" y=\"-22.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\" fill=\"#000000\">value = [1142, 594]</text>\n<text text-anchor=\"middle\" x=\"864\" y=\"-7.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\" fill=\"#000000\">class = exceeds $50K/yr</text>\n</g>\n<!-- 9&#45;&gt;10 -->\n<g id=\"edge10\" class=\"edge\">\n<title>9&#45;&gt;10</title>\n<path fill=\"none\" stroke=\"#000000\" d=\"M921.7426,-103.9815C914.2351,-94.8828 906.2804,-85.242 898.7536,-76.1199\"/>\n<polygon fill=\"#000000\" stroke=\"#000000\" points=\"901.3354,-73.7495 892.2714,-68.2637 895.9361,-78.2045 901.3354,-73.7495\"/>\n</g>\n<!-- 11 -->\n<g id=\"node12\" class=\"node\">\n<title>11</title>\n<polygon fill=\"#a5d3f3\" stroke=\"#000000\" points=\"1128,-68 970,-68 970,0 1128,0 1128,-68\"/>\n<text text-anchor=\"middle\" x=\"1049\" y=\"-52.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\" fill=\"#000000\">entropy = 0.937</text>\n<text text-anchor=\"middle\" x=\"1049\" y=\"-37.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\" fill=\"#000000\">samples = 5053</text>\n<text text-anchor=\"middle\" x=\"1049\" y=\"-22.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\" fill=\"#000000\">value = [1785, 3268]</text>\n<text text-anchor=\"middle\" x=\"1049\" y=\"-7.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\" fill=\"#000000\">class = below $50K/yr</text>\n</g>\n<!-- 9&#45;&gt;11 -->\n<g id=\"edge11\" class=\"edge\">\n<title>9&#45;&gt;11</title>\n<path fill=\"none\" stroke=\"#000000\" d=\"M990.6298,-103.9815C998.2189,-94.8828 1006.26,-85.242 1013.8686,-76.1199\"/>\n<polygon fill=\"#000000\" stroke=\"#000000\" points=\"1016.7039,-78.1849 1020.4213,-68.2637 1011.3283,-73.7012 1016.7039,-78.1849\"/>\n</g>\n<!-- 13 -->\n<g id=\"node14\" class=\"node\">\n<title>13</title>\n<polygon fill=\"#e88d4c\" stroke=\"#000000\" points=\"1322,-68 1146,-68 1146,0 1322,0 1322,-68\"/>\n<text text-anchor=\"middle\" x=\"1234\" y=\"-52.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\" fill=\"#000000\">entropy = 0.429</text>\n<text text-anchor=\"middle\" x=\"1234\" y=\"-37.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\" fill=\"#000000\">samples = 774</text>\n<text text-anchor=\"middle\" x=\"1234\" y=\"-22.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\" fill=\"#000000\">value = [706, 68]</text>\n<text text-anchor=\"middle\" x=\"1234\" y=\"-7.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\" fill=\"#000000\">class = exceeds $50K/yr</text>\n</g>\n<!-- 12&#45;&gt;13 -->\n<g id=\"edge13\" class=\"edge\">\n<title>12&#45;&gt;13</title>\n<path fill=\"none\" stroke=\"#000000\" d=\"M1234,-103.9815C1234,-95.618 1234,-86.7965 1234,-78.3409\"/>\n<polygon fill=\"#000000\" stroke=\"#000000\" points=\"1237.5001,-78.2636 1234,-68.2637 1230.5001,-78.2637 1237.5001,-78.2636\"/>\n</g>\n<!-- 14 -->\n<g id=\"node15\" class=\"node\">\n<title>14</title>\n<polygon fill=\"#eb9e67\" stroke=\"#000000\" points=\"1516,-68 1340,-68 1340,0 1516,0 1516,-68\"/>\n<text text-anchor=\"middle\" x=\"1428\" y=\"-52.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\" fill=\"#000000\">entropy = 0.696</text>\n<text text-anchor=\"middle\" x=\"1428\" y=\"-37.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\" fill=\"#000000\">samples = 935</text>\n<text text-anchor=\"middle\" x=\"1428\" y=\"-22.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\" fill=\"#000000\">value = [760, 175]</text>\n<text text-anchor=\"middle\" x=\"1428\" y=\"-7.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\" fill=\"#000000\">class = exceeds $50K/yr</text>\n</g>\n<!-- 12&#45;&gt;14 -->\n<g id=\"edge14\" class=\"edge\">\n<title>12&#45;&gt;14</title>\n<path fill=\"none\" stroke=\"#000000\" d=\"M1306.2384,-103.9815C1323.7603,-93.911 1342.4374,-83.1764 1359.7676,-73.2161\"/>\n<polygon fill=\"#000000\" stroke=\"#000000\" points=\"1361.7681,-76.1032 1368.6941,-68.0856 1358.2799,-70.0342 1361.7681,-76.1032\"/>\n</g>\n</g>\n</svg>\n"
          },
          "metadata": {},
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn import tree\n",
        "\n",
        "\n",
        "privileged_groups = [{'sex':1,'age': 1}]\n",
        "unprivileged_groups = [{'sex':0,'age': 0}]\n",
        "dataset_orig = load_preproc_data_german(['sex','age'])\n",
        "\n",
        "#dataset_orig_fix = dataset_orig.convert_to_dataframe()[0].drop(['race', 'sex'], axis=1)\n",
        "#STEP 3: We split between training and test set.\n",
        "train_, test_ = dataset_orig.split([0.8], shuffle=True)\n",
        "print(\"training data size\", train_.features.shape)\n",
        "print(\"dataset feature names\", train_.feature_names)\n",
        "\n",
        "# Different criterions\n",
        "criterions = ['gini', 'entropy']\n",
        "# Maximum number of levels in tree\n",
        "max_depth = [int(x) for x in np.linspace(2, 100, num = 5)]\n",
        "max_depth.append(None)\n",
        "\n",
        "#Normalize the dataset, both train and test. This should always be done in any machine learning pipeline!\n",
        "scale = StandardScaler()\n",
        "\n",
        "X_train_ = scale.fit_transform(train_.features)#do not read the first two column (sex,race) while fitting to the data\n",
        "y_train_ = train_.labels.ravel()\n",
        "\n",
        "X_test_ = scale.transform(test_.features) #do not read the first two column (sex,race) while fitting to the data\n",
        "y_test_ = test_.labels.ravel()\n",
        "\n",
        "#STEP 5: Mitigate the bias, e.g. by transforming the original dataset via reweighing.\n",
        "di = DisparateImpactRemover(sensitive_attribute='age',repair_level = 0.85)\n",
        "#We obtain a set of weights for the training set, to use in scikit-learn.\n",
        "trainining_ = di.fit_transform(train_)\n",
        "\n",
        "learner = DecisionTreeClassifier(criterion='entropy',max_depth=3)  \n",
        "learner.fit(X_train_,y_train_,sample_weight=trainining_.instance_weights)\n",
        "predictions = learner.predict(X_test_)\n",
        "model_acc = sum(predictions==y_test_)/len(y_test_)\n",
        "\n",
        "test_pred = test_.copy()\n",
        "test_pred.labels = predictions\n",
        "\n",
        "classified_metric = ClassificationMetric(test_, test_pred, unprivileged_groups=unprivileged_groups, privileged_groups=privileged_groups)\n",
        "\n",
        "#retrieving metrics from the classifier. The accuracies include percentage of true pred, statistical parity difference, disparate impact and equal opportunity.\n",
        "result = {'Accuracy_pre':model_acc,'Accuracy_post':classified_metric.accuracy(),'statistical_parity_difference': classified_metric.statistical_parity_difference(),\n",
        "      'disparate_impact': classified_metric.disparate_impact(),'equal_opportunity_difference': classified_metric.equal_opportunity_difference()}\n",
        "\n",
        "results_[str(fold_num)+'DecisionTree_'+str(criterion_1)+str(depth)]=result#saving the results of the decision tree classifier "
      ],
      "metadata": {
        "id": "2l7Y-S5qMTOZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4319ef87-dd98-4561-be8a-062105c7c79e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "training data size (800, 11)\n",
            "dataset feature names ['age', 'sex', 'credit_history=Delay', 'credit_history=None/Paid', 'credit_history=Other', 'savings=500+', 'savings=<500', 'savings=Unknown/None', 'employment=1-4 years', 'employment=4+ years', 'employment=Unemployed']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_pred_ = test_pred.labels.copy() - 1 #on the german dataset we have 2 and 1 so subtract by 1"
      ],
      "metadata": {
        "id": "Bh2UxShaNDGu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plot_model_performance(learner, test_.features, test_pred_)"
      ],
      "metadata": {
        "id": "XYDdLV8bMj8O",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 520
        },
        "outputId": "e89d90f5-9f1b-459d-a083-d33eb89c4525"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "#### Accuracy of the model :"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.095\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "#### F1 score of the model :"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.17351598173515984\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1080x432 with 3 Axes>"
            ],
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAA3sAAAGGCAYAAADCXpgNAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAgAElEQVR4nOzdeXhM5///8eckEVkkGq2tlohWg9qiCGqrNQQtrV1obW2JpbVra6u1pNZqVa0tpUGDUmortdRWqpb4WCKWUntklWXm94df5mskJEgyI16P65rrknvOnPOa6XTOeZ/7PvcxmEwmEyIiIiIiIpKt2Fk7gIiIiIiIiGQ8FXsiIiIiIiLZkIo9ERERERGRbEjFnoiIiIiISDakYk9ERERERCQbUrEnIiIiIiKSDanYk1QlJCSwcOFCWrZsiY+PD5UqVeLtt99m8eLFGI3GDN/etGnT8PX1pUmTJhmyvhkzZlCpUqUMWdeDXLhwAW9vb6pUqUJ8fHyqy/Tv3x9vb29WrlyZKdvetGlThq5XRMRaAgIC8Pb2tniUL1+eZs2a8eOPP2bKvietPD179szSbSaLjY1l9uzZNG/enPLly+Pj40Pr1q358ccfSUpKskqm9MiKfW+yixcv8tlnn1GnTh3KlClDzZo16d27N4cOHbJYztvbmx9//DFLMllL3bp1GTt27GO9VscT2Z+DtQOI7UlISKB79+6cOHGCvn37Uq1aNWJjY9m+fTuTJk1i9+7dzJgxA4PBkCHbi4yM5OuvvyYgIIBu3bplyDq7dOlC+/btM2RdaYmPj+f333+nYcOGFu3R0dFs3rwZZ2fnR1rf8ePH6dWrF1u2bHngMgULFmTHjh3kzp37sTKLiNiimjVrMn78ePPf0dHRbN++nbFjxxIbG0uXLl2yLMuMGTOws8v6c+JRUVF06tSJW7du0adPHypWrEhSUhI7d+5k6tSpbNu2ja+++gp7e/ssz3a/TZs2sXDhQr7//nsg6/a9R48e5b333qNkyZKMGzeOokWL8u+//7JgwQI6duzI5MmT8fPzy/QcT2rs2LG4u7vTu3fvJ1rP8uXLcXR0TNeyJpMJX19fVq5cSeHChXU88QxQsScpzJ8/nwMHDrBy5UpKlChhbi9ZsiSenp706dOHnTt3UqNGjQzZ3u3bt80/Pvnz58+Qdbq6uuLq6poh60pL5cqVWbVqVYpib+PGjRQoUOCBvX4Pcv9ZydTY29uTN2/eR1qviIitc3R0tPhty5s3L8WKFSMsLIzZs2dnabH33HPPZdm27hUUFMSlS5dYtWoV+fLlM7d7eXlRqVIl3nnnHb7//nveffddq+S71/37q6zY9xqNRgYOHIi3tzfz5s3DweHuoWzhwoWpXLkyPXv25IsvvqBevXrkyJEjU7M8qb///puaNWs+8Xry5MmT7mXPnDlDRESE+W8dT2R/GsYpKSxevJjmzZtbFHrJGjVqxObNmy0Kvfnz59OgQQPKlClDjRo1GDdunLnASR4esGPHDj766CMqVqxIjRo1mD59OgB79uyhbt26APTq1Yu6des+cEhBpUqVmDFjBnD3x37KlCnUrVuXsmXLUqtWLSZMmEBCQgKQcihJdHQ0I0aMoHr16pQpUwY/Pz+WLl1qfn7lypWULVuWU6dO0bZtW8qXL0+jRo3SNayhTp06bNu2jVu3blm0r1mzhjp16qRYft26dbz11luUKVOGqlWr0rt3by5fvmzOPXLkSC5evGge/pmcbePGjVSrVo2goCCLz+jmzZtUq1aNmTNnmrdx+/ZtqlWrxpQpU9LMLyJi60qUKMGtW7e4c+eOuW3x4sU0a9aM8uXLU7duXebOnWvxmuvXr9O/f38qV66Mr68v/fv359q1a+bnQ0ND6dq1K76+vrz22mv069fP4vnkYZyRkZGUKVOGxYsXW6z/0qVLeHt7s3bt2nSvb/jw4QwePJhy5cpx+vTpFO8zOjqalStX8t5771kUeslKlizJm2++yQ8//GBu8/b2ZvHixQwePBgfHx9ee+01hg8fTmJi4iO919SyLV68GD8/P1599VVq1KjB0KFDuX37NgBDhgxhzpw57N27F29vb/bs2ZNi3+vt7U1ISAijR4+mSpUqVK1alREjRlhkW7lyJXXr1qV8+fJ069aNgwcPmteXmj179nD69Gn69u1rLvSSGQwGxo4dy5o1aywKPaPRyIQJE6hcuTI+Pj4MHz7cfLwAD98vP+7nA3dHSk2dOpVatWrh4+NDx44d+eeff4C7Qy///vtvZs6cibe3t/k1D/teJ+/7Q0JCaNy4MW3btjWvK3kY58OOj/bs2WO+XKZevXoMGTIk1WOu1atX07RpU8qVK0fTpk1Zs2ZNqv8tku3YsYO3336bcuXK0aBBAxYsWGDx2d0/HHrs2LHmYz+4+z354YcfeOedd6hZsyaTJk2ievXqKYZuDxs2jMaNGwOQlJTEzJkz8fPzo1y5cjRu3JiQkJCH5nxWqdgTCxcvXuTy5csPPdNUuHBh878XLVrEl19+Sa9evVi3bh0jRoxg1apVjBkzxuI1kydPplatWoSEhNCqVSu++uor/vrrL3x8fAgODgZg/PjxLF++PF05g4OD+f777xkxYgQbNmxg/PjxrFu3jjlz5qS6/ODBg/njjz8ICgpi7dq1tGrVipEjR5p30nB3aMPYsWPp27cvq1atwtPTk6FDhxIbG/vQLK+//jqurq6sW7fO3Hbt2jV2795t/lFK9r///Y/+/ftTv359NmzYwLx587h8+TJDhw4F7g6Bad26NQUKFGDHjh3mH2WTycTSpUtZtGgRXbt2tVinh4cHQ4cOZc6cOVy8eBGA6dOn4+7uTq9evdL1eYqI2LIzZ87wwgsvkDNnTuDuPmDMmDG0bNmS1atX06tXL6ZNm2YuyEwmEz179uTixYvMnz+fhQsXcu7cOfr16wfc/Y3u3LkzBoOBhQsXMnfuXMLDw/nwww8xmUwW23Zzc+P1119n8+bNFu2//fYbzs7OvPHGG+le386dO/Hw8GD9+vUUKVIkxfs8evQocXFxvPbaaw/8LGrUqMH58+ctipHZs2dTqlQpQkJCGDZsGCtWrGDevHmP9F7vz7Zt2zZGjx5N165d2bRpE9OnT2ffvn188cUXAHzyySfUrFkTHx8fduzYgY+PT6p5v/32W4oWLcry5cvp168fS5cu5ddffwXg2LFj5vWsXLmSBg0a8MknnzzwvQMcOHAAV1fXB35GefLkSdG7uGTJEgoUKMDy5csZPHgwy5YtM+//09ovP+7nA3fnI0j+roaEhODp6Um3bt24ceMGy5cvJ1euXHTp0oUdO3YAaX+vk82dO5dPPvnEfOL8Xg87PvLx8TEPkw4ODk71s96+fTtDhw6lffv2rFmzhvbt2zNo0CB2796d6ucdGhrKBx98wBtvvMHq1av5+OOPCQoKYsWKFaku/yDff/89nTt3Jjg4GD8/P65fv27Rc5yUlMSWLVvMx0XTp0/nu+++o0ePHqxevZpWrVoxdOhQfv/990fa7rNAwzjFwtWrVwF48cUX07X8okWLaNGiBW+99RYARYsW5dy5c0ybNo1hw4aZl/P19aVFixYAfPjhh3z99dccOXKEihUrmocfuLu7kydPHmJiYtLc7smTJ3nxxRepXbu2Oe/ChQtTHbLx77//snHjRr788kuqVasGQNeuXdm7dy9Lly7F398fuHsGrmPHjuZlOnXqxLZt2wgPD6dkyZIPzOLg4ECDBg1YvXq1+VqFtWvXUqBAAcqXL2+xbNGiRVmzZg3FihXDwcGBQoUK0bJlS8aOHUtiYiKurq44OTmlGFaRkJBAu3btzL2tUVFRFutt3rw5q1evZuLEifTu3ZulS5cyf/78dI/hFxGxRQkJCWzbto3ly5dbDFucM2cOTZs25b333gPA09OT48ePs2DBAjp06MA///zDoUOHWLFiBWXKlAHgs88+Y/HixURGRhIcHEx8fDxTp04lV65cwN3ehhYtWnDgwIEUk4z4+fnx2WefERkZiZubG3B3qP4bb7yBi4sLCxcuTNf6oqOjGThw4AOvt0vPPjj5uf/++48CBQoA8NJLL5k/H09PTzZv3sz69evp0aNHut/r/dlee+011q5dy8svvwzcvVbcz8/PfGLTzc0NR0dHcuTI8dBhgMWLFzdnK1q0KDNnzuTIkSM0a9aMtWvX8txzzzF8+HDs7e156aWXCAsLS7XXM9mVK1coUKDAI80bcG8GT09P8zHIW2+9leZ+Obn38FE/n/j4eJYuXUrPnj2pVasWcLdAjouL48KFC5QrVw6DwYCLi4v580vre52sWrVqD7yU5mHHR46Ojri7uwN3i2I3NzeLIZ1wt+iqWbOm+XjG09OTCxcu8N9//6W6vaVLl+Ll5UVgYCAAxYoV4+LFixY9nOnx8ssv06xZMwAKFChA4cKF2bx5MxUrVgRg37593Lx5kyZNmhAfH8+iRYt49913admyJXD3ZPm+fftYsGBBqqOqnmXq2RMLyT+e6Zn1LDIykvPnz1OhQgWL9jJlynDnzh3Onj1r0ZbM0dERV1fXR/4huFeNGjU4deoU77//PuvWrePmzZt4eXlZ9DomO3bsGECKwqtMmTKEhoamaEuWXISmJ6e/vz8HDx4kPDwcuDuE8/5ePQAnJyeOHTtGx44dqVq1Kj4+PowbN46EhIQ0i9xXX331oc+PGjWKP/74g8DAQFq2bEnlypXTzC0iYkt+//13fHx8zI8KFSowdOhQOnfuTJ8+fYC7+57w8HB8fX0tXuvr68u5c+eIjo7myJEjGAwGSpUqZX6+XLlyTJw4ETc3N44ePUrp0qXNxQ9A6dKlcXNzS7FfAKhfvz5wt9cD4MaNG/z111/mk4XpXZ+3t/dDJ1ZJ3gffPzzxXsmjTe4tdu7vVStVqhSXLl16omyurq7s3r2bFi1a4Ovri4+PDwsWLHjkffe9+1W4u29NXseFCxcoXry4xXbTmg/AYDA88sys92fInTs30dHRQPr3y4/6+Zw/f57IyEhKly5tfo2LiwtBQUGUK1cuRcb0fK+T3bvO+z3K8VFqkr8v9xo0aJD5pH56lu/WrZu5YE2v+9fh5+dn0Zu+adMmSpYsyUsvvcSZM2eIiYlJ9bM6fvz4I233WaCePbGQfJbw3LlzKYq4+yX/8Ny7AwHMwyeioqLMz90/I6XBYEgxVOZR1KlTh2+//ZaFCxcyaNAgjEYj9evXZ9SoUXh4eKQ7570/nvfnTN6RpidnlSpVyJs3L6tXr6ZZs2b8888/jBo1KsVya9euZeDAgQQEBPDpp5/i5ubGb7/9xuTJk9PcRloXvRcqVIjq1auzadMmi2EkIiJPC19fX0aOHGn++9NPPyUhIYF+/fqZf5OTf7dHjx5tMd18cgFw7do1bt++jbOz8wMLq6ioKA4ePJiiSIqNjTX3rt3r3qGc/v7+bN68GRcXF3OPTXrXl9bvePI++MKFCw/sLUsevpm8bHK+e7m4uBAZGflE2ebMmcOXX35Jnz59qFu3Ls7Oznz//fePfF2Uk5OTxd/37v8jIiJSZE9rYpyCBQty6dIli163tCQP/00tQ3r3y4/6+ST3mKV3wpr0fK+Tv8/3H8/c61GOj1Jz+/btR5pk51GXf5D71+Hn58d3333HmTNn8PLyYtOmTebexuTRTR988IHFjLmJiYkkJCQQHx+vkU33ULEnFvLnz0+RIkXYunUrzZs3T3WZlStX8sYbb5h/bJJ3KMmS/77/Bzy9UiuyjEYjcXFxFsvVqlWLWrVqmW9xMG7cOEaPHp1iUpLkHJGRkRY7kXuH4zwpe3t7/Pz8WL9+PQ4ODnh6eqbaE/frr79SunRpPv300wzZ7r3279/Ptm3bqFatGhMmTGDJkiUZdnsMEZGs4OzsjKenp/nvTz75hBYtWrBixQreeecd4P8OdPv160e9evVSrKNgwYLmSwISEhJSHd7v5uZm7ulL7bnU+Pn5MXbsWBISEti4cSP169c3H1A+zvpSU7p0aVxcXFi3bt0Dr4Hbu3cvxYoVs5jA5f6RIdHR0ebheo+b7ddff6V+/foWk2vcO7FKRnB0dLSYdAdIMazwfpUqVSIuLo5du3aZi+173bhxg+3bt/Pmm2+max/4uPvltD6fRxkdBOn7Xl+5ciVd60rv8VFqPDw8Hqn31sPDI83/ZvefNE/P5Tply5Y1D+X09fXl0qVL5uv1kr+348ePT/VYK70nAZ4VGsYpKXTs2JH169ezf//+FM9t376dYcOGsWfPHnLlykWxYsX466+/LJY5dOgQLi4ueHl5Pdb2k3/w7v3xOH78uMXMWTt27ODUqVPA3bNBzZs3p1mzZua2e7366qsYDIZUcz5sKMSj8vf359SpU6xatSrVIZxw9/qTe8+smUwm80XiT9LTGR8fz2effUZAQABBQUGcOnWKJUuWPPb6RERsQcmSJWnVqhVBQUHmfUKuXLnw8vLi8uXLeHp6mh9ubm7m68iSr7M+ePCgeV3Hjx+nXbt2/Pfff5QtW5bz589TsGBBi3UkJiY+sPejfv36xMXF8ccff7B7927zgSfwWOtLjZOTE23atGH58uVcuHAhxfOhoaGsXr06xW0XDhw4YPH3sWPHzPvgx812//4qLi6OTZs2PdG+6n5FixblxIkTFjeKf9g9ZuFusVeqVCmmTJmSYgK15InWJk+enGLkzoM87n45rc/nxRdfJHfu3BbfwcTERDp16pTqJCLp+V6nx6McH6WmVKlSKY6XxowZw6xZsx64/P234Jg9ezajR482v6/7i8fDhw+nK4ufnx/btm1j8+bNlC9f3jwU1cvLC1dXV65evWrxWTk5OZEnTx6r3B/TlunTkBQCAgKoXbs2PXr0YP78+YSFhXHq1CnmzJlDnz59aN++vflmpZ07d2bVqlX89NNPnD9/nnXr1jFv3jzatWv32F3ouXPnplChQixbtoyTJ0/y999/M3HiRJ5//nnzMitXrqRfv37s3buXS5cusX//frZu3ZrqdWr58+fHz8+PKVOmsGPHDsLDw/nmm2/YvXs3nTt3frwPKRU+Pj4UKlSIsLAwi4OAe5UtW5YDBw6wa9cuTp8+zccff2y+pmT//v3ExcXh7u7O1atXOXDggMVsaw8ze/ZsoqOjCQwM5Pnnn6dv3758+eWXD7ygWkTkadGvXz8SEhL48ssvzW1dunRh6dKlLF26lHPnzrF//366d+/OoEGDgLvX5/n4+PD5559z/PhxQkNDGTNmDCaTifz589OyZUsSExMZMmQIoaGhnDlzhokTJ9KyZUvzrMb3Sx7KOWXKFFxdXalevbr5ucdZ34P06dMHb29v2rVrx4oVKzh37hxnz55lyZIldO7cmQYNGpin3E928uRJ5s6dy9mzZ1m+fDl//PEHTZs2faJsZcuWZevWrRw+fJjjx4/zwQcfUKNGDaKjo/nnn39ISEjA3d2dsLAwjhw5wvXr1x/pfQI0bNiQ69evExQURFhYGMHBwezbty/N102aNIlLly7RoUMHtm3bxsWLFzlw4AC9evViy5YtBAUFPXSo4/3vM6398oNe97DPx2Aw0LZtW+bPn8+2bds4d+4cY8eOJTQ01HwNobu7O4cOHSI0NJS4uLg0v9fpkdbxUXKv2Pbt2zlz5kyK1wcEBLBv3z7mzZvHhQsXCA4OZsmSJaleZwjQvn17Ll++zIQJE7hw4QIbN27km2++MZ9wefXVVzl06BCbN2/m7NmzTJw4Md33H/bz8+PQoUNs2LDB4rjK0dGRgIAAZs2axbp16zh//jw7duygQ4cO6bos5lmjfk5Jwd7enq+++oqlS5eyYsUKpk+fTo4cOXjppZcYN26cxf9w7du3Jz4+3nwW54UXXqBjx47mWZke14QJExg1ahQtW7akWLFifPLJJxbXcYwePZpx48bx0UcfERERQd68ealXrx79+/dPdX3jxo1jwoQJDBw4kMjISLy8vAgKCsrwGZuaNGnCli1bLO6Zc6/OnTtz4sQJevXqhZubG126dKFt27acOHGCjz/+mHnz5tG8eXPWrFlD586d6d+/P7lz537oNk+fPs3s2bOZNGmSecx78kHCyJEj+frrrzP0PYqIZKU8efLQq1cvvvjiC9555x3Kli1L69atSUxMZP78+Xz++efkypWLRo0aMWDAAPPrpk+fzpgxY+jQoQM5cuSgWrVq5lmiX3jhBRYuXMjEiRNp27YtRqORsmXLMnfu3IdOZOHn58eQIUNo06aNxVCxx11falxcXFi0aBELFy5k4cKFjB49Gjs7O1555RUGDBjA22+/nWJ4YkBAAGFhYeahrm3btqV169ZPlK1fv378+++/dOrUibx589K3b1+qVKnCX3/9RefOnVm9ejVt2rQxH2RPmDDhkd4n3L3efdCgQcydO5elS5dSs2ZNhg4dyrvvvvvQE8YlSpRg1apVfPXVV4wYMYJr166RN29eKlWqxPLly3nppZfSnSE9++XH/Xx69+5NQkICw4YNIyYmhpIlSzJnzhxeeOEF4O7M4JMnT6ZTp06sWrUqXd/rtKR1fFSxYkWqVq3KuHHjqF27dopbTNSsWZOxY8fy7bffMmXKFIoUKcK4ceMeOHFOiRIlmDlzJlOmTGHx4sXkz5+fwMBA8/evc+fOhIaGMnDgQBwdHWnTpg0tW7Zk2bJlab6XsmXLkj9/fsLDw1OMmOrbty85cuRg0qRJXLlyBQ8PD95880369u2b7s/qWWEwZWR/vIiIiIhkGW9vb3OB9LQxmUzmQi3Z+vXr6du3Lzt27HjoLR1EJH00jFNEREREslxoaCg1a9Zk2rRpnD9/ngMHDjBz5kxq1KihQk8kg2gYp4iIiIhkueSJVr755hvmzZuHu7s71apVY/DgwdaOJpJtaBiniIiIiIhINqRhnCIiIiIiItmQij0REREREZFsyKav2YtLtHYCya48Kj/ZrSFE0hJ7cGaGrcvZ58m+rxmZRWzD1auR1o4gIiJZJG9et8d+rXr2REREREREsiGb7tkTERHAoPNyIiIi8uhU7ImI2DqDwdoJRERE5CmkYk9ExNapZ09EREQeg44gREREREREsiH17ImI2DoN4xQREZHHoGJPRMTWZeIwzmXLljF+/Hh69+5N165diY+Pp3nz5hbLxMXF4eDgwKZNmxgyZAhbt27Fw8PD/Ly/vz+9e/fOtIwiIiLyeFTsiYjYukzq2Rs1ahQ3btygePHi5jZHR0fWr19vsdzAgQMpVaqU+e+OHTuquBMREXkK6Jo9EZFnlL+/P9OmTcPV1fWBy+zatYvQ0FA6deqUhclEREQkI6hnT0TE1mXSMM5KlSqluczkyZPp168fDg7/t7vYvXs3O3fu5MaNG5QtW5bBgweTL1++TMkoIiIij089eyIits5geLLHY9q6dStGo5F69eqZ21577TXq1q3LwoULWbVqFUajkQEDBmTEu3wmLFu2jAoVKjB37twHLhMSEkLjxo1p0KABrVq14vDhw1mYUEREshP17ImI2Dor3WdvxYoVvPnmmxZtrVq1svg7MDAQf39/oqKiyJUrV1bGe+qkdo3k/UJDQxkzZgzLly+nWLFirFu3jt69e7Nx40YcHR2zMK2IiGQH6tkTEbF1VujZi4mJ4Y8//qBu3boW7SdPniQ2Ntb8t8lkwmAwWAzzlNSl5xrJ1atXU7t2bYoVKwZAkyZNMJlM7N27N4tSiohIdqJiT0REUjh27BhGoxFPT0+L9iFDhjBr1iwAkpKSmDt3LjVr1sTJyckaMZ8q6blG8syZM3h5eVm0eXp6curUqcyKJSIiNspkNGK4/GQn+3QqVkTE1mXCMM6kpCT8/f0BuHTpEqdOnSI4OJgGDRrQv39/Ll++TN68eVO8burUqYwaNYqGDRtiMBgoU6YM48ePz/B8z6rY2Fhy5sxp0ebk5ERMTIyVEomIiDXERkUx+N1JFDAdY8r2emm/4AFU7ImI2LpMuM+evb19ivvp3atp06Y0bdo0RXuRIkX47rvvMjyP3OXi4sKdO3cs2mJjY3FxcbFSIhERyWqXT5/hvfYLOBD2HK6OrzHlCdalYZwiIrbOYPdkD3lqlChRgrCwMPPfJpOJM2fO4O3tbcVUIiKSVRyuH2LvrI84EPYcXs/fZn1wrSdbXwblEhGRzKKC7ZnRvHlz2rZty4kTJ/D29iY4OBgXFxcqV65s7WgiIpLJHM/+jPvOD+haPpYoU178+o8gT6EiT7ROFXsiIiKZ7GHXSAI4OzvTs2dPXn75ZUaOHMnHH39MQkICefPmZdasWZrtVEQkG0tMSGTyx1/Sreg35M4fS+xLHWnfYQrY50z7xWnQ3kNExNbZZfw1e5K10rpG8l4Pul5SRESyn5tXrvNhu5ls+ceNDQVas3NxIeLLBGbY9foq9kREbJ2GcYqIiGQ7/zt4lM4BKzh9xY18uaIJGu9LfNkWGboNFXsiIrYuE2bjFBEREevZuGwd7w84TNSdXFQsep35i1pRsHSFDN+Oij0REREREZEs8s2oOYyYdRuTyZHWVf9j0qL+OD+XL1O2pbFBIiK2TrdeEBERefoZk3A9MJyi1xYBMKpTBDN+/jzTCj1Qz56IiO3TME4REZGnWmL0LfLs6UHOC+vpVMWe0v4dKebXP9O3q1O+IiK2Tj17IiIiT609W/bxeuXpnNh7EKPjc0TUD6GY3/tZsm0dBYiI2DqD4ckeIiIiYhWLZ66gZYffCbuWi6l7G3KzyVYSCtbOsu1rGKeIiIiIiEgGSkhIYkTgN3z38x3Anr5N/mXIrDEYXTyyNIeKPRERW6ehmCIiIk+N61cjeb/tTLb/kxNH+0Rm9I2jxaCJYGef5VlU7ImI2DoNxRQREXkqJEVfp2XDWRy/6EoBtyh+CCpCubfes1oeFXsiIrZOPXsiIiI2zz7iJB5bWjO8jjNf7qjJ/HlNyFc2667PS42KPRERW6eePREREZtlNJo4ue0Xql/uiV1CBO/ULUvdkT0x5Pa0djTNxikiIiIiIvI4oqLu0L3VNBp0OMZfZ1y5U7QZN/022EShB+rZExGxfRrGKSIiYnPCw67xXuvvOBLuSG6nJC4X7IRn7SE2td9WsSciYutsaMPS3EIAACAASURBVKchIiIisHPLEbp1+4XrUY5457vO4pllKFqnnbVjpaBiT0TE1umaPREREZtgMplY8NV6ho05SpLREb/S5/h6Xjtci/taO1qqdLpYREREREQkHa7/tZrRE/4myWhH/8YnWbhmoM0WeqCePRER26dhnCIiItZlMuF8ZAolj47ih/be3HSvQdMhX4KDs7WTPZSKPRERW6dhnCIiIlbzz6HzXFw/hYAXvwOgfkAbYst8/FTsn1XsiYjYOvXsiYiIWMWqZXvo0387SUkFKN3Pi5fbjSW+aFNrx0o3FXsiIrbuKThzKCIikp0YjSa+GLGSL2eHA/a8W+1/vPju98TnL2ftaI9ExZ6IiIiIiMj/Fxl5h8B35/HrH7HYGYxMan+CgLFjwSWftaM9MhV7IiI2zqCePRERkSwRFnaDzq3nERpuh4dzLN8PuUmVHl+BvaO1oz0WFXsiIjZOxZ6IiEgWSIjGefcg/v2vOKXz32ZJUGFebDDsqb6cQsWeiIite3r3MSIiIjbPZDJhH30B963tyJt4mPUfvkKhlmNw9vazdrQnpmJPRERERESeSXfuJDKo91Jez/kTvSofJtGtON49l5CU+xVrR8sQKvZERGychnGKiIhkvP/+i6JLu/nsO5LAWqeqvFPXAfsm32HKmcfa0TKMij0RERunYk9ERCRjHTxwkfc6/si/1+0o8lwEy0bewe7NZZjsclg7WoZSsSciYuNU7ImIiGSclcv+ot/HW4hLsKOG1zkWTnsVt6rdrR0rU6jYExGxcSr2REREMsZ3MzYy7PN/ADu6V/+HMTM6YyhSx9qxMo2dtQOIiIiIiIhkthyXt/MO/cmXK4rp7fczbtGn2brQA/XsiYjYvkzs2Fu2bBnjx4+nd+/edO3aFYC6detiNBpxcnIyLzd06FBq165NXFwcI0aM4MCBAxgMBipWrMioUaMslhUREbEl//0XRdFby3DbO5DncidydPpRaDgHk6O7taNlOhV7IiI2LrOGcY4aNYobN25QvHjxFM9NnDgRX1/fFO3Tpk0jIiKCX3/9FYPBQGBgINOnT2fQoEGZklFERORJbNl0ive7hzCy3m/0q5VIzKt9MfmMBDt7a0fLEhrGKSJi4wwGwxM9HsTf359p06bh6uqa7iwhISEEBASQI0cOHBwcCAgIYPXq1RnxNkVERDKMyWRi1rTttO+wiohoO3aFexJRfRbRr33+zBR6oJ49ERGbl1k9e5UqVXrgcwsWLGDixInExsbSoEEDAgMDiYmJ4caNGxQrVsy8XLFixbh69SoRERHkzp07U3KKiIg8itjYBAb0WUnwqouAgRFN9tJ7Qj/iC1S1drQsp2JPREQsNGrUiPLly9OoUSMuX75Mt27dcHR05O233wawuD4v+d+xsbEq9kRExOouXYrkvfY/8NfRWFwd45nX7S/qDQjCmKuItaNZhYo9EREbl9W3Xhg8eLD53wULFqRjx44EBwcTEBAAQFxcnPn5mJgYAFxcXLI0o4iISAomEwN7zOWvo0aKedwkeNhlPNvPx5gj/ZcrZDe6Zk9ExNYZnvDxCO7cuUNoaKhFm9FoJEeOHOTOnZu8efMSFhZmfu7MmTMULFgQd/fsP6OZiIjYsKR4cu3uzbf1gmhV7ii/f+uEZ6fZ8AwXeqBiT0TE5mXWBC2piY6Opm3btmzbtg2AiIgIgoODadCgAQAtW7Zk7ty5xMfHEx8fz9y5c2nZsmWGv2cREZH0SEw0Erx4D7l/a4bzqUUUzXuH7+Y0waX2J2BQqaNhnCIiz6CkpCT8/f0BuHTpEqdOnTIXdbNmzWLy5MmMGzcOOzs7GjVqxLvvvgtAYGAgo0aNwt/fH4PBQPXq1fnggw+s+E5ERORZdetWHD3eW8bvO69zu4k9A5u+yO03fiTxeR9rR7MZKvZERGxcZlyzZ29vz/r16x/4/MqVK1Ntd3R0ZOzYsRmeR0RE5FH873/X6dRuCWfOJ5DXNZqqFZy51eR3jC4FrB3NpqjYExGxcVk9QYuIiIgt+23DaT58P4TIGAMVXrzEsuGReDT/AaODs7Wj2RwNZBURsXVZOEGLiIiIrTKZTEybspOATncLvdblj7D+24J4tPgaVOilSj17IiI2Tj17IiIikHjrIhuWrsdk8mCM/3Z6jexGgmdTa8eyaSr2RERERETEpjlc+4s8W9sR0jGK/dfLUbPvZBI8XrV2LJunYk9ExMapZ09ERJ5Ve/ZcZMXctXxX81PsTXHkLVGd17t/T5LTC9aO9lRQsSciYuNU7ImIyLPoh+8PM3jwRhISDdTOWZI27coT5RsE9o7WjvbUULEnImLjVOyJiMizJCEhieGfbmLu/KOAgT41/+StHm2IKtMTtE98JCr2RERsnfZrIiLyjLhxI5Zu7y1nx+6rONonMqv1Zt4eNJCEQvWtHe2ppGJPRERERESs7ty5CN5+6wfCL9whv1sUwR/+wavdZ5GQu4S1oz21VOyJiNg4DeMUEZFngWf0GgrnOMULhe35aVA47i1+Iimnh7VjPdVU7ImI2DgVeyIikl0ZjSbiYu6QN/RzXI7NIORdFwxlAjC+/iMmO5UqT0qfoIiIjVOx9/Q7fPgwY8aM4ebNmzg4OPD+++/z1ltvpVhu6dKlfP/99xiNRnLlysVHH31E9erVrZBYRCTzRUXF0yfwFxL/Pcgv7WdisncgZ73xxL3ynrWjZRsq9kRERDJRfHw8gYGBDB48GH9/f8LDw3n77bcpVaoU3t7e5uUOHjxIUFAQq1at4sUXX2T37t307NmTLVu24OGhYUwikr2cOxdB547BHA29jbuTM8dvFqdwu+kkFKhp7WjZip21A4iISBoMT/gQq9q9ezcA/v7+AHh6elK7dm3Wrl1rsVxoaCjFixfnxRdfBKBatWrEx8dz4cKFrA0sIpLJdu06T6MGCzkaeptX8l5j5ydbKNhtpQq9TKBiLxs48s9hOrZrTVO/BrzVrDFrVoVYO5I8hbq0fJ1ru4LoF1DP3FY4/3MET32fv3/+jIMrPmXasDY4O+UwP/+cmzM/TOxC7MGZPP+cqzViPxMMBsMTPcS6zpw5g6enp0Wbl5cXJ0+etGirWrUqZ8+e5cSJEwBs2rSJF154gVdeeSXLsoqIZLb58//mnbeDuX4zET/vk2yfEEaB91ZidPOydrRsScM4n3Lx8fF81DeQjwcMpnETf86Fh9O+zduULFWKEq94p70CEWDKkNbk9cjFibD/LNpnDe/AsdP/0qrfbHI42LNqZk+G9WjMZ9NX85ybM9sWDSB4wwErpX52qGB7usXExODk5GTRljNnTmJjYy3avLy86Nu3Ly1atMDd3Z34+HimTJlCzpw5szKuiEimWfXzMQYP3gzAgNo7GT7wVeIqzcRkZ2/lZNmXevaecnv+vDs8qHGTu8ODinp6UrNWbX5dt/ZhLxOxELx+Px0HzyMq5o5Fe9lXCrFpdygACYlJbNv/P0q/9KL5+bb957Bo1Z9ZmvVZpJ69p5urqytxcXEWbbGxsbi4uFi0bdu2jTlz5rBhwwb+/PNPfvzxRwYNGsTRo0ezMq6ISKYw3LlBu5xDaFLyfyxsv5rPJrxDXJXRoEIvU2V6z97Jkyf55ZdfOHXqlHnn5u3tTfPmzVMMa5FHF3bmDEWLWn6OnsW8OH5MBweSfrsOnUm1fcOOo7zTsCJb957AydGBBtVKsfy3vwC4FRnLrchYihbMk5VRRZ46L7/8MnPnzrVoO336tMXkLHC32KtWrRpFihQBwNvbm5IlS/Lnn3/y6quvZlleEZGMdOTIVYrmuoznwQAcIs+wulc+btddzJ28vtaO9kzI1J69kJAQ2rdvz8WLFylVqhTVq1fH29ub06dP06pVKzZt2pSZm38mxMamb3iQyOP4ZFoIFUsX5d/fJ3Jh60RuR8cxd8VOa8d65qhn7+nm6+uLg4MDK1asAO5OxLJz506aN29usVyJEiXYt28fN27cAODff/8lNDSUUqVKZXlmEZGMsHr1//Bv8gPvd/wObp0lIU95bjX9nUQVelkmU3v25syZw4oVKyhatGiK50JDQxk0aBD169fPzAjZnotL+oYHiTyOldM+YM3vhxk7ex32dnYEDW7Fognv0W7Ad9aO9mxRvfZUy5EjB7NmzWLUqFHMnj2bnDlzMnbsWLy8vAgKCsLZ2ZmePXvSpk0bLl26RLt27cyFes+ePXWfPRF56hiNJr6YuIsvp+wBoLD7TWIKN+dOna8hhyZ0y0qZWuwlJCSkWugBlCxZkjt37qT6nKTfSy+/zMIFlsODws6c1uQs8sSef86VKuW86DBoLkajCaMxiWXr9hEys6e1oz1z1Dv39CtVqhRLly5N0d6/f3/zv+3s7Pj444/5+OOPszKaiEiGioqKp+eHa1m/IQw7g5HJzX6je686xFYYAgZNF5LVMvUT9/Dw4Ndff031uZ9//lk3ic0Alav44mDvQMjPd4cHnQgNZfeunfg3a57GK0Ue7vqtaC5djeCtehXMbf61y3L4hO75ldU0jFNERJ4GYWG3aOL3Pes3hPGccyzregTT5dMPifUZpkLPSgwmk8mUWSs/fPgwvXv3xmAw4OnpiZOTE3FxcZw9exZ7e3tmzZpFyZIlH/j6uMTMSpa9hB4/zrgxo7h54waOOXPyYc9A6jdsZO1YNs2jcqC1I9gMOzsDfy3/BIAiBfIQFXuHmxHRrN56mJDNhxj/UQsK5s0NwMnwKwyYtJywC9doUb8CI3o2JYeDPcWL5OXUuSskJRnp9tn37D8abs23ZBNiD87MsHW91D/1k2bpdTqocQYlEVtx9WqktSOIiKTwxcifmTwrjFL5rrKy52byt/mWxOcrpP1Ceai8ed0e+7WZWuzB3fvA7d69m7CwMGJiYnBxcaFEiRLmC9YfRsWeZBYVe5LZMrLYe3nAkxV7pyar2MtuVOyJiK1xPPcLrtt7MOG3inz4Zjw0XoTRpYC1Y2ULT1LsZfqtFxwdHalduza1a9fO7E2JiGRLGoopIiK26M6dRL6YuIuPau/BK/xzAAb0KEhktRlg75TGqyUrZHqxJyIiT0a1noiI2JorV6J5791V7Nt/mf9tDOPX7gaiK44k9tV+2nHZEBV7IiIiIiKSbn///R+dO/3Mv5diKJw7gjFNd3H7jaXEF9FlA7ZGxZ6IiI3TME4REbEVK1eG0q/veuLuGHm92Dl+6rkTpzeXEO9R2trRJBUq9kREbJxqPRERsTaTycS4cTuZNm0vAN18D/Dl+ze5U/9XkpxesHI6eRAVeyIiNs7OTtWeiIhYlwETOa/uwN7OganN19Olcymiq84He0drR5OHULEnImLj1LMnIiLWYjSasEuKxn1HD8ZXWEub/AV55a2+RJf8QDuop4BuZS8iIiIiIils3XqWOrXmEre0OTnP/wI53Sne+StiS32oQu8poWJPRMTGGQyGJ3qIiIg8CpPJxNdfH6Bdu5WE/u82s9e5k+j+MreabCHhxXrWjiePQMM4RURsnOo1ERHJKnFxiQwYsImffjoGwPAGvzO0k4FbdTZjyulh5XTyqFTsiYjYOPXOiYhIVrh8OYp3O6/ir4P/4ZIjnkXtfqZx69pEVhoHdiobnkb6ryYiYuMys9hbtmwZ48ePp3fv3nTt2hWA8+fPM2bMGMLDw0lMTMTX15fhw4eTM2dOhgwZwtatW/Hw+L+zu/7+/vTu3TvTMoqISOaLioqnUcMfuHQ5Bk+PW4R0+YmXWgwm+pX3rB1NnoCKPRGRZ9SoUaO4ceMGxYsXt2jv27cvtWvXZvbs2cTExBAQEMCCBQt4//33AejYsaOKOxGRbCZ34hk+qLKTzUc8+KnbRnI2m0tc/tetHUuekIo9EREbl1kde/7+/lSqVImAgABzm9FopHv37tSsWRMAFxcXqlatyokTJzInhIiIWE1SkpHw8AhK5tiN2x/d+KxmFAP8yxPbYAMJuYpaO55kABV7IiI2LrOGcVaqVClFm52dHY0bNzb/HR8fz/bt22nbtq25bffu3ezcuZMbN25QtmxZBg8eTL58+TIlo4iIZI5bt+J4//21HDl4lv29ppLbI4o4r5ZEV58FDi7WjicZRLdeEBGxcQbDkz0eV3x8PAMGDCBv3ry0adMGgNdee426deuycOFCVq1ahdFoZMCAARn0TkVEJCucPHkDv0aL2bo1HFNCDP/ediPKZySRNeer0Mtm1LMnIiIp3Lhxg8DAQF544QW+/vprHBzu7i5atWplsVxgYCD+/v5ERUWRK1cua0QVEZFHsHHjGT54/xcioxIp/+JlVnb7heff+pLYwn7WjiaZQMWeiIiNy+pbL0RERPDuu+9Sq1Yt+vfvb7H9kydPUrhwYZydnYG7N941GAzmYlBERGyTyWRixox9jB27A5MJ3il3lO+6HybJL4T43K9YO55kEg3jFBGxcVk9jHP06NFUrlyZAQMGpCg0hwwZwqxZswBISkpi7ty51KxZEycnp4x4qyIikkmOHLnCuHF3C73Rjbbw/ZBbJLTYSJIKvWxNp2JFRGxcZvTsJSUl4e/vD8ClS5c4deoUwcHB+Pj48Msvv1CoUCF27txpXr5IkSLMmTOHqVOnMmrUKBo2bIjBYKBMmTKMHz8+w/OJiEgGSrpDtcjPmdo8lCLP3aZhm8ZE+gwHO3trJ5NMZjCZTCZrh3iQuERrJ5DsyqNyoLUjSDYXe3Bmhq2ryrjfn+j1e4fVyYgYYkOuXo20dgQReQrs3fsviVHXaHLnY3Jc3YvJ3onI6l9xx6tV2i8Wm5E3r9tjv1Y9eyIiIiIi2cySJUcYOGAjbjljOfRRKC++WIjbbywh8Xkfa0eTLKRiT0TExmX1BC0iIvL0Skw0MmLENubMOQhAx6qHeOGlMtysuwiTs+6J+qxRsSciYuNU64mISHrcuBFL925r+GPHBXLYJ/F1y19o37ECUVV+AHtHa8cTK1CxJyJi49SzJyIiaTl+/BqdA37m7LlI8uWKYsW7yynfpi9R3l2tHU2sSMWeiIiNU61nO65evcq1a9coVaqUtaOIiFi4GX6C8xciqFjoMivf30DuFt8Ql/91a8cSK1OxJyIikobz588zdOhQDhw4wPPPP8+OHTsYMmQILVq0wNfX19rxROQZ53j+V5rd7Mbarvmo6uNOot9aEnIVtXYssQG6qbqIiI0zGAxP9JAnN3z4cKpXr86+fftwc7s7BXb79u0JCgqycjIReVZFRyfwfo+17PhuEu5b22KXEEnthuWJf3MdRhV68v+pZ09ExMapXrO+8+fPM3/+fOD/rqEsV64cMTEx1owlIs+o8+dv0yngZ44eu86B3yNpMsSOhMqfElvmY+00xIKKPRERG6feOetzcHAgMjLS3KsHEBUVhdFotGIqEXkW7d59gS7vhXD9Rjwvv3CdkO6riWu4hPjCja0dTWyQij0REZE0NGvWjHbt2tGuXTuio6P56aefCA4OpkmTJtaOJiLPkAUL/mbYsC0kJppo+MopfvjgAPZNlxP/nLe1o4mNUrEnImLj1LNnfT179sTZ2ZnVq1fj7OzMunXrePvtt2nTpo21o4nIM2L8uB1MmboXgP61dzG6m5GYNzaQ5PiclZOJLVOxJyJi41TrWd/y5cvp0qULXbp0sWifOXMmgYGBVkolIs+MpDs0zR/CdzmfZ2aLdbzT+Q2ifUaAnb21k4mNU7EnImLj1LNnPVevXuXKlSt88803vPrqq5hMJvNzERERzJ07V8WeiGSa69djecHlNrl/70jdHHs4Ozw3DvWCiC7e2trR5CmhYk9ExMap1rOegwcPMn36dC5evEjLli0tnnNwcKBZs2ZWSiYi2d2aNf+jb+9fWRSwjhYl9pDkUgiD/xLuPO9j7WjyFFGxJyIi8gANGzakYcOG9OnTh+nTp1s7jog8A4xGE5Mn72by5D8B2PSPB02rVyWizg+YnPNZOZ08bXRTdRERG6ebqltfaoVeUlISb775phXSiEh2FRUVT5cuq5k8+U/sDEaCmm1g0qB83Gr4iwo9eSzq2RMRsXGq16zvyJEjDB8+nHPnzpnvrZeQkECRIkWsnExEsouzZ2/ROWAlx0/c4jnnWH7s+DOvd/qA6Fe6akcgj009eyIiNs7OYHiihzy5kSNH8vrrrzN16lRy587NlClTeOONN5g2bZq1o4lINmA0mngvIJjjJ27hnfcauwcsp2rvqcR5d1OhJ09EPXsiIjZO+3nri4yMpH///gA4OTlRu3ZtfHx86NevH/PmzbNyOhF52jldXM9c/6+Z5FiR2T1Ogv8qEnIVtXYsyQbUsyciIpIGOzs7oqOjzX/Hxsbi7u7OpUuXrJhKRJ5m8fFJ/LruFC6HJ+G+tS2+L57ih+GJGFuuw6hCTzKIevZERGycJlmxPn9/fxo1asSWLVvw8fEhMDCQl156ydqxROQpdeVKNF3eW8XefZdx6riSVhUgymcEsWU+1nAOyVAq9kREbJyd9vtWFxgYSLly5XB0dGTIkCGMGzeOf//9l4kTJ1o7mog8ZQ4f/o/OASu5eCmWwrkj8CoQx+26S4kv3Nja0SQbUrEnImLj1LNnG2rVqgWAu7s7EyZMsHIaEXkahYScoG+fX4mNM1K92DmW9dyDy1tLiX/O29rRJJvSNXsiIiIPkJCQwIwZM/jggw9YsGCBxXNHjhyhVatW1gkmIk8Vo9HE2DF/0KPHWmLjjHSt8hfrR53Dqd06klToSSZSz56IiI1Tx571BAUFcejQIV5//XV++ukn7OzsePPNNwkKCiIkJIQOHTpYO6KIPAUibtxm5Y+7sbezY0rz9XTpUY2YiiPBzt7a0SSbU7EnImLjDKjas5Zt27YRHBxMrly5aNmyJe3atePrr7+mfPnyrFq1Ci8vL2tHFBEbZ4j9D699HVndMZxrcbmp0nkAMcXbWDuWPCNU7ImI2DhN0GI9JpOJXLlyAVCoUCGMRiOTJk2iRo0aVk4mIrbu99/DObDtAGNfHYZ9zL+ULVGI2298y53nfawdTZ4hKvZERGycJmixHjs7y0vb3dzcVOiJyEOZTCZmz/6LkSO3YTRCg/dzUqt6VSLq/IDJOZ+148kzRhO0iIiIpNPjFt6HDx+mdevWNGjQgMaNGxMSEpLqcsePH6d169bUq1ePJk2asGnTpieJKyJZLC4ukT691zN8+N1C79P626jqV5tbDX9RoSdW8cCevWbNmqX54jVr1mRoGBERSUkde9Zz7do1xowZ88C/AT799NOHriM+Pp7AwEAGDx6Mv78/4eHhvP3225QqVQpv7/+bhS8mJobu3bszbNgwmjRpwv79+5k+fTp16tTBwUEDcURs3X//RfFup585cPAqzjkSWNBuNY27v0fMK131Qy5W88C9R5cuXbIyh4iIPICdDhKspl69ekRHRz/w7/TYvXs3AP7+/gB4enpSu3Zt1q5da1HsbdmyhTx58tCkSRMAKlWqxKJFi570LYhIFjh+/BptWi3j8pU7FH3uFj+//yvFO0whroCGfYt1PbDYa9GiRYq2mzdv4uHhkamBRETEkmo96xk/fvwTr+PMmTN4enpatHl5eXH06FGLtmPHjlG4cGGGDRvGgQMHeP755/noo4+oXLnyE2cQkcxVNOlPXI2Xqel1m6V9/iHnmytIyFXU2rFE0r5m7+bNmwwePJhy5cqZz0oGBQVx4sSJTA8nIiLytIuJicHJycmiLWfOnMTGxlq03b59mz///JNWrVqxfv16WrVqxYcffsiNGzeyMq6IpFNSkpGE+ERc/pmM518d2PL+An4Ze5scrddgVKEnNiLNYm/EiBE4OjqyYsUK3N3dAahQoUKKaxZERCRzGAyGJ3qIdbm6uhIXF2fRFhsbi4uLi0Wbm5sbZcuWxcfHB4PBQIsWLXBycuLgwYNZGVdE0iEiIo4O7VcwuutwXA+OBiBP3X7cqTcPcrhaOZ3I/0mz2Dty5Aiff/45JUqUME9BXa9ePZ1pFBHJIgbDkz3Eul5++WXOnj1r0Xb69GmL6/Xg7rV8kZGRFm0Gg0GTs4jYmFOnbuDXcBFbtp5n+R8uXIwpwO26S4kt218/umJz0iz2cuTIQXx8vEVbQkICRqMx00KJiMj/sTMYnujxMMuWLaNChQrMnTvX3Hbjxg0+/PBD6tevT8OGDZkwYYL5N99oNDJhwgQaNGhAgwYNNMwwHXx9fXFwcGDFihUAhIaGsnPnTpo3b26xXJMmTQgLC2P79u0AbNq0iTt37lChQoUszywiqdu06QyNGn7P6bAoyhb8j92fbMC57RriCze2djSRVKVZ7NWqVYsePXqwbds27ty5w86dO+nTpw+vv/56VuQTEXnmGZ7w8SCjRo1i165dFC9e3KJ95MiR5MuXj40bNxISEsLevXv58ccfAViyZAl79+5l1apV/Pbbb+TPn59Ro0Zl5Nu1SfHx8cyYMYNGjRpRt25dABYuXMilS5fSfG2OHDmYNWsWwcHBNGzYkIEDBzJ27Fi8vLwICgpi1qxZALi7uzNz5kwmTpxI/fr1+frrr5k1axa5c+fO1PcmImkzmUzMmLGXDh1CiIxKomXZY/w+9hQenVaT9Jx32isQsRKDyWQyPWyBO3fu8MUXX7BhwwYiIiIoUKAAfn5+9OrVK8UF5xktLjFTVy/PMI/KgdaOINlc7MGZGbautguf7JqtpZ19Um3fv38/lSpVIiAggDp16tC1a1eioqKoUqUK69ato1ixYgD89NNPrFy5kqVLl/LOO+/QunVrWrduDcDZs2fx9/dn3759Ka5By04+/fRTrly5QocOHRg3bhwbNmxg2bJlbN26lW+++SbL81y9Gpn2QiKSYWZ/vZfPRuwAYFTDrXzctwKxr40EO3vrBpNnQt68bo/92jQvBMiZMyefffYZn3322WNvREREHl9mTbJSqVKlFG3h4eEAFC36fzPJFStWjJMnTwJ3byOQXAQmq/8XUQAAIABJREFUL2c0Gjl79iylS5fOlJy2YNeuXfz22284ODgwceJEANq0acOCBQusG0xEMp0h9gof5BvJqsKlGNpgD/Xf701s8TbWjiWSLmkWe9euXWPq1Kns27ePiIgIPDw8qF69Or169SJPnjxZkVFE5Jlml4XX+8fGxpIjRw7zhFwATk5O5tsExMbGWozqsLOzw9HRkZiYmKwLaQU5cuTA3v7uGfx7B8SkMThGRJ5i//xzhdLPnyPvrgDsYy6ya0gYUXUXc+eFitaOJpJuaRZ7AwcOxNXVlX79+pE7d24iIiL45ZdfGDRoEN99911WZBQReaZl5e0TXFxciI+Px2g0mgu+mJgY8xBNFxcXi9sIJCUlER8fj6tr9p5qvEKFCgwZMoTu3bubezLnz59P2bJlrR1NRDLBjz8eYeCA3+hY8W++e+ciCfl8iajzAybn/NaOJvJI0iz2wsPD2bJli0Vbo0aNqF+/fqaFEhGR/5OVM3kXK1YMe3t7wsPD8fLyAixvE1CiRAnCwsKoUqUKAGFhYdjb25uXza6GDh3KkCFDaN68OUajkWbNmtGwYUNd4iCSzSQmGhk54ne+nXMI+H/s3XmcjXX/x/HXObOPLTKGrCNZbpGthCzRmMyIrNmXbJEtOyE0tiJLhSF7oUQqhCypbJE1UXYhjGX2mTPLuX5/+N1zNw1mMDPXmZn30+M8Hp1rrutc73PJmetzvht4OscQWbIz0TWngZObyelEHlyKxV6BAgWIiYlJ0m0nLi6OAgUKpGswERHJeJ6envj5+REUFMTkyZMJDw9n5cqVdO3aFYBmzZrx6aef4u/vT86cOQkKCiIgICDdJ+wyW1hYGPPmzSM6OpqIiAjy58+vBetFspjbt6Pp2f1rdv50BRenBD5qvom2/doQXaa71s+TTOuexd6WLVsAqFWrFl26dKFx48bky5ePkJAQNmzYQIMGDTIspIhIdpYeRUVCQgIBAQEA/P3335w+fZrVq1fj6+vL2LFjGT16NL6+vjg5OeHv70/z5s0BaN26NX/99RctWrTAMAyefvppJkyYkOb5HE1AQADly5enWbNmNGrUSIWeSBZz8uQNOnf4knMXo/DKEcmXPb6j4utTiSlY2+xoIo/knksv/HcdoXseaLGwbdu2dAn1X1p6QdKLll6Q9JaWSy90WXn0kY5f0rZiGiXJvsLCwti8eTMbN27k8OHD1K5dm6ZNm1KvXr3EiVsykpZeEElb/bou5PMNoVQu/DdfDjhEnpYLsecsbnYsESCdll749zi9fztz5sxDn1RERFJPrUjmy507N61ataJVq1bcunWLTZs2sXz5csaMGcPu3bvNjiciD8sw8PxtOgtqTsEnujZDX89DwotfY3fJ2pNOSfaR4pg9uPON5uXLl7Hb7VgsFiIjIxk0aBA//fRTeucTERFxGHFxcRw5coQjR45w+vRpSpYsaXYkEXkIUVFxzJ65i9HVlpLj6hoMNwuj365F1NODNT5PspQUi71169YxevRo4uPjsVgsGIaBq6srfn5+GZFPRCTb022H+Xbs2MGmTZvYvn07BQsW5JVXXmHgwIEUKlTI7Ggi8oAuXQqjc8cvOXY8hIjnbcxtm4vwFz4htmgjs6OJpLkUi725c+cyd+5cqlevTtOmTfnqq6+YP38+VapoQUkRkYxg1bfMphs3bhwBAQEsX76csmXLmh1HRB7S3r2X6dZlDcG34nny8Vu86XeZkEbbSXisjNnRRNJFisWe1Wqldu07MxEZhoG7uztvvvkmbdq04YUXXkj3gCIi2Z1qPfPt3LnT7Agi8oiWLzvKiBFbiYsH39JnWDbkOs6N1pHgltfsaCLpJsViz83NjaNHj1KxYkXc3Ny4cuUKhQoV4ubNmxmRT0Qk29MELeZp0aIFa9asoXLlyvf8ezh48GAGpxKRBxEfb2f029tYtPgYAG/V2cO4YWWxVZuNYc342XRFMlKKxV6fPn3o0KEDBw4cwM/PjzZt2pA/f36KFCmSEflERERM88477wAQFBRkchIReVjOscFE/L4FVycv5rXaRIuBPbGVbGN2LJEMkWKx17BhQ3bv3o2rqyu9e/emYMGCRERE0KRJk4zIJyKS7alhzzwVK95Zo3D9+vV3XTy+W7duPPfccxkdS0RSwTAMXG4dJveOdixuep3BL5bjqU6zseXXvBOSfdyz2AsJCbnr85QWWxcRkbSlCVrMc+jQIQ4dOsS2bdvw8fFJ8rOQkBB14RRxUBs2nGL+zM1saTsZJ+dwXApXp2T7T4n38DY7mkiGumex9/zzz99zfIJhGFgsFk6cOJFuwURE5A7VeuZxdXXl0qVLREZGsn379mQ/GzVqlEnJRORu7HaD6dN28/60fQAsLVmW1zuXI6L6dHByMzmdSMa7Z7G3bdu2jMwhIiL3oAlazFO+fHnKly/PE088Qffu3c2OIyL3ERERS783v2XDdxewWuxMCdhGx4GtiSjbU9+aSbZ1z2KvcOHCGZlDJEOd2j7d7Agikgns3r2bmjVrUqxYMbZs2XLXfRo2bJjBqUTk3y5cCKVLhy84/kc4edxjWNF1MzV7B2IrWNvsaCKmSnGCFhERMZfV7ADZ2Pz586lZsyZTpky5688tFouKPRGTXb4cjp/vEm6FJFDG6wZrBh6gYNtlxOUsbnY0EdOp2BMRcXDqxmmeJUuWACQbryciDsIwKHUziCaljnAtIieLRsTg5PsVdpccZicTcQgq9kREHJxVtZ7pQkJC2L17N/7+/gQHBzN9+p2u4G+99Rbe3prdTySjxcYmcOvaTZ46Owz3C2uZ19IJW5W3sT0zWOPzRP4hVb2D9u3bx9ixYxk8eDAAe/fuJS4uLl2DiYiIOIrRo0dz4cIFAMaPH09ISAiPP/44Y8aMMTmZSPYTHBxFy2af8dorc4j9YwN2l1zE+K7AVmmICj2Rf0mxZW/RokUsWbKExo0bs2vXLgB27NjBli1bGDt2bLoHFBHJ7tSyZ75Tp07x0UcfERkZyY8//sgPP/xAvnz58Pf3NzuaSLZy7Nh1Onf4gkt/x/JEbjgf9x+KNJpPwmNlzI4m4pBSLPZWrlzJ2rVryZ8/Pzt27ABgyJAhvPLKK+keTkRENGbPEVitdzrCHDhwgLJly5IvXz4A7Ha7mbFEspWv152kf7+NRNvg+eJ/sWrYZXI0WUeCW16zo4k4rBSLPavVSv78+ZNsc3Fx0c2HiEgGUcue+YoUKcLIkSM5fPgwHTp0AODrr78mb17dZIqkN7vdYOrkn5gx6wAAXaodYvqYEiRUX4lhdTI5nYhjS3HMnre3N59//jnwv2+XN2zYkKwAFBGR9GGxPNpDHt3kyZPJly8fr732Gu3btwfg119/5Z133jE5mUjWt23jIWbMOoDVYueDpluZ+aE/CTXeBRV6IimyGIZh3G+HkydP0r17dxISEoiIiCBv3rw4Ozszb948Spcuna7hYuLT9eUlG7sRbjM7gmRxRfK6pdlrDdvwxyMd/16AxrKklStXrnDjxg0KFChAwYIFTcsRHBxu2rlFMpLzzUPk3tGOUV+U48UKoVTv/T7x+auYHUskQ3l55XroY1Psxlm2bFm2bt3K/v37CQsLw9vbm4oVK+Lq6vrQJxURkdSzqnnOdGfOnOGtt97i9OnTuLi4YLPZqFixItOnT6do0aJmxxPJcn788SLF4n6i2tW3sCTEENi1CKH1VhDvoaVORB5Eit049+/fz7Fjx3B3d6dAgQIYhsGRI0fYv39/RuQTEcn2rI/4kEf37rvv0rhxY3799VeOHDnCgQMHqFu3Lu+++67Z0USyFMMwmB90gNdar6Z9v98JizCILtWJkIbrMVToiTywFFv2evbsmeS5zWbDyckJb29vtm7dmm7BRETkDjXsmS84ODjJ78OcOXPy5ptvaukFkTRks8UzbMh3rPz8FGCh5TMnoHYgEeV66oNQ5CGlWOwdOnQoyfPY2FhWrVqFk5MGxYqIZAR14zSf3W4nMjKSHDlyJG6LjIw0MZFI1nLtWgRdO37BgcMheLjEsbD9VhoNGktswdpmRxPJ1FIs9v7N1dWVTp060bx588QZyURERLKyBg0a0LFjR1q2bEm+fPm4efMma9aswdfX1+xoIpneoUNX6drxC65cj6foY6F82X8/T3b5hLicxc2OJpLpPXCxB3D27FmuX7+e1llEROQu1LBnvoEDB+Ll5cV3333HjRs38PLyonnz5rRt29bsaCKZm2FwdutnXLluoVaJi6x4OwwP/9XYXXKkfKyIpCjFYq9y5cpJFlC32+3YbDa6deuWrsFEROQOLapuPmdnZzp37kznzp3NjiKSdcRFkmvPm7zhvZZ8Hcrj27458VVm6BsukTSUYrEXFBSU5LnVaqVQoUIULlw43UKJiMj/aMyeeUJDQwkMDOSnn37C1dWVgIAABg0ahIuLi9nRRDKtsDAbg/t/zZjnllA158/YXXLh99YYYotqwiORtJZisffjjz8yZMiQjMgiIiLiUKZMmUJMTAwzZswgNjaWRYsWMWfOHAYMGGB2NJFM6cyZ23Rqu5JT52O4fLQcu0dfIbz+KhIeK2t2NJEsKcVi75dffuH27dvkzZs3I/KIiMi/qGHPPAcPHuTbb7/F1dUVgEqVKtG5c2cVeyIPYfv2c/Tq/hWhEVCh0DU+HXyZ0IAdGG66xxRJLykWexUqVKBFixZUrlw5WcE3evTodAsmIiJ3aMyeeSwWS2KhB5AnTx5iY2NNTCSS+RiGwccf7SNw4i7sdgvNnj7BvPHeWGp9imF9qLkCRSSVUvwXFhUVRfXq1QGtKSQiYgYLqvbMYrVazY4gkukNGfgty1eeBiyMbfgTb41rTVwpzWQrkhHuWexNmzaNIUOGMHny5IzMIyIi/6KWPfNERETw/fffYxhG4rbIyMgk2xo2bGhWPBGH53zzMH7u81nr+gKLO+/kpbcCictf1exYItmGxfjnb7B/8Pf3Z+PGjRmdJ4mYeFNPL1nYjXCb2REkiyuS1y3NXmvK9jOPdPyI+k+mUZLsp379+vf9ucViYdu2bRmU5n+Cg8Mz/JwiDyI83Eb+G9+Qa/ebWBJiuOT6Au5NF2N4eJsdTSTT8fLK9dDHqqO0iIiDS4+WvQMHDiQbd3379m0aNGjA3r17sdvtuLu7J/5s5MiR1K1bN+2DOLjt27ebHUEk0/l81W+MGbWZrd3nU7VIDNGlOuFWfTqGU9p9CSYiqXPPYu/GjRsEBgbe92BN0CIikv4s6TAdZ7Vq1di0aVPic5vNRtOmTWnTpg179+5l6tSpieO1RURSIz7ezrvjvmfu/OOAhW9+L0vp5oOIKdNT0wqLmOSexZ7dbteELCIiDiAjxuzNmTOH6tWrU7FixfQ/mYhkOSEhMfR6fTU7fg7G2ZrAh61/oM2o4cQUrGN2NJFs7Z7FnpeXlyZnERFxAOn9hfiNGzdYtWoV69evT9y2ZMkSpk6dSnR0NL6+vvTt2zfJEgQiIv/155836dR2JWf/isUrRyRf9N1PhZ4fE5erhNnRRLI9jdkTEcnmFi5cSJMmTfDy8gLAz8+PZ555Bj8/P65evUr37t1xdXWlb9++JicVEUcTEx1Hy6ZLuXoTKj3xN5+/fZO8r67A7pLT7GgiAtxzASGN1RARcQxWi+WRHveTkJDAunXrePXVVxO3DR8+nJdffhmLxUKhQoXo0KGDJioB9u3bx9ixYxk8eDAAe/fuJS4uzuRUIiaKj8Jrfw8+bvw5r1X6jc1BXuRtuRBU6Ik4jHsWe+PGjcvAGCIici9Wy6M97ueXX37B1dWV8uXLA3cmajl58mSSfex2Oy4uLun19jKFRYsWMXToUHLmzMnhw4cB2LFjh4Y7SLYUFRXH7q2/8tgmP9zPr+XVyn8R9EkLLNWHaSIWEQdzz2JPREQcg8XyaI/7OXjwIKVKlUp8HhkZSZs2bdi5cycAoaGhrF69Gl9f3/R8iw5v5cqVrF27lmHDhiWOXRwyZAi7d+82OZlIxrp8OZymjRbSutN2Dhy8SUIuH0L8txFbLMDsaCJyFxqzJyKSjV27di1xrB5Avnz5mDNnDtOmTWPSpElYrVb8/Pzo0qWLeSEdgNVqJX/+/Em2ubi4pMuyGCKOat++y7ze6QuCbxs8+XgoHkWf4bb/xxhu+cyOJiL3oGJPRMTBWUm/gmLChAnJttWsWZO1a9em2zkzI29vbz7//HNee+21xAJvw4YNyQpAkazq02WHGD5iO3HxFl566gyLA/PiVnc5hlW3kiKOTP9CRUQcnBqPzDdq1Ci6d+/OzJkziYiIoE6dOjg7OzNv3jyzo4mkq7i4BMaO2sTCpX8AFvrX3s87k5uQULqt2dFEJBVU7ImIOLiMWFRd7q9s2bJs3bqV/fv3ExYWhre3NxUrVkz12oNHjx4lMDCQ27dv4+zsTK9evZLMgPpvhw8fpm3btkycOJHmzZun1dsQeWBXf9vHF58fw9XJypx2P9N85DvE569qdiwRSSUVeyIiDi6l5RMk/e3fvx8Ad3d33N3dMQyDI0eOAPDss8/e99jY2Fj69u3L8OHDCQgI4MKFC7Ro0YJy5cpRpkyZZPvbbDZGjx6Nt7d32r8RkQfgdm41lX9/k9UdC+NZ8EnKdV9AvIf+vxTJTFTsiYiIpKBnz55JnttsNpycnPD29mbr1q33PXbPnj0ABATcma2wePHi1K1blw0bNty12Js5cyb16tVLLCZFMtrGDX8SdeQLehd5D4A6AbWIqD4dw8nN5GQi8qBU7ImIODg17Jnv0KFDSZ7HxsayatUqnJycUjz27NmzFC9ePMk2Hx8fjh8/nmzfgwcP8vPPP7NmzRq6dev2aKFFHpDdbjDj/R+YOv0QzlY3ag8uSPFXBhNTpqc+iEQyKa2zJyLi4KwWyyM9JO25urrSqVMn1qxZk+K+UVFRuLu7J9nm5uZGdHR0km0xMTGMGTOGiRMnpnosoEhaiYyMo0fnlUydfgiLxeDdxnsp3GEBMWV7qdATycTUsici4uB0n+WYzp49y/Xr11PcL0eOHMTExCTZFh0djaenZ5JtM2fOpEGDBlSsWDFNc4qk5OLFUDq3/ZTjp2zkdo9hec8D1BrwAfG5SpgdTUQekYo9EREHpy4Y5qtcuXKSBdTtdjs2my1VXS1LlSrFwoULk2w7c+ZMsvF6W7ZswW63s379egCCg4P5888/OXnyJKNGjUqDdyGS3P5fLtOp/efcDIWn8t/ky7evUaj1MuwuOc2OJiJpQMWeiIhICoKCgpI8t1qtFCpUiMKFC6d4bPXq1XF2dmbNmjW0aNGCkydPsmvXLgYOHJhkv+3btyd53rFjR5o1a6alFyT9xEdR/Nx47LHF8StzmU+mlMCtZqC6E4hkISr2REQcnEU3Xqb78ccfGTJkyEMd6+Liwpw5cxg/fjxBQUG4ubkxceJEfHx8mD59Oh4eHvTp0yeNE4vcW3y8HZfoS+TZ2R4v2xF+GlCMQs2mkODT2OxoIpLGLIZhGGaHuJeYeLMTSFZ1I9xmdgTJ4orkTbspypcd+OuRju9UrWgaJcm+WrduTVBQEHnz5jU7CgDBweFmR5BM6saNKLp3/IyWJTbyVo1tJOTyIfTFVSQ8Vs7saCJyD15euR76WLXsiYg4OM2oab4KFSrQokULKleunKzgGz16tEmpRB7Mb78F06Xdp1y8avDXmYp0DYB434UYbvnMjiYi6UTFnoiISAqioqKoXr06AJGRkSanEXlw3379O/36biTKZuW5opdYMcmTON/VYNWtoEhWpn/hIiIOTu165pk2bRpDhgxh8uTJZkcReSh2u8H7k7YyffYxwEqnasd4f5ovlv+0MzuaiGQAzegtIuLgLJZHe8jD+/cMmSKZzaS31zB99jGsFjvTWuxmxtI3VeiJZCNq2RMRcXCajVNEHobbuS9564mRfJe/NTO6nOH5vh+R4FnQ7FgikoFU7ImIODh1wTDPjRs3CAwMvO8+mqBFHM3x367yrG0eOY5/QO488GtQCDE1l2J3SrtZgkUkc1CxJyIicg92u10TskimYRgGn8zbw9jxewh8+ReGN3Ai4tkpxJTpqT7dItmUij0REQenbpzm8fLy0uQskinYbPGMHLSOT1dfBCyExech9KV1xBWqa3Y0ETGRij0REQenUk9E7uf69Ui6tV/GviPRuDvHsaDrrzQa8R5xuUqYHU1ETKZiT0TEwallzzz/XVtPxFEdOXyVLu1XcDkYiuQJ5YuRf1Oq4yLsLjnNjiYiDkDj/kVERO5h3LhxZkcQuScjLpIJA+dzORhqlbjIzk88KdV1HqjQE5H/p5Y9EREHp2/lROTfrBF/kfuHdqxscZYZ3nUYNrEdllKvmB1LRByMij0REQenbpwi8l9hYTaWf/QNo0sOxzn2BgWL+DAqaCwJj5UzO5qIOCAVeyIiDk6lnogAnD17m06vLeXPC3bcGpVlcAcXwuosxnDLZ3Y0EXFQKvZERBycGvZEZMe20/Tqvo6QSCvlva/TpGUVQhuMA6tu5UTk3jQURERERMRBGYbBvNk7advua0Iirbz69B9s/uxpCrwSqEJPRFKkTwkREQdnVUdOkWzJZotnWL/VrFz3N2BhdKNfGfj+QOwFnjU7mohkEir2REQcnLpximRPbue/4sLh/Xi6FGBhz2P4Dp2O3bOg2bFEJBNRsSci4uAsatkTyVaMhHhyHgnE87cPWNs5B+fzvEbJtouwO7mZHU1EMhkVeyIiDk4teyLZx+oVB9i64itWt/wIw8kJz/rvULJsL30QiMhDUbEnIiIiYrKEBDsTR3/LRwvPAI+zrkJVGvR/h7hCdc2OJiKZmIo9EREHpwlaRLK20NAYendextbdEThbE5jZ7jD1Ri0kLpeP2dFEJJNTsSci4uDUe0sk6zr15006t1nK6UuQP0ckK4de5pkeQdhdcpodTUSyABV7IiIOTsWeSNb0+7G/aPLKKsKinHjmiauseO9xvHzngEXLIItI2lCxJyKSzVy6dIkGDRrg45O0i9iKFSuIiIhg9OjRXLlyBScnJ1q2bEmPHj1MSiqSdVkj/uLZMx2oWqgij+eKZdaHjXEp84rZsUQki1GxJyLi4NJr6YVNmzYl29ajRw/8/Pzo2bMnt27donnz5pQuXZq6dTVJhEhaiI6Ow7i0h6KHumKNCear/hHEv7wce97/mB1NRLIg9RMQEXFwVsujPVLr9OnT/PHHH3Ts2BGAfPny0bRpU7755pt0emci2cuVK+G82nAO3bt8gT3yJrGFXiSu+fcq9EQk3ahlT0TEwaVXy97QoUM5ceIErq6udOrUCU9PTwoUKICHh0fiPiVKlGDHjh3pcn6R7OSXvRd4vdMXXA9xwidfXs4W7EveBuPAqlsxEUk/+oQREXFwaT1Bi6enJy1atKBjx46UK1eOAwcO0K1bN3r06IG7u3uSfd3c3IiOjk7bACLZzIolexg2ahex8U7Uf+o8Cz98jhxVOpgdS0SyARV7IiLZTL58+Zg0aVLi82rVqlG/fn3WrVuH3W5Psm90dDSenp4ZHVEkS4iPtzN++BqClv8FWOlb9xhjZvfGUug5s6OJSDahMXsiIg7O8oh//i0kJIQLFy4k2Wa326lYsSLXrl1L0pJ35swZypQpk+7vUSQr+nzWUoKW/4WLUwJBXY8ybulkFXoikqFU7GUBvx07Soe2rWn8si+vvtKIb79eZ3YkyQLWr/uSgHrP8cVnSxK3Xb92lTHDBtCpZWPavurHZ4vnm5YvO0nrCVoOHz5M27ZtuXz5MgB//vknP/74I507d6ZChQosWLAAgCtXrvDNN9/QvHnzjHy7IpmfPYEcB8fR7/GBdKx6hE3v/k2zSUHYPQuanUxEshl148zkYmNjeWtAXwYNGU4j/wAuXrhAu9daULZcOZ4qrW/j5eHMen8ioSG3KVo86Tpsk94ZQamnyjJh6kzCw8Lo37MjRYv7UKe+r0lJs4e0nqClXr169OrVi27dugF3xuUFBgbyzDPPMH36dEaPHo2vry/Ozs707t2b559/Pk3PL5KVbf3uN2rHBOIVvhHDyYmPP6hBTNleaT/4VkQkFVTsZXL79u4BoJF/AADFihendp26fLdxg4o9eWj1fRtRoVIVBvV+PXFbVGQkxw4fZOjbE7BYLOTOk4eApi3ZvmWjir10lh73iJ07d6Zz587JthcuXJjFixen/QlFsjjDMJg5eSOTZ/5BnZIF2NLvcaLrLyGukNaoFBHzmNqNc8aMGWaePks4d/YsxYoVT7KteAkfzpw+ZVIiyQoqVKqSbJvFYsFisWC3JyRu88yRg0t/XUi2r4hIdhIZGUevDguZPPMPLBaDhpXCCX9lmwo9ETGdqcXe999/b+bps4To6ChNlS4ZwsPTk2eqVGPlskXEx8dx6+ZNNq//mlhbrNnRsjzLIz5EJP38dTGUpr4fsu77MHK52Vgz7Aw9P/wQI3dJs6OJiKRvN86UugKFhoam5+mzBU/PHMTExCTZpqnSJb2MeGcSH02fwuttmuFd6Amq13yBXT/9YHasLM+qsT4iDmnvrjN067yG4DBnSuW/yaqpuSnW+COwaP47EXEM6VrszZ49m5IlS96z8IiKikrP02cLT5YqxdIlC5NsO3f2jMbrSbrwKuDN+Kn/63796aL5lC5TzsRE2YNKPRHHY434i72LphIcVhbfMudYMPclPJ9uanYsEZEk0rXYGzFiBHv37r3n2Dx/f//0PH228Oxz1XF2cmbdV2t4tVkL/jh5kj27d/Fm/4FmR5MsaMLbQyhXviKt2nXi6pXLbPj6S8ZPnWl2LBGRDOV8bQ95dnbg3do38HmsAU1GTMDi9bTZsUREkrEYhmGk5wkmTJhA48aNqVIl+YQPjRo14rvvvrvnsTHx6Zks6zh54gSTAsdz+9YtXN3c6N2nLy819DM7lkO7EW4zO4LDSkhIoFsqhyASAAAgAElEQVS7ZgBcv3oVD08PcuXOwwt1G1DvJT+mTXyHiIhwnJyc6NyjD/V9G5mc2DEVyeuWZq+190zIIx3//JOPpVEScRTBweFmR8iWbtyIYky/hcyuPY0nct4mttCLhNVZjOGWz+xoIpKFeXnleuhj073Yux+73Y7Veu9+7Sr2JL2o2JP0lpbF3r4zjza+ufqTedIoiTgKFXsZ77ejV+jS7lMuXnfm1adP8NnU/ERWfResWsVKRNLXoxR7pn5C3a/QExGROzQ/i4i51q85QN8BO4iKdebZoleYMqk2kc92MjuWiEiK9HWUiIiDU60nYg673WD6hHW8P+cc4ESH5/7gvXmv41ykutnRRERSRcWeiIiIyL/Y7QY92wbxzY4orBY7k1uf5PUpEzByPGF2NBGRVFOxJyLi6NS0J5Kx7AnkOhzIs2772elek+XDbvJ8748wnNzNTiYi8kBU7ImIODiLqj2RDBMdehPvA2/gdnkzo15yolU3f/LVGanBsyKSKWmGFBERB2exPNpDRFJmGAaLPtpCzWfncv34HuyueQnz/Yp8dfvoH5KIZFpq2RMRcXC6zRRJX7GxCYzqt4xlX90GPFl3ri7tugZiz+VjdjQRkUeiYk9ERESyreDrkXRvG8SeY+DuHMe8Pn/hPzwIu0tOs6OJiDwyFXsiIo5OTXsi6eLYoYt0br+CSzdcKZwnjJUTPSjbajZYNMpFRLIGFXsiIg5OE7SIpL1bF0/TtMkaImyu1ChxmSXzapO3yqtmxxIRSVMq9kREHJzmhhBJW87X9vDU3g684/sUx2/5MCnoTZy9nzY7lohImlOxJyIiks6OHj1KYGAgt2/fxtnZmV69evHqq8lbkZYtW8bnn39OfHw8Hh4eDB06lFq1apmQOGsKD7dx+YflvBA6HIs9jn5tniasTiC4P252NBGRdKFiT0TEwalhL3OLjY2lb9++DB8+nICAAC5cuECLFi0oV64cZcqUSdxv+/btzJ8/ny+//JKCBQuyceNG+vfvz+7du3FzczPxHWQNZ88E06X1IoJvxrF/QA68X2hPZNV3wapbIRHJujQCWUTE0Vke8SGm2rNnDwABAQEAFC9enLp167Jhw4Yk+xUrVoyZM2dSsGBBAOrXr09ERASXL1/O2MBZ0M4tR2n00iJO/uVCwdyRhFV9l8hnJ6vQE5EsT59yIiIOThO0ZG5nz56lePHiSbb5+Phw/PjxJNtKlSqV5PmWLVvw9vamaNGi6Z4xqzIMg09mbmDMlJPYDRdeqXCOjxa2w6NEDbOjiYhkCBV7IiIOThO0ZG5RUVG4u7sn2ebm5kZ0dPQ9j9m3bx8TJ07kgw8+wMXFJb0jZkk2Wzwjei/is/URgJWRr/zJW7PGQs4nzI4mIpJhVOyJiIikoxw5chATE5NkW3R0NJ6ennfdf926dUydOpUZM2ZQs2bNjIiY9Rh2Tq2ZwqqN7ni4JLBgwA0aDp4JTu4pHysikoWo2BMRcXBq2MvcSpUqxcKFC5NsO3PmTJLJWf5r9erVzJ07l+XLlyfr1impY4kNJdfPPXgxfhOftK7MUw1aU7rJMDWRi0i2pAlaREQcnSZoydSqV6+Os7Mza9asAeDkyZPs2rWLJk2aJNnv9OnTTJs2jaVLl6rQe0hrlu3gl/c74HZpE3bXvLw6cjylm76pQk9Esi217ImIODhN0JK5ubi4MGfOHMaPH09QUBBubm5MnDgRHx8fpk+fjoeHB3369GHZsmXExsbSo0ePJMePHDmSunXrmpQ+c0hIsDNpxKd8uPQGedxrcHxiGO4BS7Dn8jE7moiIqSyGYRhmh7iXmHizE0hWdSPcZnYEyeKK5E27ddF+vxL5SMf/54kcaZREHEVwcLjZERxGaEg0b3aYy5ZfwNmawAevX6LN+IngktPsaCIiacLLK9dDH6uWPREREcmUTp+8Quc2Szl1xY3HPaNYPsGFah1ngEWjVEREQMWeiIjDUydOkeR2fneA7m9sIzTajQqFglk+7zmeqNHM7FgiIg5FX32JiDg6TdAikoTztT0UODyQqFgrzSqfZ/2Gtir0RETuQi17IiIOThO0iNxhtxt4nl5Kzl8GU6dwHD+OL07JjrOxeDxudjQREYeklj0RERFxeH9fvk3jOlPZsfBjLPY4osr14cnuS1XoiYjch1r2REQcnJYIk+zu190n6Np5HVdD3RgTXp863QYQV7qD2bFERByeWvZERBychuxJdvb5J5to2nI9V0PdqPvUZdasbqpCT0QkldSyJyLi6NKpYtuzZw8ffPAB4eHh2O122rVrR5cuXahfvz52ux13d/fEfbWwt2S0+Hg7gYMWMWdVGOBE7wbnGTtvOE55CpsdTUQk01CxJyLi4NJjgpbg4GD69OnDnDlzqFGjBhcvXqRp06Y888wzAEydOpXq1aun+XlFUsWwM6TT+6zY6oKzNYGZvW7Qeux74OSe8rEiIpJI3ThFRLIhJycn3nvvPWrUqAFAsWLFKFWqFH/88YfJySS7s8SGkntHGwb8ZwnF8oawfqYHrcdNUaEnIvIQ1LInIuLg0mOClnz58uHr65v4/OLFi5w6dYoqVaoAsGTJEqZOnUp0dDS+vr707dsXV1fXtA8i8g/njhyiyvkeOIf+SbVSj7F/S10sxRuYHUtEJNNSsSci4uDSe5KVq1ev8sYbb9C9e3dKly6Nn58fzzzzDH5+fly9epXu3bvj6upK37590zmJZFeGYfDhuyuY9PHfrOrgTLO65Qh9cSWWXCXNjiYikqmpG6eIiKNLx+k4jx8/zmuvvcarr76aWMwNHz6cl19+GYvFQqFChejQoQPbt29Pj3cmQlRkLH3azCTwo2sYWDhj1CSk0VbsKvRERB6ZWvZERBxcekzQAncKvZ49ezJ27Fj8/PwAsNlsnDt3jrJlyybuZ7fbcXFxSZcMkr1dOn+d119bwOFzHuRys7HwbSv1ek3DsOi7aBGRtKBPUxGRbMhmszFgwIAkhR5AZGQkbdq0YefOnQCEhoayevXqJOP7RNLCLz8cwq/BQg6f8+DJ/Lf5fvlT1HtjOKjQExFJMxbDMAyzQ9xLTLzZCSSruhFuMzuCZHFF8rql2WuduxHzSMf75E8+i+H69esZOnQoxYsXT7I9ICCAqlWrMm3aNCIjI7Farfj5+dG3b1+cndUZxFEEB4ebHeHRXNlDTd/vOB38GA3KXiFoaXty+1Q2O5WIiEPy8sr10Meq2JNsScWepLe0LPbOP2KxV+IuxZ5kbpm52HP/cwk5fxnMkb8eZ/mf/gz98G2ccnqZHUtExGE9SrGnr2lFRBxdek/HKZIBbgaHsW3u+7xZYhYApV9qzojhgWDVrYiISHrRJ6yIiIikqxOHT9Gp3RdcuJGXvB0q0bhPL2yl2psdS0Qky1OxJyLi4NJrNk6RjPDd51vpM+RXIm0eVCt2nWden4itVG2zY4mIZAsq9kREHJxFtZ5kQna7wcwxi5myIARwoW2NS7y3+C3c8hU1O5qISLahYk9ExMGp1pPMJiIihrc6zebrn52xWAwCO96g+5SJWJw9zI4mIpKtqNgTEXFwatmTzMQSG4b79jc4dqIkedw9WTQ+J7W7TNL/yCIiJlCxJyIiImnCKew0uXe0xTn0D77uWZLo6pMpUbOR2bFERLItFXsiIg5PLSLi+JbN+JzLe7/hg4A/iH+sHIWarcSeq6TZsUREsjUVeyIiDk6938SRxdriGdvnQxZ9awBVae7rSdlGH2C4PPwiwCIikjZU7ImIODjVeuKogq/epOdrc9h1IgduzvF8PNigTLcgDIvV7GgiIoKKPRERh6eWPXFEx/f/RudO67h4MwdP5Ann09n/4elGrcyOJSIi/6BiT0RERB7Ivg2bea3XYaJiPanuc51Fy1rjVaaK2bFERORf1M9CRMTBWR7xj0hacj+1lBeudKVonlA61f6bLzcPUqEnIuKg1LInIuLoVK+JA4gIiyTf0bHkOrsA3GDrhwaudSdhcXIxO5qIiNyDij0REQenWk/Mdv7kOTq3XU6tIlcJau1KRI1ZuJVqb3YsERFJgYo9ERERuaef1v9A9757uB2VExJ8uFjzG3I8WdPsWCIikgoq9kREHJxm4xQzGIbB4veW8vaMGyTY3QiodJWPlvclh3cxs6OJOIyWLV8hPj4eT09P4M6/G1dXV5o3b03Tps0T97t16yaLFi3gwIF9WK1W4uPjefrpinTr1ovChYsk7hcREcHixfPZtetnLBaIj4+nQoVn6NGjN4UKPZHh7+9hnDlzmlGjhjBnzic8/nh+s+OkyG638/HHs/j5550A+PiUZMSIsTz22GPJ9m3Z8hXsdjvu7u6J2/r1e4saNV7AZovh/fcnc/ToYSwWCxUqPMPQoSOJi4unV68uDB8+mooVK2XY+/ovFXsiIg5Ok6xIRrPFxPJ2jxks2+wCWBnW4iaDZo/H6uJhdjQRhzNgwGBefPGlxOcXLpznjTdep0iRolSt+iyhoSH06tWV6tVrsHjxCjw8PIiLi2PZskX07NmZ+fOXUrhwEWJjY+nfvxeFCxdlwYKl5MqVC5sthoUL59OzZxeWL//irgWII4mPj2f06GH07z84UxR6AF99tZrDhw+yZMlK3N3dmT59KtOnT+Hdd6fcdf/Ro8dTpUq1ZNsXLJhHWFgYK1asAeDtt4eycGEQffoMYOTIsYwePZyVK9fi4ZGxn6OajVNExNFZHvEh8gAssWF8PGAkyza74OESx+IxzgyZE6hCTySVihcvwZNPluKPP04CsGrVZzz2WF6GDBmZeKPv4uJCt269qFy5KosXLwDgu+/WExoaytix75IrVy4A3Nzc6dOnP4MHD8fVNflkSAkJCXz44Qe0atWUNm2aM2nSeGJjYwF44YVqnDz5e+K+w4YNZOHCIAD69u3JvHkf0blzWxYvXkD9+rU4ffpU4r5nz56mQYNaREZGcPXq34wYMYg2bZrTokVj5syZhd1uv+t7X79+Hblz56FWrdoAREZGMG7c27Rv35JWrZowbNhbhISEALBx47f07/8GY8eOpG/fngCcPn2Kfv160aZNc1q1asqKFcsTX/v69WsMHTqA9u1b0rLlK7z77hhsNtuD/vUks2nTBpo2bY6HhwcWi4XXXmvHjz/uIDo6+oFfp1Wr13B2dsbZ2ZmWLV9j8+bvAHj66YqULl2GL79c9ch5H5Ra9kREHJzqNckoTmGnyb2jLW9XO8eRP9oyfMzL/KdBgNmxRMi9rSVul7dkyLlshRsS1uDLhz7+0KFfOXnyd958cwAABw78wosvNrjrvi++6Mvs2dMA+PXX/dSsWRsXl+RFXb16dz9+9eqVnDx5ghUrvsRisTB06ACWL19Mt269Usz5yy97CQpajLu7O2fOnGbHjq2UKvUUAFu3buGFF+ri6ZmDPn268/zztZg8eTpRUZH07duTYsVK0Lhx02SvuXnzRl56yS/x+dKli4iICGf58i+w2+307/8Gy5cvpl+/twA4fvwY778/iypVqhETE8OgQX3p2rUHzZq15MaNG3Tv3pEyZcpSteqzfPTRTAoU8Ob992cRHR1Nly5t+fbbr2jZsk2SDLdu3UwsHv8tMPA9SpZ8Msm2CxcuULTo/7qnFy5cBMMw+OuvC5QuXTbZa3zxxQo+/ngWMTHR1KnzIq+/3pPo6ChCQm5TtGjxxP2KFi3OzZs3CAsLI3fu3NSv78tnny2lY8euKf3VpCkVeyIiIsL2VWt5JW4wzsZN3L3Lsfzbt7DnKml2LBGHN2vWdBYsmAvAzZs3KFy4KOPHT6ZcufIAhIeH3bNLY/78XoSGhgIQFhaarBBJyc6d2/Hz808sEKdMmY6TU+pu75977vnEsWcNGzZi3rwP6dGjNwDbt3/PgAFDuHjxAmfOnGbu3IVYLBZy5MhJ48avsm3blmTFXnx8PL//fpwBA4Ymbuvdux9xcXFYrVasVivPPFOZs2dPJ/48Z86ciV0iDx8+SEJCPM2atfz/a5MfX9+X2bp1C1WrPsu4cRNJSEgAwMPDg3LlynPp0l/J3le+fI8ndqVMjZiYaNzc3BKfW61WXFxciI6OSbZvvXoNKF/+aerVa8D169cYPLg/rq6u+Pu/ApDkdf773zEx0eTOnZsKFZ7h3LmzicVfRlGxJyLi4DRBi6SnhPgEpg6azcxVBh2r1mH+0DgiagdhuOQyO5pIokdpaUtv/xyzN2/eRxw/fiyxGyNA3rx5CQ6+ftdjb968Qb58jwOQJ89jXLt27YHOHRISQs6c//u36ubmfp+9k8qdO0/if9eoUYvJkydw+vQp4uPjiY6O4rnnnufEieNYLBa6d++UuG9cXDyPP/54stcLCwslISGBvHnzJm47ffoUCxbM5eLFC1itFkJDQyhfvsJdM4SHhxEVFUW7di0St8XGxlK+/NPAnWJw6dKFXLt2FavVys2bN/Dz80/1+70XDw+PJN1BExISiIuLw9Mzedf1vn0HJv63t3dBWrZszbfffk2LFq8BJHmd/3YD9fC4M3nPf/+eb926qWJPRET+RxO0SHoJuxVC33az2HQwF05WO5Vq/IewekOwWJ3MjiaSKXXq1JW2bZuzZcsmGjZ8GYDnnqvBtm1baN++M5Z/fXv3ww/bqFGjFgDPPlud+fPnEB09KNkkHosXL6Bhw0ZJZu6EOwVESMjtxOcRERHExMSQP39+rFYrCQn/G1sXHh52z9wuLi68+GIDfvhhG7GxNl56yQ8nJye8vAr8//lXJGm1Sq2RIwfj5+fPpEnv4+zszJw5szh//txd9y1QwJvcufPctVUuJiaGYcMG0r//YBo3borVamXMmBF3fZ0H7cbp4/MkFy9eoHLlqgBcvHgBJycnihUrnmQ/m83GX39dTOzqCmC3Gzg7O5M7d24efzw/Fy9eoGDBQgBcuHCOAgW8E8dfmkUTtIiIODiL5dEeIndz9thJ/OvdKfTyeUaz9qMidBw7XIWeyCPw9MxBz55v8vHHM4mKigSgVau2REdHM3VqYGJrT3x8PMuXL+bYsSN07doDAD8/f7y9CzJ27Ahu375TwNlsNubO/ZDvvltPrlzJW4Pq1q3Pxo3rsdliSEhIIDBwLF99tRq4Uzz9+eedSWJOnvw9yQQsd+Pn58/evbvZuXMHL798Z6yut3dBypUrzxdfrADuLFOwZMknfP/9pmTH586dBycnp8TscKf4LF26LM7Ozpw/f47du3cRFRV11/OXK1ceZ2fnxNeOj49n9uzpHDjwC/Hx8cTExFC2bDmsVitHjx7m+PFjd51E5b/dOO/2uFs32UaNGrN27RdERERgGAaffrqYBg0aJmsljY6O5o03urJnzy4AwsLC+Pbbr6hb90UA/P1fYcWKZcTFxREbG8vKlZ8SENAk8fhbt24m5stIKvZERESymR/Wfodf46/482ounn7iNt9/05Aa/5rkQEQejr//K+TP75U482XOnDmZO3cRzs4udO3ajnbtWtChQysuXrzAvHmLyJ/fC7jTujZr1hyKF/ehd+/XadeuBa+/3p7IyEiCgpbctetf8+atqFbtWdq2bUH79q3IlSs3nTrdmQCkd+/+LF++mA4dWvPNN19Rp069++auWLESISG3cXd356mnyiRunzBhMseOHaVt2+a0a9eSc+fOUr16zWTHOzs785//lOe3344kbnvzzQHMnPk+HTq04rPPljJ06EhOnz7FjBnvJTve1dWVqVNn8PXXa2nbtjkdOrQmNjaOihUrkTNnTrp168XQoQPp2LE1O3fu4K23hvLjjzv47LOlKf+l3EeTJs14/vladO/ekTZtmhEfH8/AgXfGHQYHX6dduxaEh4fz2GOPMXnydD75ZB5t2zand+/XqVmzNq1btwOga9ceFCxYiA4dWtGp02sUK1aCTp1eTzzPb78dxcenZIZ24QSwGIZhZOgZH0BMvNkJJKu6Ef7oU/WK3E+RvA/e3eVebkclPNLxeT3VUpPVBAeHP/Sx7qeW0rvfD3x2sAJNqgYza/kAcuQvmIbpRCS7+uqrL9myZSNz5y4yO4rDGTFiEOXLV3io2Ti9vB6+K6ha9kREHJy6cUqasMeRc99gcu3px/yW3zCjdxjzv52gQk9E0kzjxk25fTuEffv2mB3FoZw4cZwTJ35PnMglI6nYExFxcJZH/CNy9fxFhrUeif3oEgyrK/H1ZtF+/DiszsnX8xIReVguLi4EBk5lxoz3E8eoZXeRkRFMnDieCROm4OnpmeHnVzdOyZbUjVPSW1p24wyLsae8033kdtf3elnNg3TjPLTjZ7r0/IG/Qz3pV+8o4z/uQ7zXc+kXTkRE0tSjdOPU0gsiIiJZ1JcfLWPgxKvEJnhSp8wN3pw+lngvLZQuIpJdqNgTEXFw6ogpDyo+Lp7J/abz4VoXwJleL99m7Py3cXHPYXY0ERHJQCr2REQcnao9eQAxobd4vcVMth59DGdrAtMHutF2+ATN1iMikg2p2BMRcXCaZEVSyynsNAV3tKWwazny5yjNktkVee6VJikfKCIiWZKKPRERB6cGGUkN+7ktPL6vO9bYED563YlBTw+jUNmnzY4lIiIm0hRtIiIimZhht/PxqJn4vrqNyLAobEUDiGn6vQo9ERFRsSci4ugsj/i4m6NHj9K6dWt8fX1p1KgR69atS983kc2l9nqvW7eORo0a4evrS6tWrTh69Oh9XzcqLIy+Lccz/hM7x/725uuYwYTV+wzD5eGn6RYRkaxD3ThFRBxdGnfjjI2NpW/fvgwfPpyAgAAuXLhAixYtKFeuHGXKlEnbk0mqr/fJkycJDAzkyy+/pESJEmzcuJF+/frx/fff4+rqmux1/z51ii7tlnPowmPkdIslaGIxfDu1y8i3JiIiDk4teyIiDs7yiH/+bc+ePQAEBAQAULx4cerWrcuGDRsy9H1lF6m93t988w1169alRIkSAPj7+2MYBr/88kuy1/x18/c0fPlzDl14jJL5w9i8pp4KPRERSUbFnohINnP27FmKFy+eZJuPjw+nTp0yKVHWltrrffbsWXx8fJJsK168OKdPn072mk26HOZauCf1/nOL77b14Knnnk/74CIikuk5dDdOd4dOJ5lZkbxuZkcQSTUPl7R9vaioKNzd3ZNsc3NzIzo6Om1PJEDqr3d0dDRubkk/m9zd3YmKikr2mrHxE9I+qIiIZDlq2RMRyWZy5MhBTExMkm3R0dF4enqalChrS+319vT0xGazpbifiIhIaqnYExHJZkqVKsX58+eTbDtz5owmZ0knqb3eTz31FOfOnUt8bhgGZ8+e1d+LiIg8NBV7IiLZTPXq1XF2dmbNmjXAnVkgd+3aRZMmTUxOljWl9no3adKEnTt38scffwCwevVqPD09efbZZzM8s4iIZA0q9rIArZcl6e3zzz+nUqVKLFy40OwokgZcXFyYM2cOq1evpmHDhgwdOpSJEycmmxxE0sb9rvf06dOZM2cOcKcFcNy4cQwaNIiGDRuyYsUKPDw8aNSoUZquzZdVpfZ34bJlywgICMDPz49XX32VXbt2ZXBSx/Cg9w6HDx+mXLlyrF27NoMSOpbUXq8TJ07QunVrGjRogL+/P1u3bs3gpI4htddr1apVBAQE0KhRI1q1asXu3bszOKljSc391gN/5huSqdlsNqN27drG+vXrDcMwjPPnzxtVq1Y1Tp48aXIyySrGjRtn9O/f32jWrJnxySefmB1HJFtI7Wf7iRMnjKpVqxrnzp0zDMMwNmzYYNSpU8ew2WwZHdlUqb1e27ZtM2rVqmX8/fffhmHcuV5VqlQxYmJiMjyzmR703iEmJsYICAgw6tata6xZsyYjozqE1F6vyMhIo1atWsaGDRsMwzCM/fv3Gx07djTi4uIyPLOZUnu9Dh48aFSrVs24fPmyYRiGsXv3bqNSpUrGrVu3MjyzI0jN/dbDfOarZS+T03pZkt4CAgKYNWsWOXLkMDuKSLaRHmvzZWWpvV7FihVj5syZFCxYEID69esTERHB5cuXMzawyR703mHmzJnUq1ePokWLZlhGR5La67V9+3by5cuHv78/ANWqVWPZsmU4O2ev6eVTe71OnjxJyZIleeKJJwCoUaMGsbGxXLp0KWMDO4jU3G89zGe+ir1MTutlSXqrVq2a2RFEsp30WJsvK0vt9SpVqlSSz7QtW7bg7e2d7YqYB7l3OHjwID///DP9+/fPqHgOJ7XX6/fff6dIkSKMGjUKPz8/2rVrx/79+zMyqkNI7fV6/vnnOX/+fOI45a1bt5I/f35Kly6dYVkdSWrutx7mMz97fdWQBWm9LBGRrCc91ubLyh7md+G+ffuYOHEiH3zwAS4uabyYpYNL7fWKiYlhzJgxTJ48GVdX14yM6FBSe73CwsLYu3cvCxcuZOLEiaxbt47evXuzZcsW8uXLl5GRTZXa6+Xj48OAAQNo1qwZuXPnJjY2lhkzZiT7TJP/eZjPfLXsZXJaL0tEJOvR2nwP5kF/F65bt46BAwcyY8YMatWqlRERHUpqr9fMmTNp0KABFStWzMh4Die11ytXrlxUqFCBypUrY7FYaNasGe7u7hw6dCgj45outddr586dLFiwgM2bN7N3715WrlzJsGHDOH78eEbGzVQe5jNfxV4mp/WyRESyHq3N92Ae5Hfh6tWrmT17NsuXL6dmzZoZlNCxpPZ6bdmyhW+++Yb69etTv359Dh8+zNSpU5k0aVIGpjVfaq9X8eLFCQ8PT7LNYrFkuzF7qb1eO3fupEaNGondqMuUKUPZsmXZu3dvRkXNdB7mM1/FXian9bJERLIerc33YFJ7vU6fPs20adNYunQppUqVMiOqQ0jt9dq+fTs//PAD27dvZ/v27VSqVInhw4czatQoM2KbJrXXy9/fn3PnzvHjjz8Cd8ag2Ww2KlWqlOGZzZTa6/XUU0+xf/9+bt26BcCVK1c4efIk5cqVy/DMmUwUS1wAAA1nSURBVMXDfOZbDMMwMiqgpI8TJ04wfvx4bt26hZubG3379sXPz8/sWJIFJCQkJM6m9ffff+Pp6UmePHnw9fVl8ODBJqcTydru9dk+ffp0PDw86NOnDwDr169n7ty5xMXF4eXlxTvvvJMtJzhIzfUaO3Ys3377Ld7e3kmOHTlyJHXr1jUpuTlS+//XP3Xs2JFmzZrRvHlzExKbK7XXa9euXUyaNAmbzUaePHkYOXJktpzoLDXXy263M3PmTDZv3ozFYsFisdCmTRs6d+5sdvwMd7/7LeCRPvNV7ImIiIiIiGRB6sYpIiIiIiKSBanYExERERERyYJU7ImIiIiIiGRBKvZERERERESyIBV7IiIiIiIiWZCKPRERERERkSxIxZ6kufr161O7dm1efvll/Pz8aNiwISNHjiQkJCRNXn/EiBFMmDABgOnTpzNnzpwUj9m8eTPXrl17qPOVKVOGY8eOJdu+du1aGjdunOLxly5dokyZMomLhj6If75XERGRlPzzd/A/H1988UWqjt20aVOaZdm3bx9ly5ZNzNCwYUMCAgJSleV+jh49mmStv9WrVxMdHQ2k/r7gQZQpU4b69esnvg8/Pz+aN2/O999/n6rj4+PjWbFiRZpmEkktZ7MDSNb09ttv8/LLLwNgs9kYPHgwgYGBTJs2LU3Pk9qFvWfPns2UKVOSLaQrIiKS1fzzd7DZPDw8khSQZ86coW3bthQpUoSaNWs+1GtWrFiRtWvXAncWo548eTINGjTAw8Mj1fcFD2rWrFlUqFAh8fmvv/5K165d+frrr/Hx8bnvsb///jsrVqygXbt26ZJN5H7Usifpzs3Njfbt2/PTTz8B8OGHHzJo0CC6dOmS+KH8yy+/0LJlS/z8/PDz82PdunWJxx84cICAgAB8fX0ZMGAAUVFRiT/7Z8tXZGQkI0aMoEGDBjRs2JBp06ZhGAb9+/fn/9q596Coyj6A49+ztasQF7FAjMkcKZ2hNMyxgS4wkLosDhfFTSa0UooQLxhmDIYj2FjGTMVUIoZOVgZymyS5LEbhNhXFgGHTZFQS1JAl3pZLtFx23z8YzoCire/7xjsv8/v8teecZ89zzvnj/J7fc57n+emnn0hOTqaoqAibzcaePXvQ6/WEhoby+OOP88svv6jnfPXVVwkODiYyMpKCggKH7/PNN99Uey5jYmL45ptvRh2vqKggKiqKoKAgtm/fTn9/PwDt7e2sW7cOvV5PSEgIWVlZ2Gy263zKQgghxN87duwYUVFRarwaGW9HKiwsxGAwEB4eTnh4+KhyBQUFGAwGFi1ahNFo5Ntvv3W4fl9fX4KDg/n0008BaGtrIz4+Hr1ej8Fg4MUXX6Svrw+AmpoaIiIiMBgM6PV69u/fDwx9MZw/fz4ARqORnp4eYmNjMZvNarvgs88+IzAwkMHBQbXuQ4cOERsbC1y73eGIBQsWcPPNN9Pc3AzA77//TkJCAgaDgdDQULZu3YrVaqW1tZWkpCRaW1sJCwvjwoULEvfFuJJkT4yLvr4+tFqtum02m9mxYwevvPKK+oLcuHEj1dXV5OTkkJmZSVtbGzabjeeee47Vq1fz0UcfsX79esxm85h1vPbaa/T19VFTU0NZWRnHjx/ngw8+4PXXXweGeuUeeeQRDh48SEVFBYWFhXzyyScsXLiQtLQ0AOrq6igsLKS4uJgPP/yQs2fPOnR/ZrOZ/Px8iouLOXbsGIGBgWRkZIwq09raSllZGeXl5Xz++edUVFRgt9tJTEzE19cXk8nE0aNH+eKLL9QeSyGEEOK/pauri5SUFDIzMzGZTKSnp/P8889fMc2ip6eHjIwM8vLyqKysJC8vD5PJRH9/P9XV1bzxxhvs37+fmpoa4uLi2LRp03UlK/39/eh0Oux2O5s3b2b+/PlUV1dTWlpKQ0MDhw4dAmD79u2kp6dTVVVFUVERJ06cuGJKxHCMP3z4MMHBwer+wMBANBoN9fX16r6Kigqio6Ov2e5whN1up7KykgsXLuDv7w/A7t278fb2pqqqivLycpqamigqKmLmzJmkpKQwc+ZMTCYTHh4eEvfFuJJkT/zjLBYLBw4cYOnSpeq+O+64Qx32UFtby6xZs9SXtK+vLw899BBVVVW0tbXR3t5OVFQUALNnz+bee+8ds57q6mqio6NRFAUnJyeKiorU/41kMpl49NFHmTJlCgBPPPEEjY2N/PHHH9TV1REQEICXlxeA2gP4d4KDg6mtrcXV1RWAgIAAWltbR5UxGo0AuLm5ERQURENDAy0tLfzwww8kJiaiKAouLi4YjUYqKysdqlcIIYS43K5du66Ys1dXV4erqyuNjY1qghIQEMDAwADt7e2j/q/T6XBzc6OgoIDTp0/j4+NDbm4uWq0Wk8lEVFQUPj4+AERHR/Pnn3/S1NTk0LV9/fXXmM1mwsLCaG9v59SpU6xZswYAZ2dnYmJiqK2tBcDT05MjR45w6tQpXF1dycnJYerUqQ7Vc8MNNxAeHk5VVRUAZ86c4bvvviM8PPya7Y6rSU5OVp+lv78/hw8f5u2338bb2xsYGhWUnp6u3se8efPGTB4l7ovxJnP2xD9i165dZGdnA0NBIyQkhPXr16vHhxMtgM7OTn7++edR8wt6e3vx8fHh0qVL6HQ6nJyc1GNXe9FfvHgRNzc3ddvZ2XnMchaLhX379qk9hwAeHh6cO3eOS5cu4e7uPmq/IywWCy+//DInTpzAbrdjtVqx2+2jyoy8bnd3d1paWujs7ERRFFasWKEe6+/vx9PT06F6hRBCiMtdbc6ezWbjwIEDmEwmrFYriqKo+0fSarXk5+ezb98+4uLicHJyIikpCaPRSGdnJ19++aWakMFQYnW1Rch6e3vVa7Hb7Xh6epKdnY2fnx9NTU1MmjSJm266SS0/HI8B3nrrLXJzc3n66acZGBhg9erVrFu3zuHnEBkZSUJCAjt27KCqqorg4GDc3Nyu2e64mpFz9lJSUtDpdKM6n+vr69m7dy9nzpxBo9HQ0dExZoezxH0x3iTZE/+I65kcPm3aNGbPnk1hYeEVx06fPk1fXx99fX3odDoAOjo6RiVkw2655ZZRwWb49+XJobe3NwaDYcyvdm5ubvz222/q9nDA+TsvvfQSZ8+epaSkBBcXF8xmM5s3bx5VxmKxqF8MLRYLHh4eao9gWVkZkyZNcqguIYQQ4t9x5MgR8vPzKSgo4LbbbqO3t1f9ync5X19fsrKysNvtHD9+nA0bNnDfffcxbdo0/Pz8HF4I5fIFWkby9PTEarXS3d2Ni4sLMBS7hxMfb29vMjIyyMjIoLGxkcTERPz9/dFoHBuYNnfuXNzd3amvr6eyslJNFK/V7nDEs88+S3h4OEajkQULFtDb20tiYiLbtm1jxYoVaDQakpOTx/yvxH0x3mQYp/ifCwoKoqWlhYaGBmBorkBaWhrNzc3cfvvteHl5cfToUQCam5s5efLkmOdZsmQJhYWFDA4OYrVaefLJJ/n444+BoV5Ki8UCQFhYGMXFxXR3dwNDSzinpqZit9tZuHAhdXV1nD9/HsDhQNDV1cWsWbNwcXGhs7OTkpIS+vv71UVYAEpLS4GhRM9sNhMQEMD06dOZN28eBw8eBIZ6V3NycigvL7+eRyiEEEL8ra6uLjw9PfHx8WFwcJC8vDy0Wu2ohc8Avv/+e9auXUt3dzeKonDPPfeg0+lQFIWwsDAqKirUOe2//vormzZt4q+//rru67n11lvx8/Pj3XffBaC7u5uSkhKWLFnC+fPnWblyJR0dHQD4+fmpUyVGGl4PYDjGXy4iIoKioiLa29sJCgoCrt3ucPS6165dy86dOxkcHGRgYIDe3l7uvvtuNBoNjY2NnDx5Un2uWq2Wnp4eBgYGJO6LcSdf9sT/3NSpU9mzZw+7d+9WEzCDwcCdd96JRqMhKyuLzMxMcnNzmTNnDgaDYczzPPPMM+zcuZPQ0FAmT55MSEgIMTExwNDLfsOGDcTHx5OUlMS5c+cwGo0oioKzszNbtmxBURSCgoKIiopi2bJluLu7s3LlyqsOBx0pISGB1NRU9Ho906dPJy0tjR9//JHly5ezd+9eYKiXNDo6mosXL/Lwww+j1+uBoYVlXnjhBXX7rrvuIi4u7j9+rkIIIcRIkZGR1NTUsGjRIjw8PEhOTkav17NlyxbeeecdtdycOXOYO3cuy5YtU5OprVu3MmPGDGbMmEF8fDxr1qzBZrOh1WpJTExk8uTJ1309iqKQnZ1NZmamGtsXL15MbGwsN954I0uXLmXVqlVoNBpsNhvR0dEEBgby1Vdfqefw8vLi/vvvZ/ny5Wzbtm3Me168eDGrVq1S7+Va7Q5HPfXUU5SWlvL+++/z2GOPsXHjRhISEpgyZQoPPvgg6enppKam4uvrS0REBIqi8MADD/Dee+9J3BfjSrFfPrFICCGEEEIIIcT/PRnGKYQQQgghhBATkCR7QgghhBBCCDEBSbInhBBCCCGEEBOQJHtCCCGEEEIIMQFJsieEEEIIIYQQE5Ake0IIIYQQQggxAUmyJ4QQQgghhBATkCR7QgghhBBCCDEBSbInhBBCCCGEEBPQvwCdnRJEoDUPzgAAAABJRU5ErkJggg==\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import graphviz\n",
        "# DOT data\n",
        "dot_data = tree.export_graphviz(learner, out_file=None, \n",
        "                                feature_names=train_.feature_names,  \n",
        "                                class_names=['good cretid','bad credit'],\n",
        "                                filled=True)\n",
        "\n",
        "# Draw graph\n",
        "graph = graphviz.Source(dot_data, format=\"png\") \n",
        "graph\n"
      ],
      "metadata": {
        "id": "dON3xs89Mrjz",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 618
        },
        "outputId": "4cfc00ee-2187-47c8-885d-3a1e4139b878"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<graphviz.files.Source at 0x7f49192851d0>"
            ],
            "image/svg+xml": "<?xml version=\"1.0\" encoding=\"UTF-8\" standalone=\"no\"?>\n<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n<!-- Generated by graphviz version 2.40.1 (20161225.0304)\n -->\n<!-- Title: Tree Pages: 1 -->\n<svg width=\"1230pt\" height=\"433pt\"\n viewBox=\"0.00 0.00 1230.00 433.00\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n<g id=\"graph0\" class=\"graph\" transform=\"scale(1 1) rotate(0) translate(4 429)\">\n<title>Tree</title>\n<polygon fill=\"#ffffff\" stroke=\"transparent\" points=\"-4,4 -4,-429 1226,-429 1226,4 -4,4\"/>\n<!-- 0 -->\n<g id=\"node1\" class=\"node\">\n<title>0</title>\n<polygon fill=\"#f0b88f\" stroke=\"#000000\" points=\"698.5,-425 525.5,-425 525.5,-342 698.5,-342 698.5,-425\"/>\n<text text-anchor=\"middle\" x=\"612\" y=\"-409.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\" fill=\"#000000\">savings=&lt;500 &lt;= &#45;0.453</text>\n<text text-anchor=\"middle\" x=\"612\" y=\"-394.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\" fill=\"#000000\">entropy = 0.886</text>\n<text text-anchor=\"middle\" x=\"612\" y=\"-379.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\" fill=\"#000000\">samples = 800</text>\n<text text-anchor=\"middle\" x=\"612\" y=\"-364.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\" fill=\"#000000\">value = [557, 243]</text>\n<text text-anchor=\"middle\" x=\"612\" y=\"-349.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\" fill=\"#000000\">class = good cretid</text>\n</g>\n<!-- 1 -->\n<g id=\"node2\" class=\"node\">\n<title>1</title>\n<polygon fill=\"#eb9d65\" stroke=\"#000000\" points=\"613,-306 405,-306 405,-223 613,-223 613,-306\"/>\n<text text-anchor=\"middle\" x=\"509\" y=\"-290.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\" fill=\"#000000\">credit_history=Other &lt;= 0.433</text>\n<text text-anchor=\"middle\" x=\"509\" y=\"-275.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\" fill=\"#000000\">entropy = 0.687</text>\n<text text-anchor=\"middle\" x=\"509\" y=\"-260.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\" fill=\"#000000\">samples = 235</text>\n<text text-anchor=\"middle\" x=\"509\" y=\"-245.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\" fill=\"#000000\">value = [192, 43]</text>\n<text text-anchor=\"middle\" x=\"509\" y=\"-230.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\" fill=\"#000000\">class = good cretid</text>\n</g>\n<!-- 0&#45;&gt;1 -->\n<g id=\"edge1\" class=\"edge\">\n<title>0&#45;&gt;1</title>\n<path fill=\"none\" stroke=\"#000000\" d=\"M575.9756,-341.8796C568.1802,-332.8733 559.8633,-323.2644 551.8356,-313.9897\"/>\n<polygon fill=\"#000000\" stroke=\"#000000\" points=\"554.3691,-311.5686 545.1782,-306.2981 549.0763,-316.1498 554.3691,-311.5686\"/>\n<text text-anchor=\"middle\" x=\"543.381\" y=\"-327.5334\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\" fill=\"#000000\">True</text>\n</g>\n<!-- 8 -->\n<g id=\"node9\" class=\"node\">\n<title>8</title>\n<polygon fill=\"#f3c6a5\" stroke=\"#000000\" points=\"785,-306 647,-306 647,-223 785,-223 785,-306\"/>\n<text text-anchor=\"middle\" x=\"716\" y=\"-290.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\" fill=\"#000000\">age &lt;= &#45;0.806</text>\n<text text-anchor=\"middle\" x=\"716\" y=\"-275.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\" fill=\"#000000\">entropy = 0.938</text>\n<text text-anchor=\"middle\" x=\"716\" y=\"-260.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\" fill=\"#000000\">samples = 565</text>\n<text text-anchor=\"middle\" x=\"716\" y=\"-245.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\" fill=\"#000000\">value = [365, 200]</text>\n<text text-anchor=\"middle\" x=\"716\" y=\"-230.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\" fill=\"#000000\">class = good cretid</text>\n</g>\n<!-- 0&#45;&gt;8 -->\n<g id=\"edge8\" class=\"edge\">\n<title>0&#45;&gt;8</title>\n<path fill=\"none\" stroke=\"#000000\" d=\"M648.3742,-341.8796C656.2452,-332.8733 664.6429,-323.2644 672.7485,-313.9897\"/>\n<polygon fill=\"#000000\" stroke=\"#000000\" points=\"675.5254,-316.131 679.4706,-306.2981 670.2546,-311.5246 675.5254,-316.131\"/>\n<text text-anchor=\"middle\" x=\"681.1484\" y=\"-327.5417\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\" fill=\"#000000\">False</text>\n</g>\n<!-- 2 -->\n<g id=\"node3\" class=\"node\">\n<title>2</title>\n<polygon fill=\"#eca470\" stroke=\"#000000\" points=\"340,-187 110,-187 110,-104 340,-104 340,-187\"/>\n<text text-anchor=\"middle\" x=\"225\" y=\"-171.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\" fill=\"#000000\">employment=1&#45;4 years &lt;= &#45;0.005</text>\n<text text-anchor=\"middle\" x=\"225\" y=\"-156.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\" fill=\"#000000\">entropy = 0.755</text>\n<text text-anchor=\"middle\" x=\"225\" y=\"-141.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\" fill=\"#000000\">samples = 161</text>\n<text text-anchor=\"middle\" x=\"225\" y=\"-126.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\" fill=\"#000000\">value = [126, 35]</text>\n<text text-anchor=\"middle\" x=\"225\" y=\"-111.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\" fill=\"#000000\">class = good cretid</text>\n</g>\n<!-- 1&#45;&gt;2 -->\n<g id=\"edge2\" class=\"edge\">\n<title>1&#45;&gt;2</title>\n<path fill=\"none\" stroke=\"#000000\" d=\"M409.6706,-222.8796C385.133,-212.598 358.7214,-201.5311 333.7615,-191.0726\"/>\n<polygon fill=\"#000000\" stroke=\"#000000\" points=\"334.849,-187.7335 324.2733,-187.0969 332.1438,-194.1897 334.849,-187.7335\"/>\n</g>\n<!-- 5 -->\n<g id=\"node6\" class=\"node\">\n<title>5</title>\n<polygon fill=\"#e89051\" stroke=\"#000000\" points=\"633,-187 385,-187 385,-104 633,-104 633,-187\"/>\n<text text-anchor=\"middle\" x=\"509\" y=\"-171.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\" fill=\"#000000\">employment=Unemployed &lt;= 1.744</text>\n<text text-anchor=\"middle\" x=\"509\" y=\"-156.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\" fill=\"#000000\">entropy = 0.494</text>\n<text text-anchor=\"middle\" x=\"509\" y=\"-141.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\" fill=\"#000000\">samples = 74</text>\n<text text-anchor=\"middle\" x=\"509\" y=\"-126.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\" fill=\"#000000\">value = [66, 8]</text>\n<text text-anchor=\"middle\" x=\"509\" y=\"-111.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\" fill=\"#000000\">class = good cretid</text>\n</g>\n<!-- 1&#45;&gt;5 -->\n<g id=\"edge5\" class=\"edge\">\n<title>1&#45;&gt;5</title>\n<path fill=\"none\" stroke=\"#000000\" d=\"M509,-222.8796C509,-214.6838 509,-205.9891 509,-197.5013\"/>\n<polygon fill=\"#000000\" stroke=\"#000000\" points=\"512.5001,-197.298 509,-187.2981 505.5001,-197.2981 512.5001,-197.298\"/>\n</g>\n<!-- 3 -->\n<g id=\"node4\" class=\"node\">\n<title>3</title>\n<polygon fill=\"#ea9a61\" stroke=\"#000000\" points=\"138,-68 0,-68 0,0 138,0 138,-68\"/>\n<text text-anchor=\"middle\" x=\"69\" y=\"-52.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\" fill=\"#000000\">entropy = 0.65</text>\n<text text-anchor=\"middle\" x=\"69\" y=\"-37.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\" fill=\"#000000\">samples = 78</text>\n<text text-anchor=\"middle\" x=\"69\" y=\"-22.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\" fill=\"#000000\">value = [65, 13]</text>\n<text text-anchor=\"middle\" x=\"69\" y=\"-7.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\" fill=\"#000000\">class = good cretid</text>\n</g>\n<!-- 2&#45;&gt;3 -->\n<g id=\"edge3\" class=\"edge\">\n<title>2&#45;&gt;3</title>\n<path fill=\"none\" stroke=\"#000000\" d=\"M166.9114,-103.9815C153.2811,-94.2394 138.7816,-83.8759 125.2342,-74.193\"/>\n<polygon fill=\"#000000\" stroke=\"#000000\" points=\"127.1092,-71.2311 116.9384,-68.2637 123.0388,-76.926 127.1092,-71.2311\"/>\n</g>\n<!-- 4 -->\n<g id=\"node5\" class=\"node\">\n<title>4</title>\n<polygon fill=\"#eeae80\" stroke=\"#000000\" points=\"294,-68 156,-68 156,0 294,0 294,-68\"/>\n<text text-anchor=\"middle\" x=\"225\" y=\"-52.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\" fill=\"#000000\">entropy = 0.834</text>\n<text text-anchor=\"middle\" x=\"225\" y=\"-37.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\" fill=\"#000000\">samples = 83</text>\n<text text-anchor=\"middle\" x=\"225\" y=\"-22.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\" fill=\"#000000\">value = [61, 22]</text>\n<text text-anchor=\"middle\" x=\"225\" y=\"-7.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\" fill=\"#000000\">class = good cretid</text>\n</g>\n<!-- 2&#45;&gt;4 -->\n<g id=\"edge4\" class=\"edge\">\n<title>2&#45;&gt;4</title>\n<path fill=\"none\" stroke=\"#000000\" d=\"M225,-103.9815C225,-95.618 225,-86.7965 225,-78.3409\"/>\n<polygon fill=\"#000000\" stroke=\"#000000\" points=\"228.5001,-78.2636 225,-68.2637 221.5001,-78.2637 228.5001,-78.2636\"/>\n</g>\n<!-- 6 -->\n<g id=\"node7\" class=\"node\">\n<title>6</title>\n<polygon fill=\"#e89152\" stroke=\"#000000\" points=\"450,-68 312,-68 312,0 450,0 450,-68\"/>\n<text text-anchor=\"middle\" x=\"381\" y=\"-52.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\" fill=\"#000000\">entropy = 0.503</text>\n<text text-anchor=\"middle\" x=\"381\" y=\"-37.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\" fill=\"#000000\">samples = 72</text>\n<text text-anchor=\"middle\" x=\"381\" y=\"-22.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\" fill=\"#000000\">value = [64, 8]</text>\n<text text-anchor=\"middle\" x=\"381\" y=\"-7.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\" fill=\"#000000\">class = good cretid</text>\n</g>\n<!-- 5&#45;&gt;6 -->\n<g id=\"edge6\" class=\"edge\">\n<title>5&#45;&gt;6</title>\n<path fill=\"none\" stroke=\"#000000\" d=\"M461.3375,-103.9815C450.4703,-94.5151 438.9295,-84.462 428.0865,-75.0168\"/>\n<polygon fill=\"#000000\" stroke=\"#000000\" points=\"430.1734,-72.1929 420.3341,-68.2637 425.5755,-77.4712 430.1734,-72.1929\"/>\n</g>\n<!-- 7 -->\n<g id=\"node8\" class=\"node\">\n<title>7</title>\n<polygon fill=\"#e58139\" stroke=\"#000000\" points=\"606,-68 468,-68 468,0 606,0 606,-68\"/>\n<text text-anchor=\"middle\" x=\"537\" y=\"-52.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\" fill=\"#000000\">entropy = 0.0</text>\n<text text-anchor=\"middle\" x=\"537\" y=\"-37.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\" fill=\"#000000\">samples = 2</text>\n<text text-anchor=\"middle\" x=\"537\" y=\"-22.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\" fill=\"#000000\">value = [2, 0]</text>\n<text text-anchor=\"middle\" x=\"537\" y=\"-7.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\" fill=\"#000000\">class = good cretid</text>\n</g>\n<!-- 5&#45;&gt;7 -->\n<g id=\"edge7\" class=\"edge\">\n<title>5&#45;&gt;7</title>\n<path fill=\"none\" stroke=\"#000000\" d=\"M519.4262,-103.9815C521.5495,-95.5261 523.7904,-86.6026 525.935,-78.0623\"/>\n<polygon fill=\"#000000\" stroke=\"#000000\" points=\"529.3546,-78.815 528.3957,-68.2637 522.5654,-77.1101 529.3546,-78.815\"/>\n</g>\n<!-- 9 -->\n<g id=\"node10\" class=\"node\">\n<title>9</title>\n<polygon fill=\"#e8f4fc\" stroke=\"#000000\" points=\"781,-187 651,-187 651,-104 781,-104 781,-187\"/>\n<text text-anchor=\"middle\" x=\"716\" y=\"-171.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\" fill=\"#000000\">sex &lt;= &#45;0.414</text>\n<text text-anchor=\"middle\" x=\"716\" y=\"-156.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\" fill=\"#000000\">entropy = 0.997</text>\n<text text-anchor=\"middle\" x=\"716\" y=\"-141.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\" fill=\"#000000\">samples = 115</text>\n<text text-anchor=\"middle\" x=\"716\" y=\"-126.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\" fill=\"#000000\">value = [54, 61]</text>\n<text text-anchor=\"middle\" x=\"716\" y=\"-111.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\" fill=\"#000000\">class = bad credit</text>\n</g>\n<!-- 8&#45;&gt;9 -->\n<g id=\"edge9\" class=\"edge\">\n<title>8&#45;&gt;9</title>\n<path fill=\"none\" stroke=\"#000000\" d=\"M716,-222.8796C716,-214.6838 716,-205.9891 716,-197.5013\"/>\n<polygon fill=\"#000000\" stroke=\"#000000\" points=\"719.5001,-197.298 716,-187.2981 712.5001,-197.2981 719.5001,-197.298\"/>\n</g>\n<!-- 12 -->\n<g id=\"node13\" class=\"node\">\n<title>12</title>\n<polygon fill=\"#f1b991\" stroke=\"#000000\" points=\"1101,-187 893,-187 893,-104 1101,-104 1101,-187\"/>\n<text text-anchor=\"middle\" x=\"997\" y=\"-171.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\" fill=\"#000000\">credit_history=Other &lt;= 0.433</text>\n<text text-anchor=\"middle\" x=\"997\" y=\"-156.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\" fill=\"#000000\">entropy = 0.892</text>\n<text text-anchor=\"middle\" x=\"997\" y=\"-141.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\" fill=\"#000000\">samples = 450</text>\n<text text-anchor=\"middle\" x=\"997\" y=\"-126.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\" fill=\"#000000\">value = [311, 139]</text>\n<text text-anchor=\"middle\" x=\"997\" y=\"-111.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\" fill=\"#000000\">class = good cretid</text>\n</g>\n<!-- 8&#45;&gt;12 -->\n<g id=\"edge12\" class=\"edge\">\n<title>8&#45;&gt;12</title>\n<path fill=\"none\" stroke=\"#000000\" d=\"M785.1002,-235.2369C816.5756,-221.9075 854.5623,-205.8206 889.6291,-190.9702\"/>\n<polygon fill=\"#000000\" stroke=\"#000000\" points=\"891.1099,-194.1441 898.9533,-187.0215 888.3801,-187.6983 891.1099,-194.1441\"/>\n</g>\n<!-- 10 -->\n<g id=\"node11\" class=\"node\">\n<title>10</title>\n<polygon fill=\"#bfdff7\" stroke=\"#000000\" points=\"754,-68 624,-68 624,0 754,0 754,-68\"/>\n<text text-anchor=\"middle\" x=\"689\" y=\"-52.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\" fill=\"#000000\">entropy = 0.973</text>\n<text text-anchor=\"middle\" x=\"689\" y=\"-37.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\" fill=\"#000000\">samples = 62</text>\n<text text-anchor=\"middle\" x=\"689\" y=\"-22.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\" fill=\"#000000\">value = [25, 37]</text>\n<text text-anchor=\"middle\" x=\"689\" y=\"-7.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\" fill=\"#000000\">class = bad credit</text>\n</g>\n<!-- 9&#45;&gt;10 -->\n<g id=\"edge10\" class=\"edge\">\n<title>9&#45;&gt;10</title>\n<path fill=\"none\" stroke=\"#000000\" d=\"M705.9462,-103.9815C703.8987,-95.5261 701.7379,-86.6026 699.6698,-78.0623\"/>\n<polygon fill=\"#000000\" stroke=\"#000000\" points=\"703.0523,-77.159 697.297,-68.2637 696.2489,-78.8065 703.0523,-77.159\"/>\n</g>\n<!-- 11 -->\n<g id=\"node12\" class=\"node\">\n<title>11</title>\n<polygon fill=\"#fbe9dd\" stroke=\"#000000\" points=\"910,-68 772,-68 772,0 910,0 910,-68\"/>\n<text text-anchor=\"middle\" x=\"841\" y=\"-52.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\" fill=\"#000000\">entropy = 0.994</text>\n<text text-anchor=\"middle\" x=\"841\" y=\"-37.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\" fill=\"#000000\">samples = 53</text>\n<text text-anchor=\"middle\" x=\"841\" y=\"-22.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\" fill=\"#000000\">value = [29, 24]</text>\n<text text-anchor=\"middle\" x=\"841\" y=\"-7.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\" fill=\"#000000\">class = good cretid</text>\n</g>\n<!-- 9&#45;&gt;11 -->\n<g id=\"edge11\" class=\"edge\">\n<title>9&#45;&gt;11</title>\n<path fill=\"none\" stroke=\"#000000\" d=\"M762.5454,-103.9815C773.1579,-94.5151 784.4282,-84.462 795.0171,-75.0168\"/>\n<polygon fill=\"#000000\" stroke=\"#000000\" points=\"797.4551,-77.5322 802.5878,-68.2637 792.7954,-72.3084 797.4551,-77.5322\"/>\n</g>\n<!-- 13 -->\n<g id=\"node14\" class=\"node\">\n<title>13</title>\n<polygon fill=\"#f4caab\" stroke=\"#000000\" points=\"1066,-68 928,-68 928,0 1066,0 1066,-68\"/>\n<text text-anchor=\"middle\" x=\"997\" y=\"-52.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\" fill=\"#000000\">entropy = 0.948</text>\n<text text-anchor=\"middle\" x=\"997\" y=\"-37.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\" fill=\"#000000\">samples = 303</text>\n<text text-anchor=\"middle\" x=\"997\" y=\"-22.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\" fill=\"#000000\">value = [192, 111]</text>\n<text text-anchor=\"middle\" x=\"997\" y=\"-7.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\" fill=\"#000000\">class = good cretid</text>\n</g>\n<!-- 12&#45;&gt;13 -->\n<g id=\"edge13\" class=\"edge\">\n<title>12&#45;&gt;13</title>\n<path fill=\"none\" stroke=\"#000000\" d=\"M997,-103.9815C997,-95.618 997,-86.7965 997,-78.3409\"/>\n<polygon fill=\"#000000\" stroke=\"#000000\" points=\"1000.5001,-78.2636 997,-68.2637 993.5001,-78.2637 1000.5001,-78.2636\"/>\n</g>\n<!-- 14 -->\n<g id=\"node15\" class=\"node\">\n<title>14</title>\n<polygon fill=\"#eb9f68\" stroke=\"#000000\" points=\"1222,-68 1084,-68 1084,0 1222,0 1222,-68\"/>\n<text text-anchor=\"middle\" x=\"1153\" y=\"-52.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\" fill=\"#000000\">entropy = 0.702</text>\n<text text-anchor=\"middle\" x=\"1153\" y=\"-37.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\" fill=\"#000000\">samples = 147</text>\n<text text-anchor=\"middle\" x=\"1153\" y=\"-22.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\" fill=\"#000000\">value = [119, 28]</text>\n<text text-anchor=\"middle\" x=\"1153\" y=\"-7.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\" fill=\"#000000\">class = good cretid</text>\n</g>\n<!-- 12&#45;&gt;14 -->\n<g id=\"edge14\" class=\"edge\">\n<title>12&#45;&gt;14</title>\n<path fill=\"none\" stroke=\"#000000\" d=\"M1055.0886,-103.9815C1068.7189,-94.2394 1083.2184,-83.8759 1096.7658,-74.193\"/>\n<polygon fill=\"#000000\" stroke=\"#000000\" points=\"1098.9612,-76.926 1105.0616,-68.2637 1094.8908,-71.2311 1098.9612,-76.926\"/>\n</g>\n</g>\n</svg>\n"
          },
          "metadata": {},
          "execution_count": 42
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def avenger_dictionary(dict_): \n",
        "  Accmean_1 = sum(col['Accuracy_pre'] for col in dict_)/ len(dict_)\n",
        "  Accmean_2 = sum(col['Accuracy_post'] for col in dict_)/ len(dict_)\n",
        "  Statmean = sum(col['statistical_parity_difference'] for col in dict_)/ len(dict_)\n",
        "  Dispmean = sum(col['disparate_impact'] for col in dict_)/ len(dict_)\n",
        "  Equalmean = sum(col['equal_opportunity_difference'] for col in dict_)/ len(dict_)\n",
        "  keys = ['Accuracy pre','Accuracy post','statistical_parity_difference', 'disparate_impact','equal_opportunity_difference']\n",
        "  values = [Accmean_1,Accmean_2, Statmean, Dispmean,Equalmean]\n",
        "\n",
        "  dictionary = dict(zip(keys, values))\n",
        "  return dictionary\n",
        "\n",
        "def avenger_dictionary_(dict_): \n",
        "  '''\n",
        "  use for addversarial debiasing and certain trees\n",
        "  '''\n",
        "  Accmean_1 = sum(col['Accuracy'] for col in dict_)/ len(dict_)\n",
        "\n",
        "  Statmean = sum(col['statistical_parity_difference'] for col in dict_)/ len(dict_)\n",
        "  Dispmean = sum(col['disparate_impact'] for col in dict_)/ len(dict_)\n",
        "  Equalmean = sum(col['equal_opportunity_difference'] for col in dict_)/ len(dict_)\n",
        "  keys = ['Accuracy ','statistical_parity_difference', 'disparate_impact','equal_opportunity_difference']\n",
        "  values = [Accmean_1, Statmean, Dispmean,Equalmean]\n",
        "\n",
        "  dictionary = dict(zip(keys, values))\n",
        "  return dictionary\n",
        "\n",
        "def avenger_dictionary_1(dict_): \n",
        "  '''\n",
        "  use for trees\n",
        "  '''\n",
        "  Accmean_1 = sum(col['Accuracy_pre'] for col in dict_)/ len(dict_)\n",
        "\n",
        "  Statmean = sum(col['statistical_parity_difference'] for col in dict_)/ len(dict_)\n",
        "  Dispmean = sum(col['disparate_impact'] for col in dict_)/ len(dict_)\n",
        "  Equalmean = sum(col['equal_opportunity_difference'] for col in dict_)/ len(dict_)\n",
        "  keys = ['Accuracy ','statistical_parity_difference', 'disparate_impact','equal_opportunity_difference']\n",
        "  values = [Accmean_1, Statmean, Dispmean,Equalmean]\n",
        "\n",
        "  dictionary = dict(zip(keys, values))\n",
        "  return dictionary\n",
        "\n",
        "def retrive_metrics_dataframe_garry(results_dict):\n",
        "  '''\n",
        "  This function uses the results_dict dictionary retrived from the task1_vanilla_test function\n",
        "  and returns a dataframe with each metrics with the assistance of the avenger_dictionary function\n",
        "  '''\n",
        "  results_1_entropy = []\n",
        "\n",
        "  results_2_entropy =  []\n",
        "\n",
        "  results_3_entropy =  []\n",
        "\n",
        "  results_4_entropy = []\n",
        "\n",
        "  results_5_entropy = []\n",
        "\n",
        "  for key in results_dict.keys():\n",
        "\n",
        "    if key[1:]=='GerryFairClassifier_1e-05':\n",
        "      results_1_entropy.append(results_dict[key])\n",
        "\n",
        "    if key[1:]=='GerryFairClassifier_0.001':\n",
        "      results_2_entropy.append(results_dict[key])\n",
        "\n",
        "    if key[1:]=='GerryFairClassifier_1':\n",
        "      results_3_entropy.append(results_dict[key])\n",
        "\n",
        "    if key[1:]=='GerryFairClassifier_100':\n",
        "      results_4_entropy.append(results_dict[key])\n",
        "\n",
        "    if key[1:]=='GerryFairClassifier_10000':\n",
        "      results_5_entropy.append(results_dict[key])\n",
        "\n",
        "  results_dataframe = {}\n",
        "  results_dataframe['GerryFairClassifier_1e-05'] = avenger_dictionary(results_1_entropy)\n",
        "  results_dataframe['GerryFairClassifier_0.001'] = avenger_dictionary(results_2_entropy)\n",
        "  results_dataframe['GerryFairClassifier_1'] = avenger_dictionary(results_3_entropy)\n",
        "  results_dataframe['GerryFairClassifier_100'] = avenger_dictionary(results_4_entropy)\n",
        "  results_dataframe['GerryFairClassifier_10000'] = avenger_dictionary(results_5_entropy)\n",
        "  df_ = []\n",
        "  df_ = pd.DataFrame(results_dataframe).T\n",
        "\n",
        "  return df_\n",
        "\n",
        "def retrive_metrics_dataframe(results_dict):\n",
        "  '''\n",
        "  This function uses the results_dict dictionary retrived from the task1_vanilla_test function\n",
        "  and returns a dataframe with each metrics with the assistance of the avenger_dictionary function\n",
        "  '''\n",
        "  results_1_entropy = []\n",
        "\n",
        "  results_2_entropy =  []\n",
        "\n",
        "  results_3_entropy =  []\n",
        "\n",
        "  results_4_entropy = []\n",
        "\n",
        "  results_5_entropy = []\n",
        "\n",
        "  for key in results_dict.keys():\n",
        "\n",
        "    if key[1:]=='GerryFairClassifier_1e-05':\n",
        "      results_1_entropy.append(results_dict[key])\n",
        "\n",
        "    if key[1:]=='GerryFairClassifier_0.001':\n",
        "      results_2_entropy.append(results_dict[key])\n",
        "\n",
        "    if key[1:]=='GerryFairClassifier_1':\n",
        "      results_3_entropy.append(results_dict[key])\n",
        "\n",
        "    if key[1:]=='GerryFairClassifier_100':\n",
        "      results_4_entropy.append(results_dict[key])\n",
        "\n",
        "    if key[1:]=='GerryFairClassifier_10000':\n",
        "      results_5_entropy.append(results_dict[key])\n",
        "\n",
        "  results_dataframe = {}\n",
        "  results_dataframe['GerryFairClassifier_1e-05'] = avenger_dictionary_(results_1_entropy)\n",
        "  results_dataframe['GerryFairClassifier_0.001'] = avenger_dictionary_(results_2_entropy)\n",
        "  results_dataframe['GerryFairClassifier_1'] = avenger_dictionary(results_3_entropy)\n",
        "  results_dataframe['GerryFairClassifier_100'] = avenger_dictionary(results_4_entropy)\n",
        "  results_dataframe['GerryFairClassifier_10000'] = avenger_dictionary(results_5_entropy)\n",
        "  df_ = []\n",
        "  df_ = pd.DataFrame(results_dataframe).T\n",
        "\n",
        "  return df_\n",
        "\n",
        "def retrive_metrics_dataframe_addver(results_dict):\n",
        "  '''\n",
        "  This function uses the results_dict dictionary retrived from the task1_vanilla_test function\n",
        "  and returns a dataframe with each metrics with the assistance of the avenger_dictionary function\n",
        "  '''\n",
        "  results_1_entropy = []\n",
        "\n",
        "  results_2_entropy =  []\n",
        "\n",
        "  results_3_entropy =  []\n",
        "\n",
        "  results_4_entropy = []\n",
        "\n",
        "  results_5_entropy = []\n",
        "\n",
        "  for key in results_dict.keys():\n",
        "\n",
        "    if key[1:]=='AdversarialDebiasing_1e-05':\n",
        "      results_1_entropy.append(results_dict[key])\n",
        "\n",
        "    if key[1:]=='AdversarialDebiasing_0.001':\n",
        "      results_2_entropy.append(results_dict[key])\n",
        "\n",
        "    if key[1:]=='AdversarialDebiasing_1':\n",
        "      results_3_entropy.append(results_dict[key])\n",
        "\n",
        "    if key[1:]=='AdversarialDebiasing_100':\n",
        "      results_4_entropy.append(results_dict[key])\n",
        "\n",
        "    if key[1:]=='AdversarialDebiasing_10000':\n",
        "      results_5_entropy.append(results_dict[key])\n",
        "\n",
        "  results_dataframe = {}\n",
        "  results_dataframe['AdversarialDebiasing_1e-05'] = avenger_dictionary_(results_1_entropy)\n",
        "  results_dataframe['AdversarialDebiasing_0.001'] = avenger_dictionary_(results_2_entropy)\n",
        "  results_dataframe['AdversarialDebiasing_1'] = avenger_dictionary_(results_3_entropy)\n",
        "  results_dataframe['AdversarialDebiasing_100'] = avenger_dictionary_(results_4_entropy)\n",
        "  results_dataframe['AdversarialDebiasing_10000'] = avenger_dictionary_(results_5_entropy)\n",
        "  df_ = []\n",
        "  df_ = pd.DataFrame(results_dataframe).T\n",
        "\n",
        "  return df_\n",
        "\n",
        "def retrive_metrics_dataframeTrees(results_dict):\n",
        "  '''\n",
        "  This function uses the results_dict dictionary retrived from the task1_vanilla_test function\n",
        "  and returns a dataframe with each metrics with the assistance of the avenger_dictionary function\n",
        "  '''\n",
        "  results_2_entropy,results_2_gini = [],[]\n",
        "\n",
        "  results_26_entropy,results_26_gini =  [],[]\n",
        "\n",
        "  results_51_entropy,results_51_gini =  [],[]\n",
        "\n",
        "  results_75_entropy,results_75_gini = [],[]\n",
        "\n",
        "  results_100_entropy,results_100_gini = [],[]\n",
        "\n",
        "  results_None_entropy,results_None_gini = [],[]\n",
        "\n",
        "  for key in results_dict.keys():\n",
        "\n",
        "    if key[1:]=='DecisionTree_entropy2':\n",
        "      results_2_entropy.append(results_dict[key])\n",
        "    if key[1:]=='DecisionTree_gini2':\n",
        "      results_2_gini.append(results_dict[key])\n",
        "\n",
        "    if key[1:]=='DecisionTree_entropy26':\n",
        "      results_26_entropy.append(results_dict[key])\n",
        "    if key[1:]=='DecisionTree_gini26':\n",
        "      results_26_gini.append(results_dict[key])\n",
        "\n",
        "    if key[1:]=='DecisionTree_entropy51':\n",
        "      results_51_entropy.append(results_dict[key])\n",
        "    if key[1:]=='DecisionTree_gini51':\n",
        "      results_51_gini.append(results_dict[key])\n",
        "\n",
        "    if key[1:]=='DecisionTree_entropy100':\n",
        "      results_100_entropy.append(results_dict[key])\n",
        "    if key[1:]=='DecisionTree_gini100':\n",
        "      results_100_gini.append(results_dict[key])\n",
        "\n",
        "    if key[1:]=='DecisionTree_entropyNone':\n",
        "      results_None_entropy.append(results_dict[key])\n",
        "    if key[1:]=='DecisionTree_giniNone':\n",
        "      results_None_gini.append(results_dict[key])\n",
        "\n",
        "  results_dataframe = {}\n",
        "  results_dataframe['DecisionTree_entropy2'] = avenger_dictionary_(results_2_entropy)\n",
        "  results_dataframe['DecisionTree_gini2'] = avenger_dictionary_(results_2_gini)\n",
        "  results_dataframe['DecisionTree_entropy26'] = avenger_dictionary_(results_26_entropy)\n",
        "  results_dataframe['DecisionTree_gini26'] = avenger_dictionary_(results_26_gini)\n",
        "  results_dataframe['DecisionTree_entropy51'] = avenger_dictionary_(results_51_entropy)\n",
        "  results_dataframe['DecisionTree_gini51'] = avenger_dictionary_(results_51_gini)\n",
        "  results_dataframe['DecisionTree_entropy100'] = avenger_dictionary_(results_100_entropy)\n",
        "  results_dataframe['DecisionTree_gini100'] = avenger_dictionary_(results_100_gini)\n",
        "  results_dataframe['DecisionTree_entropyNone'] = avenger_dictionary_(results_None_entropy)\n",
        "  results_dataframe['DecisionTree_giniNone'] = avenger_dictionary_(results_None_gini)\n",
        "  df_ = []\n",
        "  df_ = pd.DataFrame(results_dataframe).T\n",
        "\n",
        "  return df_\n",
        "\n",
        "def retrive_metrics_dataframeTrees_1(results_dict):\n",
        "  '''\n",
        "  This function uses the results_dict dictionary retrived from the task1_vanilla_test function\n",
        "  and returns a dataframe with each metrics with the assistance of the avenger_dictionary function\n",
        "  '''\n",
        "  results_2_entropy,results_2_gini = [],[]\n",
        "\n",
        "  results_26_entropy,results_26_gini =  [],[]\n",
        "\n",
        "  results_51_entropy,results_51_gini =  [],[]\n",
        "\n",
        "  results_75_entropy,results_75_gini = [],[]\n",
        "\n",
        "  results_100_entropy,results_100_gini = [],[]\n",
        "\n",
        "  results_None_entropy,results_None_gini = [],[]\n",
        "\n",
        "  for key in results_dict.keys():\n",
        "\n",
        "    if key[1:]=='DecisionTree_entropy2':\n",
        "      results_2_entropy.append(results_dict[key])\n",
        "    if key[1:]=='DecisionTree_gini2':\n",
        "      results_2_gini.append(results_dict[key])\n",
        "\n",
        "    if key[1:]=='DecisionTree_entropy26':\n",
        "      results_26_entropy.append(results_dict[key])\n",
        "    if key[1:]=='DecisionTree_gini26':\n",
        "      results_26_gini.append(results_dict[key])\n",
        "\n",
        "    if key[1:]=='DecisionTree_entropy51':\n",
        "      results_51_entropy.append(results_dict[key])\n",
        "    if key[1:]=='DecisionTree_gini51':\n",
        "      results_51_gini.append(results_dict[key])\n",
        "\n",
        "    if key[1:]=='DecisionTree_entropy100':\n",
        "      results_100_entropy.append(results_dict[key])\n",
        "    if key[1:]=='DecisionTree_gini100':\n",
        "      results_100_gini.append(results_dict[key])\n",
        "\n",
        "    if key[1:]=='DecisionTree_entropyNone':\n",
        "      results_None_entropy.append(results_dict[key])\n",
        "    if key[1:]=='DecisionTree_giniNone':\n",
        "      results_None_gini.append(results_dict[key])\n",
        "\n",
        "  results_dataframe = {}\n",
        "  results_dataframe['DecisionTree_entropy2'] = avenger_dictionary_1(results_2_entropy)\n",
        "  results_dataframe['DecisionTree_gini2'] = avenger_dictionary_1(results_2_gini)\n",
        "  results_dataframe['DecisionTree_entropy26'] = avenger_dictionary_1(results_26_entropy)\n",
        "  results_dataframe['DecisionTree_gini26'] = avenger_dictionary_1(results_26_gini)\n",
        "  results_dataframe['DecisionTree_entropy51'] = avenger_dictionary_1(results_51_entropy)\n",
        "  results_dataframe['DecisionTree_gini51'] = avenger_dictionary_1(results_51_gini)\n",
        "  results_dataframe['DecisionTree_entropy100'] = avenger_dictionary_1(results_100_entropy)\n",
        "  results_dataframe['DecisionTree_gini100'] = avenger_dictionary_1(results_100_gini)\n",
        "  results_dataframe['DecisionTree_entropyNone'] = avenger_dictionary_1(results_None_entropy)\n",
        "  results_dataframe['DecisionTree_giniNone'] = avenger_dictionary_1(results_None_gini)\n",
        "  df_ = []\n",
        "  df_ = pd.DataFrame(results_dataframe).T\n",
        "\n",
        "  return df_\n",
        "\n",
        "\n",
        "def retrive_metrics_dataframe_tree_repair(results_dict):\n",
        "  '''\n",
        "  This function uses the results_dict dictionary retrived from the task1_vanilla_test function\n",
        "  and returns a dataframe with each metrics with the assistance of the avenger_dictionary function\n",
        "  '''\n",
        "  results_1_entropy = []\n",
        "\n",
        "  results_2_entropy =  []\n",
        "\n",
        "  results_3_entropy =  []\n",
        "\n",
        "  results_4_entropy = []\n",
        "\n",
        "  results_5_entropy = []\n",
        "\n",
        "  for key in results_dict.keys():\n",
        "\n",
        "    if key[1:]=='DecisionTree_0.1':\n",
        "      results_1_entropy.append(results_dict[key])\n",
        "\n",
        "    if key[1:]=='DecisionTree_0.3':\n",
        "      results_2_entropy.append(results_dict[key])\n",
        "\n",
        "    if key[1:]=='DecisionTree_0.5':\n",
        "      results_3_entropy.append(results_dict[key])\n",
        "\n",
        "    if key[1:]=='DecisionTree_0.7':\n",
        "      results_4_entropy.append(results_dict[key])\n",
        "\n",
        "    if key[1:]=='DecisionTree_1':\n",
        "      results_5_entropy.append(results_dict[key])\n",
        "\n",
        "  results_dataframe = {}\n",
        "  results_dataframe['DecisionTree_0.1'] = avenger_dictionary(results_1_entropy)\n",
        "  results_dataframe['DecisionTree_0.3'] = avenger_dictionary(results_2_entropy)\n",
        "  results_dataframe['DecisionTree_0.5'] = avenger_dictionary(results_3_entropy)\n",
        "  results_dataframe['DecisionTree_0.1'] = avenger_dictionary(results_4_entropy)\n",
        "  results_dataframe['DecisionTree_1'] = avenger_dictionary(results_5_entropy)\n",
        "  df_ = []\n",
        "  df_ = pd.DataFrame(results_dataframe).T\n",
        "\n",
        "  return df_"
      ],
      "metadata": {
        "id": "YhkKTIF_OAI-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!ls '/content/drive/MyDrive/UniSussex/ML/'"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NMjbqBlcIQ4q",
        "outputId": "758a81a5-f179-4493-a010-060f1e59e4de"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "AdversarialDebiasing_job_.pickle\n",
            "AdversarialDebiasing_relationshipfeature_.pickle\n",
            "DecisionTree_jobfeature_weighted_.pickle\n",
            "DecisionTree_nosensitivefeatureAdult_.pickle\n",
            "DecisionTree_nosensitivefeature_postprocessingGerman_.pickle\n",
            "DecisionTree_relationshipfeature_weighted_.pickle\n",
            "results_occupation_Tree_Extra_postprocessing_GerryFair_.pickle\n",
            "results_occupation_Tree_Extra_prepostprocessing.pickle\n",
            "SplitsExtra_AdversarialDebiasingClassifier_adult_.pickle\n",
            "SplitsExtra_GerryFairClassifier_adult_.pickle\n",
            "Submit_ML_Coursework_Final_Part2_final.ipynb\n",
            "Submit_ML_Coursework_Final_Part2.ipynb\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle\n",
        "file = open(\"/content/drive/MyDrive/UniSussex/ML/AdversarialDebiasing_job_.pickle\",'rb')\n",
        "object_file = pickle.load(file)\n",
        "file.close()\n",
        "df_deb = retrive_metrics_dataframe_addver(object_file)\n",
        "df_deb"
      ],
      "metadata": {
        "id": "t1eKYteoYqN1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "outputId": "c96dcc33-7c9b-4523-9c86-03112d8295e7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                            Accuracy   statistical_parity_difference  \\\n",
              "AdversarialDebiasing_1e-05      0.692                       0.003251   \n",
              "AdversarialDebiasing_0.001      0.577                       0.020737   \n",
              "AdversarialDebiasing_1          0.638                       0.011106   \n",
              "AdversarialDebiasing_100        0.527                      -0.003188   \n",
              "AdversarialDebiasing_10000      0.300                       0.000000   \n",
              "\n",
              "                            disparate_impact  equal_opportunity_difference  \n",
              "AdversarialDebiasing_1e-05          1.005316                      0.013857  \n",
              "AdversarialDebiasing_0.001          1.173595                      0.022503  \n",
              "AdversarialDebiasing_1              1.046815                      0.012588  \n",
              "AdversarialDebiasing_100            0.596282                     -0.013271  \n",
              "AdversarialDebiasing_10000          0.000000                      0.000000  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-67b1e97a-146e-4e5b-a37c-b5b2e08a2111\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Accuracy</th>\n",
              "      <th>statistical_parity_difference</th>\n",
              "      <th>disparate_impact</th>\n",
              "      <th>equal_opportunity_difference</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>AdversarialDebiasing_1e-05</th>\n",
              "      <td>0.692</td>\n",
              "      <td>0.003251</td>\n",
              "      <td>1.005316</td>\n",
              "      <td>0.013857</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>AdversarialDebiasing_0.001</th>\n",
              "      <td>0.577</td>\n",
              "      <td>0.020737</td>\n",
              "      <td>1.173595</td>\n",
              "      <td>0.022503</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>AdversarialDebiasing_1</th>\n",
              "      <td>0.638</td>\n",
              "      <td>0.011106</td>\n",
              "      <td>1.046815</td>\n",
              "      <td>0.012588</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>AdversarialDebiasing_100</th>\n",
              "      <td>0.527</td>\n",
              "      <td>-0.003188</td>\n",
              "      <td>0.596282</td>\n",
              "      <td>-0.013271</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>AdversarialDebiasing_10000</th>\n",
              "      <td>0.300</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-67b1e97a-146e-4e5b-a37c-b5b2e08a2111')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-67b1e97a-146e-4e5b-a37c-b5b2e08a2111 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-67b1e97a-146e-4e5b-a37c-b5b2e08a2111');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 69
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "file = open(\"/content/drive/MyDrive/UniSussex/ML/AdversarialDebiasing_relationshipfeature_.pickle\",'rb')\n",
        "object_file = pickle.load(file)\n",
        "file.close()\n",
        "df_deb = retrive_metrics_dataframe_addver(object_file)\n",
        "df_deb"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "id": "2JYLeqxkIlAY",
        "outputId": "c09866aa-13b6-4166-a098-c0945f195ed0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                            Accuracy   statistical_parity_difference  \\\n",
              "AdversarialDebiasing_1e-05   0.826589                       0.083379   \n",
              "AdversarialDebiasing_0.001   0.823626                       0.070651   \n",
              "AdversarialDebiasing_1       0.784530                      -0.014572   \n",
              "AdversarialDebiasing_100     0.741254                       0.000499   \n",
              "AdversarialDebiasing_10000   0.752156                       0.000000   \n",
              "\n",
              "                            disparate_impact  equal_opportunity_difference  \n",
              "AdversarialDebiasing_1e-05          1.906274                     -0.041674  \n",
              "AdversarialDebiasing_0.001          1.796369                     -0.069492  \n",
              "AdversarialDebiasing_1              0.891330                     -0.308977  \n",
              "AdversarialDebiasing_100            0.455607                     -0.056658  \n",
              "AdversarialDebiasing_10000          0.000000                      0.000000  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-e156278b-6700-403e-a340-c3bbd3221adf\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Accuracy</th>\n",
              "      <th>statistical_parity_difference</th>\n",
              "      <th>disparate_impact</th>\n",
              "      <th>equal_opportunity_difference</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>AdversarialDebiasing_1e-05</th>\n",
              "      <td>0.826589</td>\n",
              "      <td>0.083379</td>\n",
              "      <td>1.906274</td>\n",
              "      <td>-0.041674</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>AdversarialDebiasing_0.001</th>\n",
              "      <td>0.823626</td>\n",
              "      <td>0.070651</td>\n",
              "      <td>1.796369</td>\n",
              "      <td>-0.069492</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>AdversarialDebiasing_1</th>\n",
              "      <td>0.784530</td>\n",
              "      <td>-0.014572</td>\n",
              "      <td>0.891330</td>\n",
              "      <td>-0.308977</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>AdversarialDebiasing_100</th>\n",
              "      <td>0.741254</td>\n",
              "      <td>0.000499</td>\n",
              "      <td>0.455607</td>\n",
              "      <td>-0.056658</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>AdversarialDebiasing_10000</th>\n",
              "      <td>0.752156</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-e156278b-6700-403e-a340-c3bbd3221adf')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-e156278b-6700-403e-a340-c3bbd3221adf button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-e156278b-6700-403e-a340-c3bbd3221adf');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 70
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "file = open(\"/content/drive/MyDrive/UniSussex/ML/DecisionTree_jobfeature_weighted_.pickle\",'rb')\n",
        "object_file = pickle.load(file)\n",
        "file.close()\n",
        "df_tree = retrive_metrics_dataframeTrees(object_file)\n",
        "df_tree"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 359
        },
        "id": "DPtOs9INKap2",
        "outputId": "1c0eaeb9-d712-4bea-c492-2810e96492c7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                          Accuracy   statistical_parity_difference  \\\n",
              "DecisionTree_entropy2         0.695                       0.008627   \n",
              "DecisionTree_gini2            0.704                       0.011333   \n",
              "DecisionTree_entropy26        0.671                      -0.022072   \n",
              "DecisionTree_gini26           0.677                      -0.025540   \n",
              "DecisionTree_entropy51        0.669                      -0.007307   \n",
              "DecisionTree_gini51           0.685                      -0.007901   \n",
              "DecisionTree_entropy100       0.671                      -0.011405   \n",
              "DecisionTree_gini100          0.681                      -0.028952   \n",
              "DecisionTree_entropyNone      0.670                      -0.014184   \n",
              "DecisionTree_giniNone         0.672                      -0.018880   \n",
              "\n",
              "                          disparate_impact  equal_opportunity_difference  \n",
              "DecisionTree_entropy2             1.009525                      0.003497  \n",
              "DecisionTree_gini2                1.013396                      0.020259  \n",
              "DecisionTree_entropy26            0.969025                     -0.058900  \n",
              "DecisionTree_gini26               0.964512                     -0.070042  \n",
              "DecisionTree_entropy51            0.990641                     -0.039502  \n",
              "DecisionTree_gini51               0.993023                     -0.044106  \n",
              "DecisionTree_entropy100           0.984019                     -0.037145  \n",
              "DecisionTree_gini100              0.960204                     -0.047667  \n",
              "DecisionTree_entropyNone          0.980365                     -0.037811  \n",
              "DecisionTree_giniNone             0.976012                     -0.051399  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-acfda9ae-d873-4aa2-a38e-beabb849c71b\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Accuracy</th>\n",
              "      <th>statistical_parity_difference</th>\n",
              "      <th>disparate_impact</th>\n",
              "      <th>equal_opportunity_difference</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>DecisionTree_entropy2</th>\n",
              "      <td>0.695</td>\n",
              "      <td>0.008627</td>\n",
              "      <td>1.009525</td>\n",
              "      <td>0.003497</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>DecisionTree_gini2</th>\n",
              "      <td>0.704</td>\n",
              "      <td>0.011333</td>\n",
              "      <td>1.013396</td>\n",
              "      <td>0.020259</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>DecisionTree_entropy26</th>\n",
              "      <td>0.671</td>\n",
              "      <td>-0.022072</td>\n",
              "      <td>0.969025</td>\n",
              "      <td>-0.058900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>DecisionTree_gini26</th>\n",
              "      <td>0.677</td>\n",
              "      <td>-0.025540</td>\n",
              "      <td>0.964512</td>\n",
              "      <td>-0.070042</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>DecisionTree_entropy51</th>\n",
              "      <td>0.669</td>\n",
              "      <td>-0.007307</td>\n",
              "      <td>0.990641</td>\n",
              "      <td>-0.039502</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>DecisionTree_gini51</th>\n",
              "      <td>0.685</td>\n",
              "      <td>-0.007901</td>\n",
              "      <td>0.993023</td>\n",
              "      <td>-0.044106</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>DecisionTree_entropy100</th>\n",
              "      <td>0.671</td>\n",
              "      <td>-0.011405</td>\n",
              "      <td>0.984019</td>\n",
              "      <td>-0.037145</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>DecisionTree_gini100</th>\n",
              "      <td>0.681</td>\n",
              "      <td>-0.028952</td>\n",
              "      <td>0.960204</td>\n",
              "      <td>-0.047667</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>DecisionTree_entropyNone</th>\n",
              "      <td>0.670</td>\n",
              "      <td>-0.014184</td>\n",
              "      <td>0.980365</td>\n",
              "      <td>-0.037811</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>DecisionTree_giniNone</th>\n",
              "      <td>0.672</td>\n",
              "      <td>-0.018880</td>\n",
              "      <td>0.976012</td>\n",
              "      <td>-0.051399</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-acfda9ae-d873-4aa2-a38e-beabb849c71b')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-acfda9ae-d873-4aa2-a38e-beabb849c71b button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-acfda9ae-d873-4aa2-a38e-beabb849c71b');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 80
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "file = open(\"/content/drive/MyDrive/UniSussex/ML/DecisionTree_nosensitivefeatureAdult_.pickle\",'rb')\n",
        "object_file = pickle.load(file)\n",
        "file.close()\n",
        "df_tree = retrive_metrics_dataframeTrees_1(object_file)\n",
        "df_tree"
      ],
      "metadata": {
        "id": "lk71JE94jrdp",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 359
        },
        "outputId": "6760406a-2129-4662-91ae-92e4a6a41275"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                          Accuracy   statistical_parity_difference  \\\n",
              "DecisionTree_entropy2      0.786803                      -0.108048   \n",
              "DecisionTree_gini2         0.786803                      -0.108048   \n",
              "DecisionTree_entropy26     0.786393                      -0.095991   \n",
              "DecisionTree_gini26        0.786393                      -0.095991   \n",
              "DecisionTree_entropy51     0.786393                      -0.095991   \n",
              "DecisionTree_gini51        0.786393                      -0.095991   \n",
              "DecisionTree_entropy100    0.786393                      -0.095991   \n",
              "DecisionTree_gini100       0.786393                      -0.095991   \n",
              "DecisionTree_entropyNone   0.786393                      -0.095991   \n",
              "DecisionTree_giniNone      0.786393                      -0.095991   \n",
              "\n",
              "                          disparate_impact  equal_opportunity_difference  \n",
              "DecisionTree_entropy2             0.525767                     -0.008338  \n",
              "DecisionTree_gini2                0.525767                     -0.008338  \n",
              "DecisionTree_entropy26            0.533800                      0.012991  \n",
              "DecisionTree_gini26               0.533800                      0.012991  \n",
              "DecisionTree_entropy51            0.533800                      0.012991  \n",
              "DecisionTree_gini51               0.533800                      0.012991  \n",
              "DecisionTree_entropy100           0.533800                      0.012991  \n",
              "DecisionTree_gini100              0.533800                      0.012991  \n",
              "DecisionTree_entropyNone          0.533800                      0.012991  \n",
              "DecisionTree_giniNone             0.533800                      0.012991  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-9a7e9d20-741b-408d-b7a0-e142c5e8a5a8\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Accuracy</th>\n",
              "      <th>statistical_parity_difference</th>\n",
              "      <th>disparate_impact</th>\n",
              "      <th>equal_opportunity_difference</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>DecisionTree_entropy2</th>\n",
              "      <td>0.786803</td>\n",
              "      <td>-0.108048</td>\n",
              "      <td>0.525767</td>\n",
              "      <td>-0.008338</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>DecisionTree_gini2</th>\n",
              "      <td>0.786803</td>\n",
              "      <td>-0.108048</td>\n",
              "      <td>0.525767</td>\n",
              "      <td>-0.008338</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>DecisionTree_entropy26</th>\n",
              "      <td>0.786393</td>\n",
              "      <td>-0.095991</td>\n",
              "      <td>0.533800</td>\n",
              "      <td>0.012991</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>DecisionTree_gini26</th>\n",
              "      <td>0.786393</td>\n",
              "      <td>-0.095991</td>\n",
              "      <td>0.533800</td>\n",
              "      <td>0.012991</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>DecisionTree_entropy51</th>\n",
              "      <td>0.786393</td>\n",
              "      <td>-0.095991</td>\n",
              "      <td>0.533800</td>\n",
              "      <td>0.012991</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>DecisionTree_gini51</th>\n",
              "      <td>0.786393</td>\n",
              "      <td>-0.095991</td>\n",
              "      <td>0.533800</td>\n",
              "      <td>0.012991</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>DecisionTree_entropy100</th>\n",
              "      <td>0.786393</td>\n",
              "      <td>-0.095991</td>\n",
              "      <td>0.533800</td>\n",
              "      <td>0.012991</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>DecisionTree_gini100</th>\n",
              "      <td>0.786393</td>\n",
              "      <td>-0.095991</td>\n",
              "      <td>0.533800</td>\n",
              "      <td>0.012991</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>DecisionTree_entropyNone</th>\n",
              "      <td>0.786393</td>\n",
              "      <td>-0.095991</td>\n",
              "      <td>0.533800</td>\n",
              "      <td>0.012991</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>DecisionTree_giniNone</th>\n",
              "      <td>0.786393</td>\n",
              "      <td>-0.095991</td>\n",
              "      <td>0.533800</td>\n",
              "      <td>0.012991</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-9a7e9d20-741b-408d-b7a0-e142c5e8a5a8')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-9a7e9d20-741b-408d-b7a0-e142c5e8a5a8 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-9a7e9d20-741b-408d-b7a0-e142c5e8a5a8');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 84
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "file = open(\"/content/drive/MyDrive/UniSussex/ML/DecisionTree_relationshipfeature_weighted_.pickle\",'rb')\n",
        "object_file = pickle.load(file)\n",
        "file.close()\n",
        "df_tree = retrive_metrics_dataframeTrees(object_file)\n",
        "df_tree"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 359
        },
        "id": "lXTTuM0IL_nl",
        "outputId": "8e8138de-479b-468f-b112-c8a4b77a2972"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                          Accuracy   statistical_parity_difference  \\\n",
              "DecisionTree_entropy2      0.794370                       0.020106   \n",
              "DecisionTree_gini2         0.795564                       0.022562   \n",
              "DecisionTree_entropy26     0.819070                       0.092818   \n",
              "DecisionTree_gini26        0.817235                       0.103432   \n",
              "DecisionTree_entropy51     0.811729                       0.146129   \n",
              "DecisionTree_gini51        0.811397                       0.147548   \n",
              "DecisionTree_entropy100    0.813387                       0.149752   \n",
              "DecisionTree_gini100       0.811309                       0.148620   \n",
              "DecisionTree_entropyNone   0.811972                       0.146512   \n",
              "DecisionTree_giniNone      0.811198                       0.147584   \n",
              "\n",
              "                          disparate_impact  equal_opportunity_difference  \n",
              "DecisionTree_entropy2             1.711175                     -0.106171  \n",
              "DecisionTree_gini2                1.728922                     -0.094952  \n",
              "DecisionTree_entropy26            1.745245                     -0.001946  \n",
              "DecisionTree_gini26               1.824582                      0.027051  \n",
              "DecisionTree_entropy51            2.173655                      0.087468  \n",
              "DecisionTree_gini51               2.196868                      0.096776  \n",
              "DecisionTree_entropy100           2.224871                      0.086096  \n",
              "DecisionTree_gini100              2.220757                      0.101884  \n",
              "DecisionTree_entropyNone          2.164470                      0.083092  \n",
              "DecisionTree_giniNone             2.196917                      0.098136  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-f6b69103-6245-40d2-a293-3df529745817\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Accuracy</th>\n",
              "      <th>statistical_parity_difference</th>\n",
              "      <th>disparate_impact</th>\n",
              "      <th>equal_opportunity_difference</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>DecisionTree_entropy2</th>\n",
              "      <td>0.794370</td>\n",
              "      <td>0.020106</td>\n",
              "      <td>1.711175</td>\n",
              "      <td>-0.106171</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>DecisionTree_gini2</th>\n",
              "      <td>0.795564</td>\n",
              "      <td>0.022562</td>\n",
              "      <td>1.728922</td>\n",
              "      <td>-0.094952</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>DecisionTree_entropy26</th>\n",
              "      <td>0.819070</td>\n",
              "      <td>0.092818</td>\n",
              "      <td>1.745245</td>\n",
              "      <td>-0.001946</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>DecisionTree_gini26</th>\n",
              "      <td>0.817235</td>\n",
              "      <td>0.103432</td>\n",
              "      <td>1.824582</td>\n",
              "      <td>0.027051</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>DecisionTree_entropy51</th>\n",
              "      <td>0.811729</td>\n",
              "      <td>0.146129</td>\n",
              "      <td>2.173655</td>\n",
              "      <td>0.087468</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>DecisionTree_gini51</th>\n",
              "      <td>0.811397</td>\n",
              "      <td>0.147548</td>\n",
              "      <td>2.196868</td>\n",
              "      <td>0.096776</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>DecisionTree_entropy100</th>\n",
              "      <td>0.813387</td>\n",
              "      <td>0.149752</td>\n",
              "      <td>2.224871</td>\n",
              "      <td>0.086096</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>DecisionTree_gini100</th>\n",
              "      <td>0.811309</td>\n",
              "      <td>0.148620</td>\n",
              "      <td>2.220757</td>\n",
              "      <td>0.101884</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>DecisionTree_entropyNone</th>\n",
              "      <td>0.811972</td>\n",
              "      <td>0.146512</td>\n",
              "      <td>2.164470</td>\n",
              "      <td>0.083092</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>DecisionTree_giniNone</th>\n",
              "      <td>0.811198</td>\n",
              "      <td>0.147584</td>\n",
              "      <td>2.196917</td>\n",
              "      <td>0.098136</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-f6b69103-6245-40d2-a293-3df529745817')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-f6b69103-6245-40d2-a293-3df529745817 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-f6b69103-6245-40d2-a293-3df529745817');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 87
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "file = open(\"/content/drive/MyDrive/UniSussex/ML/results_occupation_Tree_Extra_postprocessing_GerryFair_.pickle\",'rb')\n",
        "object_file = pickle.load(file)\n",
        "file.close()\n",
        "df_garry = retrive_metrics_dataframe(object_file)\n",
        "df_garry"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "id": "DW6oRh2XMGAI",
        "outputId": "ce01629e-a048-493f-dc08-546c4a3fc1d5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                           Accuracy pre  Accuracy post  \\\n",
              "GerryFairClassifier_1e-05      0.802627       0.624938   \n",
              "GerryFairClassifier_0.001      0.802627       0.624938   \n",
              "GerryFairClassifier_1          0.717190       0.731551   \n",
              "GerryFairClassifier_100        0.240019       0.622481   \n",
              "GerryFairClassifier_10000      0.240019       0.622481   \n",
              "\n",
              "                           statistical_parity_difference  disparate_impact  \\\n",
              "GerryFairClassifier_1e-05                      -0.001496          0.997007   \n",
              "GerryFairClassifier_0.001                      -0.001496          0.997007   \n",
              "GerryFairClassifier_1                          -0.061032          0.823054   \n",
              "GerryFairClassifier_100                         0.001446          1.002892   \n",
              "GerryFairClassifier_10000                       0.001446          1.002892   \n",
              "\n",
              "                           equal_opportunity_difference  \n",
              "GerryFairClassifier_1e-05                      0.013848  \n",
              "GerryFairClassifier_0.001                      0.013848  \n",
              "GerryFairClassifier_1                          0.019144  \n",
              "GerryFairClassifier_100                        0.034528  \n",
              "GerryFairClassifier_10000                      0.034528  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-2f967208-2d4f-4565-91c1-dde584660d00\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Accuracy pre</th>\n",
              "      <th>Accuracy post</th>\n",
              "      <th>statistical_parity_difference</th>\n",
              "      <th>disparate_impact</th>\n",
              "      <th>equal_opportunity_difference</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>GerryFairClassifier_1e-05</th>\n",
              "      <td>0.802627</td>\n",
              "      <td>0.624938</td>\n",
              "      <td>-0.001496</td>\n",
              "      <td>0.997007</td>\n",
              "      <td>0.013848</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>GerryFairClassifier_0.001</th>\n",
              "      <td>0.802627</td>\n",
              "      <td>0.624938</td>\n",
              "      <td>-0.001496</td>\n",
              "      <td>0.997007</td>\n",
              "      <td>0.013848</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>GerryFairClassifier_1</th>\n",
              "      <td>0.717190</td>\n",
              "      <td>0.731551</td>\n",
              "      <td>-0.061032</td>\n",
              "      <td>0.823054</td>\n",
              "      <td>0.019144</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>GerryFairClassifier_100</th>\n",
              "      <td>0.240019</td>\n",
              "      <td>0.622481</td>\n",
              "      <td>0.001446</td>\n",
              "      <td>1.002892</td>\n",
              "      <td>0.034528</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>GerryFairClassifier_10000</th>\n",
              "      <td>0.240019</td>\n",
              "      <td>0.622481</td>\n",
              "      <td>0.001446</td>\n",
              "      <td>1.002892</td>\n",
              "      <td>0.034528</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-2f967208-2d4f-4565-91c1-dde584660d00')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-2f967208-2d4f-4565-91c1-dde584660d00 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-2f967208-2d4f-4565-91c1-dde584660d00');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 89
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "file = open(\"/content/drive/MyDrive/UniSussex/ML/results_occupation_Tree_Extra_prepostprocessing.pickle\",'rb')\n",
        "object_file = pickle.load(file)\n",
        "file.close()\n",
        "df_garry =  retrive_metrics_dataframe_tree_repair(object_file)\n",
        "df_garry"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 173
        },
        "id": "wjJQWHWeMZh2",
        "outputId": "09a28107-11bf-460b-d805-b038f335ff13"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                  Accuracy pre  Accuracy post  statistical_parity_difference  \\\n",
              "DecisionTree_0.1      0.804586       0.622803                      -0.000572   \n",
              "DecisionTree_0.3      0.804586       0.622803                      -0.000572   \n",
              "DecisionTree_0.5      0.804586       0.622803                      -0.000572   \n",
              "DecisionTree_1        0.804586       0.622803                      -0.000572   \n",
              "\n",
              "                  disparate_impact  equal_opportunity_difference  \n",
              "DecisionTree_0.1          0.998856                      0.028104  \n",
              "DecisionTree_0.3          0.998856                      0.028104  \n",
              "DecisionTree_0.5          0.998856                      0.028104  \n",
              "DecisionTree_1            0.998856                      0.028104  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-130ce9d0-8a17-4d3b-b846-d4357a5e0576\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Accuracy pre</th>\n",
              "      <th>Accuracy post</th>\n",
              "      <th>statistical_parity_difference</th>\n",
              "      <th>disparate_impact</th>\n",
              "      <th>equal_opportunity_difference</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>DecisionTree_0.1</th>\n",
              "      <td>0.804586</td>\n",
              "      <td>0.622803</td>\n",
              "      <td>-0.000572</td>\n",
              "      <td>0.998856</td>\n",
              "      <td>0.028104</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>DecisionTree_0.3</th>\n",
              "      <td>0.804586</td>\n",
              "      <td>0.622803</td>\n",
              "      <td>-0.000572</td>\n",
              "      <td>0.998856</td>\n",
              "      <td>0.028104</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>DecisionTree_0.5</th>\n",
              "      <td>0.804586</td>\n",
              "      <td>0.622803</td>\n",
              "      <td>-0.000572</td>\n",
              "      <td>0.998856</td>\n",
              "      <td>0.028104</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>DecisionTree_1</th>\n",
              "      <td>0.804586</td>\n",
              "      <td>0.622803</td>\n",
              "      <td>-0.000572</td>\n",
              "      <td>0.998856</td>\n",
              "      <td>0.028104</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-130ce9d0-8a17-4d3b-b846-d4357a5e0576')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-130ce9d0-8a17-4d3b-b846-d4357a5e0576 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-130ce9d0-8a17-4d3b-b846-d4357a5e0576');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 106
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def retrive_metrics_dataframe_addver_(results_dict):\n",
        "  '''\n",
        "  This function uses the results_dict dictionary retrived from the task1_vanilla_test function\n",
        "  and returns a dataframe with each metrics with the assistance of the avenger_dictionary function\n",
        "  '''\n",
        "  results_1_entropy = []\n",
        "\n",
        "  results_2_entropy =  []\n",
        "\n",
        "  results_3_entropy =  []\n",
        "\n",
        "  results_4_entropy = []\n",
        "\n",
        "  results_5_entropy = []\n",
        "\n",
        "  for key in results_dict.keys():\n",
        "\n",
        "    if key[1:]=='AdversarialDebiasing_1e-05':\n",
        "      results_1_entropy.append(results_dict[key])\n",
        "\n",
        "    if key[1:]=='AdversarialDebiasing_0.001':\n",
        "      results_2_entropy.append(results_dict[key])\n",
        "\n",
        "    if key[1:]=='AdversarialDebiasing_1':\n",
        "      results_3_entropy.append(results_dict[key])\n",
        "\n",
        "    if key[1:]=='AdversarialDebiasing_100':\n",
        "      results_4_entropy.append(results_dict[key])\n",
        "\n",
        "    if key[1:]=='AdversarialDebiasing_10000':\n",
        "      results_5_entropy.append(results_dict[key])\n",
        "\n",
        "  results_dataframe = {}\n",
        "  results_dataframe['AdversarialDebiasing_1e-05'] = avenger_dictionary(results_1_entropy)\n",
        "  results_dataframe['AdversarialDebiasing_0.001'] = avenger_dictionary(results_2_entropy)\n",
        "  results_dataframe['AdversarialDebiasing_1'] = avenger_dictionary(results_3_entropy)\n",
        "  results_dataframe['AdversarialDebiasing_100'] = avenger_dictionary(results_4_entropy)\n",
        "  results_dataframe['AdversarialDebiasing_10000'] = avenger_dictionary(results_5_entropy)\n",
        "  df_ = []\n",
        "  df_ = pd.DataFrame(results_dataframe).T\n",
        "\n",
        "  return df_\n",
        "\n",
        "\n",
        "def retrive_metrics_dataframe_garr_(results_dict):\n",
        "  '''\n",
        "  This function uses the results_dict dictionary retrived from the task1_vanilla_test function\n",
        "  and returns a dataframe with each metrics with the assistance of the avenger_dictionary function\n",
        "  '''\n",
        "  results_1_entropy = []\n",
        "\n",
        "  results_2_entropy =  []\n",
        "\n",
        "  results_3_entropy =  []\n",
        "\n",
        "  results_4_entropy = []\n",
        "\n",
        "  results_5_entropy = []\n",
        "\n",
        "  for key in results_dict.keys():\n",
        " \n",
        "    if key[1:]=='GerryFairClassifier_1e-05':\n",
        "      results_1_entropy.append(results_dict[key])\n",
        "\n",
        "    if key[1:]=='GerryFairClassifier_0.001':\n",
        "      results_2_entropy.append(results_dict[key])\n",
        "\n",
        "    if key[1:]=='GerryFairClassifier_1':\n",
        "      results_3_entropy.append(results_dict[key])\n",
        "\n",
        "    if key[1:]=='GerryFairClassifier_100':\n",
        "      results_4_entropy.append(results_dict[key])\n",
        "\n",
        "    if key[1:]=='GerryFairClassifier_10000':\n",
        "      results_5_entropy.append(results_dict[key])\n",
        "\n",
        "  results_dataframe = {}\n",
        "  results_dataframe['GerryFairClassifier_1e-05'] = avenger_dictionary(results_1_entropy)\n",
        "  results_dataframe['GerryFairClassifier_0.001'] = avenger_dictionary(results_2_entropy)\n",
        "  results_dataframe['GerryFairClassifier_1'] = avenger_dictionary(results_3_entropy)\n",
        "  results_dataframe['GerryFairClassifier_100'] = avenger_dictionary(results_4_entropy)\n",
        "  results_dataframe['GerryFairClassifier_10000'] = avenger_dictionary(results_5_entropy)\n",
        "  df_ = []\n",
        "  df_ = pd.DataFrame(results_dataframe).T\n",
        "\n",
        "  return df_"
      ],
      "metadata": {
        "id": "6yBNrvWDQUAM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "file = open(\"/content/drive/MyDrive/UniSussex/ML/SplitsExtra_AdversarialDebiasingClassifier_adult_.pickle\",'rb')\n",
        "object_file = pickle.load(file)\n",
        "file.close()\n",
        "#retriving the avarage of each split\n",
        "diccit_1 = {}\n",
        "diccit_2 = {}\n",
        "diccit_3 = {}\n",
        "diccit_4 = {}\n",
        "diccit_5 = {}\n",
        "\n",
        "for key in object_file:\n",
        "  df_garry =  retrive_metrics_dataframe_addver_(object_file[key])\n",
        "  diccit_1[key] = [df_garry.loc['AdversarialDebiasing_1e-05'][0],df_garry.loc['AdversarialDebiasing_1e-05'][4]]\n",
        "  diccit_2[key] = [df_garry.loc['AdversarialDebiasing_0.001'][0],df_garry.loc['AdversarialDebiasing_0.001'][4]]\n",
        "  diccit_3[key] = [df_garry.loc['AdversarialDebiasing_1'][0],df_garry.loc['AdversarialDebiasing_1'][4]]\n",
        "  diccit_4[key] = [df_garry.loc['AdversarialDebiasing_100'][0],df_garry.loc['AdversarialDebiasing_100'][4]]\n",
        "  diccit_5[key] = [df_garry.loc['AdversarialDebiasing_10000'][0],df_garry.loc['AdversarialDebiasing_10000'][4]]\n",
        "\n"
      ],
      "metadata": {
        "id": "64D5yheAMmD7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "df_1 = pd.DataFrame(diccit_1).T\n",
        "df_1.columns = ['Accuracy', 'equal_opportunity_difference']\n",
        "\n",
        "df_2 = pd.DataFrame(diccit_2).T\n",
        "df_2.columns = ['Accuracy', 'equal_opportunity_difference']\n",
        "\n",
        "df_3 = pd.DataFrame(diccit_3).T\n",
        "df_3.columns = ['Accuracy', 'equal_opportunity_difference']\n",
        "\n",
        "df_4 = pd.DataFrame(diccit_4).T\n",
        "df_4.columns = ['Accuracy', 'equal_opportunity_difference']\n",
        "\n",
        "df_5 = pd.DataFrame(diccit_5).T\n",
        "df_5.columns = ['Accuracy', 'equal_opportunity_difference']\n",
        "\n",
        "#fig = plt.figure(figsize=(10,5))\n",
        "\n",
        "fig, ax = plt.subplots(2, 1,figsize=(10,7))\n",
        "\n",
        "ax[0].plot(df_1['Accuracy'],label='Accuracy 1e-05') #row=0, col=0\n",
        "ax[0].plot(df_2['Accuracy'],label='Accuracy 0.001') #row=0, col=0\n",
        "ax[0].plot(df_3['Accuracy'],label='Accuracy 1') #row=0, col=0\n",
        "#ax[0].plot(df_4['Accuracy'],label='Accuracy 100') #row=0, col=0\n",
        "#ax[0].plot(df_5['Accuracy'],label='Accuracy 10000') #row=0, col=0\n",
        "\n",
        "ax[1].plot(df_1['equal_opportunity_difference'],label='Fairness 1e-05') #row=0, col=0\n",
        "ax[1].plot(df_2['equal_opportunity_difference'],label='Fairness 0.001') #row=0, col=0\n",
        "ax[1].plot(df_3['equal_opportunity_difference'],label='Fairness 1') #row=0, col=0\n",
        "#ax[1].plot(df_4['equal_opportunity_difference'],label='Fairness 100') #row=0, col=0\n",
        "#ax[1].plot(df_5['equal_opportunity_difference'],label='Fairness 10000') #row=0, col=0\n",
        "\n",
        "\n",
        "ax[0].set(ylabel=\"Accuracy (%)\",title=\"Experiment on impact of trade-off hyperparameter Adult Dataset (AdversarialDebiasing)\")\n",
        "ax[1].set(ylabel=\"Fairness score\",title=\"Experiment on impact of trade-off hyperparameter Adult Dataset (AdversarialDebiasing)\")\n",
        "\n",
        "#reference:https://stackabuse.com/rotate-axis-labels-in-matplotlib/\n",
        "fig.autofmt_xdate()#Rotate ticks to Fit in Matplotlib\n",
        "fig.tight_layout()\n",
        "ax[0].legend()\n",
        "ax[1].legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 507
        },
        "id": "9jFYS0NCRe2Q",
        "outputId": "439f0d23-8149-402c-daf8-8ba683e94cf8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 720x504 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "file = open(\"/content/drive/MyDrive/UniSussex/ML/SplitsExtra_GerryFairClassifier_adult_.pickle\",'rb')\n",
        "object_file = pickle.load(file)\n",
        "file.close()\n",
        "\n",
        "#retriving the avarage of each split\n",
        "diccit_1 = {}\n",
        "diccit_2 = {}\n",
        "diccit_3 = {}\n",
        "diccit_4 = {}\n",
        "diccit_5 = {}\n",
        "\n",
        "for key in object_file:\n",
        "  df_garry =  retrive_metrics_dataframe_garr_(object_file[key])\n",
        "  diccit_1[key] = [df_garry.loc['GerryFairClassifier_1e-05'][0],df_garry.loc['GerryFairClassifier_1e-05'][4]]\n",
        "  diccit_2[key] = [df_garry.loc['GerryFairClassifier_0.001'][0],df_garry.loc['GerryFairClassifier_0.001'][4]]\n",
        "  diccit_3[key] = [df_garry.loc['GerryFairClassifier_1'][0],df_garry.loc['GerryFairClassifier_1'][4]]\n",
        "  diccit_4[key] = [df_garry.loc['GerryFairClassifier_100'][0],df_garry.loc['GerryFairClassifier_100'][4]]\n",
        "  diccit_5[key] = [df_garry.loc['GerryFairClassifier_10000'][0],df_garry.loc['GerryFairClassifier_10000'][4]]"
      ],
      "metadata": {
        "id": "cGeiEaqwTqrN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "df_1 = pd.DataFrame(diccit_1).T\n",
        "df_1.columns = ['Accuracy', 'equal_opportunity_difference']\n",
        "\n",
        "df_2 = pd.DataFrame(diccit_2).T\n",
        "df_2.columns = ['Accuracy', 'equal_opportunity_difference']\n",
        "\n",
        "df_3 = pd.DataFrame(diccit_3).T\n",
        "df_3.columns = ['Accuracy', 'equal_opportunity_difference']\n",
        "\n",
        "df_4 = pd.DataFrame(diccit_4).T\n",
        "df_4.columns = ['Accuracy', 'equal_opportunity_difference']\n",
        "\n",
        "df_5 = pd.DataFrame(diccit_5).T\n",
        "df_5.columns = ['Accuracy', 'equal_opportunity_difference']\n",
        "\n",
        "#fig = plt.figure(figsize=(10,5))\n",
        "\n",
        "fig, ax = plt.subplots(2, 1,figsize=(10,7))\n",
        "\n",
        "ax[0].plot(df_1['Accuracy'],label='Accuracy 1e-05') #row=0, col=0\n",
        "ax[0].plot(df_2['Accuracy'],label='Accuracy 0.001') #row=0, col=0\n",
        "ax[0].plot(df_3['Accuracy'],label='Accuracy 1') #row=0, col=0\n",
        "#ax[0].plot(df_4['Accuracy'],label='Accuracy 100') #row=0, col=0\n",
        "#ax[0].plot(df_5['Accuracy'],label='Accuracy 10000') #row=0, col=0\n",
        "\n",
        "ax[1].plot(df_1['equal_opportunity_difference'],label='Fairness 1e-05') #row=0, col=0\n",
        "ax[1].plot(df_2['equal_opportunity_difference'],label='Fairness 0.001') #row=0, col=0\n",
        "ax[1].plot(df_3['equal_opportunity_difference'],label='Fairness 1') #row=0, col=0\n",
        "#ax[1].plot(df_4['equal_opportunity_difference'],label='Fairness 100') #row=0, col=0\n",
        "#ax[1].plot(df_5['equal_opportunity_difference'],label='Fairness 10000') #row=0, col=0\n",
        "\n",
        "\n",
        "ax[0].set(ylabel=\"Accuracy (%)\",title=\"Experiment on impact of trade-off hyperparameter Adult Dataset (GerryFairClassifier)\")\n",
        "ax[1].set(ylabel=\"Fairness score\",title=\"Experiment on impact of trade-off hyperparameter Adult Dataset (GerryFairClassifier)\")\n",
        "\n",
        "#reference:https://stackabuse.com/rotate-axis-labels-in-matplotlib/\n",
        "fig.autofmt_xdate()#Rotate ticks to Fit in Matplotlib\n",
        "fig.tight_layout()\n",
        "ax[0].legend()\n",
        "ax[1].legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 507
        },
        "id": "NmnvRU3KYeAN",
        "outputId": "d9b53157-8d30-4ac2-e99b-56b5cbdd21f1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 720x504 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "7IxrjHzxYeIh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!ls '/content/drive/MyDrive/UniSussex/ML/'\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SOIdUGkMLE2I",
        "outputId": "8f034d7d-6a0f-45d8-ba06-fcf8e1f0369a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "AdversarialDebiasing_job_.pickle\n",
            "AdversarialDebiasing_relationshipfeature_.pickle\n",
            "DecisionTree_jobfeature_weighted_.pickle\n",
            "DecisionTree_nosensitivefeatureAdult_.pickle\n",
            "DecisionTree_nosensitivefeature_postprocessingGerman_.pickle\n",
            "DecisionTree_relationshipfeature_weighted_.pickle\n",
            "results_occupation_Tree_Extra_postprocessing_GerryFair_.pickle\n",
            "results_occupation_Tree_Extra_prepostprocessing.pickle\n",
            "SplitsExtra_AdversarialDebiasingClassifier_adult_.pickle\n",
            "SplitsExtra_GerryFairClassifier_adult_.pickle\n",
            "Submit_ML_Coursework_Final_Part2_final.ipynb\n",
            "Submit_ML_Coursework_Final_Part2.ipynb\n"
          ]
        }
      ]
    }
  ]
}